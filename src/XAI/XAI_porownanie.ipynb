{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza Metod XAI dla Rozpoznawania Emocji z Danych Audio\n",
    "\n",
    "Ten notebook przedstawia kompleksowe porównanie różnych metod wyjaśnialnej sztucznej inteligencji (XAI) zastosowanych do modelu ResNet klasyfikującego emocje na podstawie spektrogramów Mela. Celem projektu jest zbadanie, które obszary spektrogramu są najistotniejsze dla rozpoznawania poszczególnych emocji oraz porównanie skuteczności różnych technik XAI w kontekście danych dźwiękowych.\n",
    "\n",
    "## Zawartość Notebooka\n",
    "\n",
    "Notebook zawiera następujące sekcje:\n",
    "\n",
    "1. **Importy i konfiguracja** - inicjalizacja niezbędnych bibliotek i ustawienie parametrów\n",
    "2. **Wczytanie modelu i danych** - załadowanie wytrenowanego modelu ResNet i przygotowanie danych testowych\n",
    "3. **Funkcja predykcji dla metod XAI** - implementacja mechanizmu przewidywania służącego metodom XAI\n",
    "4. **Generowanie wyjaśnień** - tworzenie wyjaśnień przy użyciu czterech różnych metod XAI\n",
    "5. **Wizualizacja porównawcza** - wyświetlenie wyjaśnień dla każdej emocji\n",
    "6. **Weryfikacja wyjaśnień** - obiektywna ocena skuteczności metod XAI\n",
    "7. **Porównanie wszystkich metod** - analiza statystyczna wydajności metod\n",
    "8. **Wnioski końcowe** - podsumowanie najlepszych i najgorszych technik\n",
    "\n",
    "## Zaimplementowane Metody XAI\n",
    "\n",
    "1. **LIME (Local Interpretable Model-agnostic Explanations)** - metoda tworząca lokalny model zastępczy, który przybliża zachowanie czarnej skrzynki w okolicy konkretnej próbki\n",
    "2. **LRP (Layer-wise Relevance Propagation)** - technika propagacji wstecznej, która dystrybuuje wkład każdego piksela wejściowego w końcową predykcję\n",
    "3. **GradCAM (Gradient-weighted Class Activation Mapping)** - metoda wykorzystująca gradienty płynące do ostatniej warstwy konwolucyjnej do generowania map aktywacji\n",
    "4. **Wygładzone Mapy Ciszy (Smooth Saliency Maps)** - nowatorskie podejście identyfikujące obszary spektrogramu, które najmniej przyczyniają się do klasyfikacji\n",
    "\n",
    "## Struktura Modułów\n",
    "\n",
    "Projekt wykorzystuje następujące moduły:\n",
    "\n",
    "- **xai_methods/** - zawiera implementacje wszystkich metod XAI:\n",
    "  - `lime_explainer.py` - implementacja LIME dostosowana do spektrogramów\n",
    "  - `lrp_explainer.py` - implementacja propagacji istotności w warstwach\n",
    "  - `gradcam_explainer.py` - implementacja GradCAM dla modelu ResNet\n",
    "  - `silence_maps_explainer.py` - implementacja wygładzonych map ciszy\n",
    "  - `visualization_utils.py` - narzędzia do wizualizacji wyjaśnień\n",
    "  \n",
    "- **helpers/** - moduły pomocnicze:\n",
    "  - `resnet_model_definition.py` - definicja modelu AudioResNet\n",
    "\n",
    "- **config.py** - globalna konfiguracja projektu, ścieżki do plików i parametry\n",
    "\n",
    "## Metodologia Weryfikacji\n",
    "\n",
    "Każda metoda XAI jest weryfikowana poprzez:\n",
    "1. Zasłonięcie obszarów zidentyfikowanych jako najbardziej istotne\n",
    "2. Zasłonięcie losowych obszarów o tej samej wielkości\n",
    "3. Porównanie spadku prawdopodobieństwa klasyfikacji w obu przypadkach\n",
    "\n",
    "Skuteczna metoda XAI powinna wykazywać znacznie większy spadek prawdopodobieństwa po zasłonięciu obszarów istotnych w porównaniu do losowych.\n",
    "\n",
    "## Wymagania\n",
    "\n",
    "Projekt wymaga następujących bibliotek:\n",
    "- PyTorch\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Pandas\n",
    "- Seaborn\n",
    "- LIME\n",
    "- [opcjonalnie] CUDA dla akceleracji GPU\n",
    "\n",
    "## Zastosowanie\n",
    "\n",
    "Notebook demonstruje, jak techniki XAI mogą pomóc zrozumieć, które części spektrogramu są najważniejsze dla rozpoznawania emocji, co może być wykorzystane do:\n",
    "- Ulepszenia algorytmów rozpoznawania emocji\n",
    "- Identyfikacji cech dźwięku istotnych dla poszczególnych emocji\n",
    "- Diagnostyki i debugowania modelu uczenia maszynowego\n",
    "- Zwiększenia zaufania do systemu rozpoznawania emocji\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importy i konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katalog główny projektu: c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\n",
      "Czy katalog src istnieje: True\n",
      "Urządzenie: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Dodaj katalog główny projektu do sys.path\n",
    "current_dir = (\n",
    "    os.path.dirname(os.path.abspath(__file__))\n",
    "    if \"__file__\" in globals()\n",
    "    else os.getcwd()\n",
    ")\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\", \"..\"))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Katalog główny projektu: {project_root}\")\n",
    "print(f\"Czy katalog src istnieje: {os.path.exists(os.path.join(project_root, 'src'))}\")\n",
    "\n",
    "# Standardowe biblioteki\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# Biblioteki ML\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Konfiguracja projektu\n",
    "from src.config import MODEL_DIR\n",
    "\n",
    "# Import naszych modułów XAI\n",
    "from src.XAI.xai_methods.lime_explainer import (\n",
    "    explain_with_lime,\n",
    "    verify_lime_explanation,\n",
    ")\n",
    "from src.XAI.xai_methods.lrp_explainer import LRP, verify_lrp_explanation\n",
    "from src.XAI.xai_methods.gradcam_explainer import GradCAM, verify_gradcam_explanation\n",
    "from src.XAI.xai_methods.silence_maps_explainer import (\n",
    "    create_smooth_silence_map,\n",
    "    verify_silence_map,\n",
    ")\n",
    "from src.XAI.xai_methods.visualization_utils import (\n",
    "    normalize_sample,\n",
    "    display_xai_comparison,\n",
    ")\n",
    "\n",
    "\n",
    "from src.helpers.resnet_model_definition import AudioResNet\n",
    "\n",
    "# Konfiguracja wykresów\n",
    "plt.rcParams[\"figure.facecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.facecolor\"] = \"black\"\n",
    "plt.rcParams[\"axes.labelcolor\"] = \"white\"\n",
    "plt.rcParams[\"axes.titlecolor\"] = \"white\"\n",
    "plt.rcParams[\"xtick.color\"] = \"white\"\n",
    "plt.rcParams[\"ytick.color\"] = \"white\"\n",
    "\n",
    "# Określenie urządzenia (CPU lub GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Urządzenie: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wczytanie modelu i danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano model: c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\ResNet_mel\\model_outputs\\best_model_20250515_090117.pt\n",
      "Model załadowany i gotowy do użycia.\n",
      "\n",
      "Struktura modelu AudioResNet:\n",
      "- resnet: ResNet\n",
      "- resnet.conv1: Conv2d\n",
      "- resnet.bn1: BatchNorm2d\n",
      "- resnet.relu: ReLU\n",
      "- resnet.maxpool: MaxPool2d\n",
      "- resnet.layer1: Sequential\n",
      "- resnet.layer1.0: BasicBlock\n",
      "- resnet.layer1.0.conv1: Conv2d\n",
      "- resnet.layer1.0.bn1: BatchNorm2d\n",
      "- resnet.layer1.0.relu: ReLU\n",
      "- resnet.layer1.0.conv2: Conv2d\n",
      "- resnet.layer1.0.bn2: BatchNorm2d\n",
      "- resnet.layer1.1: BasicBlock\n",
      "- resnet.layer1.1.conv1: Conv2d\n",
      "- resnet.layer1.1.bn1: BatchNorm2d\n",
      "- resnet.layer1.1.relu: ReLU\n",
      "- resnet.layer1.1.conv2: Conv2d\n",
      "- resnet.layer1.1.bn2: BatchNorm2d\n",
      "- resnet.layer2: Sequential\n",
      "- resnet.layer2.0: BasicBlock\n",
      "- resnet.layer2.0.conv1: Conv2d\n",
      "- resnet.layer2.0.bn1: BatchNorm2d\n",
      "- resnet.layer2.0.relu: ReLU\n",
      "- resnet.layer2.0.conv2: Conv2d\n",
      "- resnet.layer2.0.bn2: BatchNorm2d\n",
      "- resnet.layer2.0.downsample: Sequential\n",
      "- resnet.layer2.0.downsample.0: Conv2d\n",
      "- resnet.layer2.0.downsample.1: BatchNorm2d\n",
      "- resnet.layer2.1: BasicBlock\n",
      "- resnet.layer2.1.conv1: Conv2d\n",
      "- resnet.layer2.1.bn1: BatchNorm2d\n",
      "- resnet.layer2.1.relu: ReLU\n",
      "- resnet.layer2.1.conv2: Conv2d\n",
      "- resnet.layer2.1.bn2: BatchNorm2d\n",
      "- resnet.layer3: Sequential\n",
      "- resnet.layer3.0: BasicBlock\n",
      "- resnet.layer3.0.conv1: Conv2d\n",
      "- resnet.layer3.0.bn1: BatchNorm2d\n",
      "- resnet.layer3.0.relu: ReLU\n",
      "- resnet.layer3.0.conv2: Conv2d\n",
      "- resnet.layer3.0.bn2: BatchNorm2d\n",
      "- resnet.layer3.0.downsample: Sequential\n",
      "- resnet.layer3.0.downsample.0: Conv2d\n",
      "- resnet.layer3.0.downsample.1: BatchNorm2d\n",
      "- resnet.layer3.1: BasicBlock\n",
      "- resnet.layer3.1.conv1: Conv2d\n",
      "- resnet.layer3.1.bn1: BatchNorm2d\n",
      "- resnet.layer3.1.relu: ReLU\n",
      "- resnet.layer3.1.conv2: Conv2d\n",
      "- resnet.layer3.1.bn2: BatchNorm2d\n",
      "- resnet.layer4: Sequential\n",
      "- resnet.layer4.0: BasicBlock\n",
      "- resnet.layer4.0.conv1: Conv2d\n",
      "- resnet.layer4.0.bn1: BatchNorm2d\n",
      "- resnet.layer4.0.relu: ReLU\n",
      "- resnet.layer4.0.conv2: Conv2d\n",
      "- resnet.layer4.0.bn2: BatchNorm2d\n",
      "- resnet.layer4.0.downsample: Sequential\n",
      "- resnet.layer4.0.downsample.0: Conv2d\n",
      "- resnet.layer4.0.downsample.1: BatchNorm2d\n",
      "- resnet.layer4.1: BasicBlock\n",
      "- resnet.layer4.1.conv1: Conv2d\n",
      "- resnet.layer4.1.bn1: BatchNorm2d\n",
      "- resnet.layer4.1.relu: ReLU\n",
      "- resnet.layer4.1.conv2: Conv2d\n",
      "- resnet.layer4.1.bn2: BatchNorm2d\n",
      "- resnet.avgpool: AdaptiveAvgPool2d\n",
      "- resnet.fc: Identity\n",
      "- dropout: Dropout\n",
      "- fc: Linear\n"
     ]
    }
   ],
   "source": [
    "# Automatyczne wyszukiwanie pliku modelu\n",
    "model_files = glob.glob(os.path.join(MODEL_DIR, \"best_model_*.pt\"))\n",
    "if len(model_files) == 0:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Nie znaleziono żadnego pliku modelu w katalogu: {MODEL_DIR}\"\n",
    "    )\n",
    "else:\n",
    "    MODEL_PATH = model_files[0]  # Wybierz pierwszy znaleziony plik\n",
    "    print(f\"Załadowano model: {MODEL_PATH}\")\n",
    "\n",
    "# Załadowanie modelu\n",
    "model = AudioResNet()\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model załadowany i gotowy do użycia.\")\n",
    "\n",
    "# Struktura modelu\n",
    "print(\"\\nStruktura modelu AudioResNet:\")\n",
    "for name, module in model.named_modules():\n",
    "    if name != \"\":  # Pomijamy moduł główny\n",
    "        print(f\"- {name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\kubas\\\\Desktop\\\\Projekt dyplomowy\\\\Audio-Emotion-Recognition\\\\src\\\\ResNet_mel\\\\model_outputs\\\\X_test.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Wczytanie danych testowych\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX_test.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MODEL_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKształt danych testowych: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\.venv\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\kubas\\\\Desktop\\\\Projekt dyplomowy\\\\Audio-Emotion-Recognition\\\\src\\\\ResNet_mel\\\\model_outputs\\\\X_test.npy'"
     ]
    }
   ],
   "source": [
    "# Wczytanie danych testowych\n",
    "X_test = np.load(os.path.join(MODEL_DIR, \"X_test.npy\"))\n",
    "y_test = np.load(os.path.join(MODEL_DIR, \"y_test.npy\"))\n",
    "\n",
    "print(f\"Kształt danych testowych: {X_test.shape}\")\n",
    "print(f\"Liczba etykiet testowych: {len(y_test)}\")\n",
    "\n",
    "# Wczytanie mapowania etykiet na emocje\n",
    "label_mapping = np.load(\n",
    "    os.path.join(MODEL_DIR, \"label_mapping.npy\"), allow_pickle=True\n",
    ").item()\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}  # Odwrócenie mapowania\n",
    "print(f\"\\nMapowanie etykiet: {reverse_label_mapping}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wybierz 2 przykłady dla każdej emocji do analizy XAI\n",
    "emotion_samples = {}\n",
    "\n",
    "# Iteracja przez dane testowe\n",
    "for i, label in enumerate(y_test):\n",
    "    emotion = reverse_label_mapping[label]  # Zamiana etykiety na nazwę emocji\n",
    "\n",
    "    # Jeśli emocja nie jest jeszcze w słowniku, dodaj ją\n",
    "    if emotion not in emotion_samples:\n",
    "        emotion_samples[emotion] = []\n",
    "\n",
    "    # Dodaj próbkę, jeśli liczba przykładów dla tej emocji jest mniejsza niż 2\n",
    "    if len(emotion_samples[emotion]) < 2:\n",
    "        emotion_samples[emotion].append(X_test[i])\n",
    "\n",
    "    # Sprawdź czy mamy już po 2 przykłady dla każdej emocji\n",
    "    if all(len(samples) >= 2 for samples in emotion_samples.values()):\n",
    "        break\n",
    "\n",
    "# Wyświetlenie liczby próbek dla każdej emocji\n",
    "for emotion, samples in emotion_samples.items():\n",
    "    print(f\"Emocja: {emotion}, Liczba przykładów: {len(samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funkcja predykcji dla metod XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(images):\n",
    "    \"\"\"\n",
    "    Funkcja predykcyjna dla metod XAI.\n",
    "    Przyjmuje obrazy w formacie [N, H, W, C], przekształca je na tensor PyTorch\n",
    "    i zwraca prawdopodobieństwa dla każdej klasy.\n",
    "    \"\"\"\n",
    "    # Zamiana obrazów na tensor PyTorch i permutacja wymiarów [N, H, W, C] -> [N, C, H, W]\n",
    "    images = torch.tensor(images).permute(0, 3, 1, 2).float()\n",
    "\n",
    "    # Upewnij się, że dane mają 1 kanał\n",
    "    images = images[:, :1, :, :]\n",
    "\n",
    "    # Przewidywanie za pomocą modelu\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generowanie wyjaśnień dla wszystkich metod XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicjalizacja obiektów dla metod XAI\n",
    "gradcam = GradCAM(model, \"layer4\")\n",
    "lrp_analyzer = LRP(model)\n",
    "\n",
    "# Słowniki do przechowywania wyjaśnień\n",
    "all_explanations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generowanie wyjaśnień dla wszystkich próbek\n",
    "for emotion, samples in emotion_samples.items():\n",
    "    all_explanations[emotion] = []\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        print(f\"Generowanie wyjaśnień dla {emotion}, przykład {i + 1}/{len(samples)}\")\n",
    "\n",
    "        # Przygotowanie próbki\n",
    "        if sample.shape[0] == 1:\n",
    "            sample = sample.squeeze(0)\n",
    "\n",
    "        # Normalizacja\n",
    "        sample_norm = normalize_sample(sample)\n",
    "\n",
    "        # Konwersja do formatu PyTorch\n",
    "        sample_tensor = (\n",
    "            torch.tensor(sample).unsqueeze(0).unsqueeze(0).float().to(device)\n",
    "        )\n",
    "\n",
    "        # 1. LIME\n",
    "        print(\"  - Generowanie wyjaśnienia LIME...\")\n",
    "        lime_temp, lime_mask, segments = explain_with_lime(sample_norm, predict_fn)\n",
    "\n",
    "        # 2. LRP\n",
    "        print(\"  - Generowanie wyjaśnienia LRP...\")\n",
    "        relevance_map = lrp_analyzer.generate_relevance_map(sample_tensor)\n",
    "\n",
    "        # 3. GradCAM\n",
    "        print(\"  - Generowanie wyjaśnienia GradCAM...\")\n",
    "        gradcam_map = gradcam.generate_cam(sample_tensor)\n",
    "\n",
    "        # 4. Smooth Saliency Maps\n",
    "        print(\"  - Generowanie wygładzonej mapy ciszy...\")\n",
    "        silence_map = create_smooth_silence_map(\n",
    "            sample, n_samples=30, noise_level=0.03, percentile=25\n",
    "        )\n",
    "\n",
    "        # Zapisz wszystkie wyjaśnienia\n",
    "        all_explanations[emotion].append(\n",
    "            {\n",
    "                \"sample\": sample,\n",
    "                \"lime\": (lime_temp, lime_mask),\n",
    "                \"lrp\": relevance_map,\n",
    "                \"gradcam\": gradcam_map,\n",
    "                \"silence\": silence_map,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wizualizacja porównawcza dla każdej emocji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja porównań dla wszystkich emocji\n",
    "for emotion, explanations in all_explanations.items():\n",
    "    print(f\"\\n=== Emocja: {emotion} ===\")\n",
    "\n",
    "    for i, exp in enumerate(explanations):\n",
    "        sample = exp[\"sample\"]\n",
    "\n",
    "        # Przygotowanie wyjaśnień do wyświetlenia\n",
    "        xai_results = {\n",
    "            \"lime\": exp[\"lime\"],\n",
    "            \"gradcam\": exp[\"gradcam\"],\n",
    "            \"lrp\": exp[\"lrp\"],\n",
    "            \"silence\": exp[\"silence\"],\n",
    "        }\n",
    "\n",
    "        # Użycie funkcji wizualizacyjnej z visualization_utils\n",
    "        fig = display_xai_comparison(sample, xai_results, emotion)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Weryfikacja wyjaśnień"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja LIME\n",
    "print(\"\\n=== Weryfikacja LIME ===\")\n",
    "lime_results = []\n",
    "\n",
    "for emotion, explanations in all_explanations.items():\n",
    "    for i, exp in enumerate(explanations):\n",
    "        sample = exp[\"sample\"]\n",
    "        lime_temp, lime_mask = exp[\"lime\"]\n",
    "\n",
    "        result = verify_lime_explanation(\n",
    "            model, sample, lime_mask, device, reverse_label_mapping\n",
    "        )\n",
    "        result[\"emotion\"] = emotion\n",
    "        lime_results.append(result)\n",
    "\n",
    "        print(f\"Emocja: {emotion}, Przykład {i + 1}\")\n",
    "        print(\n",
    "            f\"  - Spadek po zasłonięciu ważnych obszarów: {result['prob_drop_important']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  - Spadek po zasłonięciu losowych obszarów: {result['prob_drop_random']:.4f}\"\n",
    "        )\n",
    "\n",
    "lime_df = pd.DataFrame(lime_results)\n",
    "lime_summary = lime_df.groupby(\"emotion\")[\n",
    "    [\"prob_drop_important\", \"prob_drop_random\"]\n",
    "].mean()\n",
    "print(\"\\nŚrednie wyniki dla LIME:\")\n",
    "print(lime_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja LRP\n",
    "print(\"\\n=== Weryfikacja LRP ===\")\n",
    "lrp_results = []\n",
    "\n",
    "for emotion, explanations in all_explanations.items():\n",
    "    for i, exp in enumerate(explanations):\n",
    "        sample = exp[\"sample\"]\n",
    "        relevance_map = exp[\"lrp\"]\n",
    "\n",
    "        result = verify_lrp_explanation(\n",
    "            model, sample, relevance_map, device, reverse_label_mapping\n",
    "        )\n",
    "        result[\"emotion\"] = emotion\n",
    "        lrp_results.append(result)\n",
    "\n",
    "        print(f\"Emocja: {emotion}, Przykład {i + 1}\")\n",
    "        print(\n",
    "            f\"  - Spadek po zasłonięciu ważnych obszarów: {result['prob_drop_important']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  - Spadek po zasłonięciu losowych obszarów: {result['prob_drop_random']:.4f}\"\n",
    "        )\n",
    "\n",
    "lrp_df = pd.DataFrame(lrp_results)\n",
    "lrp_summary = lrp_df.groupby(\"emotion\")[\n",
    "    [\"prob_drop_important\", \"prob_drop_random\"]\n",
    "].mean()\n",
    "print(\"\\nŚrednie wyniki dla LRP:\")\n",
    "print(lrp_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja GradCAM\n",
    "print(\"\\n=== Weryfikacja GradCAM ===\")\n",
    "gradcam_results = []\n",
    "\n",
    "for emotion, explanations in all_explanations.items():\n",
    "    for i, exp in enumerate(explanations):\n",
    "        sample = exp[\"sample\"]\n",
    "        gradcam_map = exp[\"gradcam\"]\n",
    "\n",
    "        result = verify_gradcam_explanation(\n",
    "            model, sample, gradcam_map, device, reverse_label_mapping\n",
    "        )\n",
    "        result[\"emotion\"] = emotion\n",
    "        gradcam_results.append(result)\n",
    "\n",
    "        print(f\"Emocja: {emotion}, Przykład {i + 1}\")\n",
    "        print(\n",
    "            f\"  - Spadek po zasłonięciu ważnych obszarów: {result['prob_drop_important']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  - Spadek po zasłonięciu losowych obszarów: {result['prob_drop_random']:.4f}\"\n",
    "        )\n",
    "\n",
    "gradcam_df = pd.DataFrame(gradcam_results)\n",
    "gradcam_summary = gradcam_df.groupby(\"emotion\")[\n",
    "    [\"prob_drop_important\", \"prob_drop_random\"]\n",
    "].mean()\n",
    "print(\"\\nŚrednie wyniki dla GradCAM:\")\n",
    "print(gradcam_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja Silence Maps\n",
    "print(\"\\n=== Weryfikacja Smooth Silence Maps ===\")\n",
    "silence_results = []\n",
    "\n",
    "for emotion, explanations in all_explanations.items():\n",
    "    for i, exp in enumerate(explanations):\n",
    "        sample = exp[\"sample\"]\n",
    "        silence_map = exp[\"silence\"]\n",
    "\n",
    "        result = verify_silence_map(\n",
    "            model, sample, silence_map, device, reverse_label_mapping\n",
    "        )\n",
    "        result[\"emotion\"] = emotion\n",
    "        silence_results.append(result)\n",
    "\n",
    "        print(f\"Emocja: {emotion}, Przykład {i + 1}\")\n",
    "        print(\n",
    "            f\"  - Spadek po zasłonięciu obszarów ciszy: {result['prob_drop_silence']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  - Spadek po zasłonięciu losowych obszarów: {result['prob_drop_random']:.4f}\"\n",
    "        )\n",
    "\n",
    "silence_df = pd.DataFrame(silence_results)\n",
    "silence_summary = silence_df.groupby(\"emotion\")[\n",
    "    [\"prob_drop_silence\", \"prob_drop_random\"]\n",
    "].mean()\n",
    "print(\"\\nŚrednie wyniki dla Smooth Silence Maps:\")\n",
    "print(silence_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Porównanie wszystkich metod XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie ramki danych porównawczej dla wszystkich metod\n",
    "comparison_data = {}\n",
    "\n",
    "# Zbieranie danych dla różnych metod\n",
    "for emotion in emotion_samples.keys():\n",
    "    comparison_data[emotion] = {\n",
    "        \"LIME_important\": lime_summary.loc[emotion, \"prob_drop_important\"]\n",
    "        if emotion in lime_summary.index\n",
    "        else np.nan,\n",
    "        \"LIME_random\": lime_summary.loc[emotion, \"prob_drop_random\"]\n",
    "        if emotion in lime_summary.index\n",
    "        else np.nan,\n",
    "        \"LRP_important\": lrp_summary.loc[emotion, \"prob_drop_important\"]\n",
    "        if emotion in lrp_summary.index\n",
    "        else np.nan,\n",
    "        \"LRP_random\": lrp_summary.loc[emotion, \"prob_drop_random\"]\n",
    "        if emotion in lrp_summary.index\n",
    "        else np.nan,\n",
    "        \"GradCAM_important\": gradcam_summary.loc[emotion, \"prob_drop_important\"]\n",
    "        if emotion in gradcam_summary.index\n",
    "        else np.nan,\n",
    "        \"GradCAM_random\": gradcam_summary.loc[emotion, \"prob_drop_random\"]\n",
    "        if emotion in gradcam_summary.index\n",
    "        else np.nan,\n",
    "        \"Silence_important\": silence_summary.loc[emotion, \"prob_drop_silence\"]\n",
    "        if emotion in silence_summary.index\n",
    "        else np.nan,\n",
    "        \"Silence_random\": silence_summary.loc[emotion, \"prob_drop_random\"]\n",
    "        if emotion in silence_summary.index\n",
    "        else np.nan,\n",
    "    }\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data).T\n",
    "\n",
    "# Wyświetlanie wyników\n",
    "print(\"Porównanie wszystkich metod XAI:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja porównawcza spadku prawdopodobieństwa dla ważnych obszarów\n",
    "important_cols = [\n",
    "    col\n",
    "    for col in comparison_df.columns\n",
    "    if (\"important\" in col or \"silence\" in col) and \"random\" not in col\n",
    "]\n",
    "plt.figure(figsize=(14, 8))\n",
    "comparison_df[important_cols].plot(kind=\"bar\", figsize=(14, 8))\n",
    "plt.title(\n",
    "    \"Porównanie spadku prawdopodobieństwa dla obszarów istotnych\",\n",
    "    fontsize=16,\n",
    "    color=\"white\",\n",
    ")\n",
    "plt.ylabel(\"Spadek prawdopodobieństwa\", fontsize=14, color=\"white\")\n",
    "plt.xlabel(\"Emocje\", fontsize=14, color=\"white\")\n",
    "plt.legend([\"LIME\", \"LRP\", \"GradCAM\", \"Smooth Saliency Maps \"], labelcolor=\"white\")\n",
    "plt.xticks(rotation=0, color=\"white\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obliczenie różnicy między spadkiem dla ważnych i losowych obszarów (skuteczność metody XAI)\n",
    "efficiency_df = pd.DataFrame()\n",
    "efficiency_df[\"LIME\"] = comparison_df[\"LIME_important\"] - comparison_df[\"LIME_random\"]\n",
    "efficiency_df[\"LRP\"] = comparison_df[\"LRP_important\"] - comparison_df[\"LRP_random\"]\n",
    "efficiency_df[\"GradCAM\"] = (\n",
    "    comparison_df[\"GradCAM_important\"] - comparison_df[\"GradCAM_random\"]\n",
    ")\n",
    "# Dla Silence Maps patrzymy na przeciwną różnicę, bo chcemy żeby był mniejszy spadek dla obszarów ciszy\n",
    "efficiency_df[\"Silence_Maps\"] = (\n",
    "    comparison_df[\"Silence_random\"] - comparison_df[\"Silence_important\"]\n",
    ")\n",
    "\n",
    "# Wyświetlanie skuteczności metod\n",
    "print(\n",
    "    \"Skuteczność metod XAI (różnica między spadkiem dla obszarów istotnych i losowych):\"\n",
    ")\n",
    "print(efficiency_df)\n",
    "print(\"\\nŚrednia skuteczność dla wszystkich emocji:\")\n",
    "print(efficiency_df.mean())\n",
    "\n",
    "# Wizualizacja skuteczności\n",
    "plt.figure(figsize=(14, 8))\n",
    "efficiency_df.plot(kind=\"bar\", figsize=(14, 8))\n",
    "plt.title(\"Skuteczność metod XAI dla każdej emocji\", fontsize=16, color=\"white\")\n",
    "plt.ylabel(\"Skuteczność (większa wartość = lepsza metoda)\", fontsize=14, color=\"white\")\n",
    "plt.xlabel(\"Emocje\", fontsize=14, color=\"white\")\n",
    "plt.legend([\"LIME\", \"LRP\", \"GradCAM\", \"Smooth Saliency Maps\"], labelcolor=\"white\")\n",
    "plt.axhline(y=0, color=\"r\", linestyle=\"-\", alpha=0.3)\n",
    "plt.xticks(rotation=0, color=\"white\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wnioski i porównanie końcowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podsumowanie skuteczności metod\n",
    "best_method = efficiency_df.mean().idxmax()\n",
    "mean_efficiency = efficiency_df.mean()\n",
    "\n",
    "print(\"Podsumowanie skuteczności metod XAI:\")\n",
    "for method, score in mean_efficiency.items():\n",
    "    status = \"✅ Skuteczna\" if score > 0 else \"❌ Nieskuteczna\"\n",
    "    print(f\"{method}: {score:.4f} - {status}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nNajlepsza metoda dla tego modelu: {best_method} (średnia skuteczność: {mean_efficiency[best_method]:.4f})\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Audio Emotion Recognition)",
   "language": "python",
   "name": "audio-emotion-recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
