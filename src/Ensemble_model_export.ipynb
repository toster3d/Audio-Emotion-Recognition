{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksport modelu ensemble do formatu ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znaleziono model ensemble: ensemble_outputs\\ensemble_run_20250519_150653\\models\\best_ensemble_model.pt\n",
      "Ładowanie modelu dla melspectrogram: best_model_melspectrogram_20250517_001727.pt\n",
      "Model załadowany pomyślnie z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/melspectrogram\\best_model_melspectrogram_20250517_001727.pt\n",
      "Ładowanie modelu dla mfcc: best_model_mfcc_20250517_004119.pt\n",
      "Model załadowany pomyślnie z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/mfcc\\best_model_mfcc_20250517_004119.pt\n",
      "Ładowanie modelu dla chroma: best_model_chroma_20250517_005304.pt\n",
      "Model załadowany pomyślnie z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/chroma\\best_model_chroma_20250517_005304.pt\n",
      "Załadowano model z weights_only=True\n",
      "Model ensemble załadowany z ensemble_outputs\\ensemble_run_20250519_150653\\models\\best_ensemble_model.pt\n",
      "Typy cech: ['melspectrogram', 'mfcc', 'chroma']\n",
      "Wagi: {'melspectrogram': 0.3617790639400482, 'mfcc': 0.42766273021698, 'chroma': 0.2105581909418106}\n",
      "Model ensemble gotowy do eksportu. Typy cech: ['melspectrogram', 'mfcc', 'chroma']\n",
      "Katalog wyjściowy: exported_models/onnx_20250520_174735\n",
      "\n",
      "Przygotowywanie danych kalibracyjnych dla kwantyzacji statycznej...\n",
      "Nie znaleziono plików audio. Generowanie sztucznych danych...\n",
      "Wygenerowano sztuczną próbkę audio 1/10\n",
      "Wygenerowano sztuczną próbkę audio 2/10\n",
      "Wygenerowano sztuczną próbkę audio 3/10\n",
      "Wygenerowano sztuczną próbkę audio 4/10\n",
      "Wygenerowano sztuczną próbkę audio 5/10\n",
      "Wygenerowano sztuczną próbkę audio 6/10\n",
      "Wygenerowano sztuczną próbkę audio 7/10\n",
      "Wygenerowano sztuczną próbkę audio 8/10\n",
      "Wygenerowano sztuczną próbkę audio 9/10\n",
      "Wygenerowano sztuczną próbkę audio 10/10\n",
      "Typy cech modelu: ['melspectrogram', 'mfcc', 'chroma']\n",
      "Utworzono czytnik danych kalibracyjnych z 10 próbkami audio\n",
      "\n",
      "Rozpoczynam eksport modelu...\n",
      "Katalog wyjściowy dla eksportu: exported_models/onnx_20250520_174735\n",
      "\n",
      "Generowanie przykładowych danych wejściowych (dummy input)...\n",
      "Wymiary wejść:\n",
      " - mel_input:   torch.Size([1, 1, 128, 130])\n",
      " - mfcc_input:  torch.Size([1, 1, 40, 130])\n",
      " - chroma_input: torch.Size([1, 1, 12, 130])\n",
      "\n",
      "--- Krok 1: Eksport modelu do formatu ONNX ---\n",
      "Rozpoczynam eksport modelu ensemble do formatu ONNX: exported_models/onnx_20250520_174735\\ensemble_model.onnx\n",
      "Test forward pass zakończony sukcesem. Kształt wyjścia: torch.Size([1, 6])\n",
      "Wymiary wejść:\n",
      " - mel_input:   torch.Size([1, 1, 128, 130])\n",
      " - mfcc_input:  torch.Size([1, 1, 40, 130])\n",
      " - chroma_input: torch.Size([1, 1, 12, 130])\n",
      "Eksport do ONNX zakończony pomyślnie!\n",
      "Weryfikacja modelu ONNX...\n",
      "✓ Model ONNX poprawny!\n",
      "Rozmiar modelu: 127.85 MB\n",
      "Liczba węzłów grafu: 164\n",
      "Liczba inicjalizatorów: 130\n",
      "Model wyeksportowany do: exported_models/onnx_20250520_174735\\ensemble_model.onnx\n",
      "Model ONNX wyeksportowany pomyślnie do: exported_models/onnx_20250520_174735\\ensemble_model.onnx\n",
      "Rozmiar oryginalnego modelu: 127.85 MB\n",
      "\n",
      "--- Krok 2: Optymalizacja modelu ONNX (onnxoptimizer) POMINIĘTA ---\n",
      "Rozmiar modelu używanego do dalszych kroków ('zoptymalizowany'): 127.85 MB (ensemble_model.onnx)\n",
      "\n",
      "--- Krok 3: Kwantyzacja modelu ONNX (static) ---\n",
      "Używam kwantyzacji statycznej. Model bazowy: exported_models/onnx_20250520_174735\\ensemble_model.onnx\n",
      "Format kwantyzacji: qdq, Per-channel: True, Reduce range: False\n",
      "Uruchamianie explicite shape inference dla exported_models/onnx_20250520_174735\\ensemble_model.onnx (static quant)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape inference (static quant) zakończone: C:\\Users\\kubas\\AppData\\Local\\Temp\\onnx_quant_static_fvpk7ww9\\inferred_shape_model_static.onnx.\n",
      "\n",
      "Próba kwantyzacji statycznej (1/3): Podstawowa statyczna int8/int8 qdq dla CNN\n",
      "  Próba kwantyzacji statycznej (Podstawowa statyczna int8/int8 qdq dla CNN) nie powiodła się: Only an existing tensor can be modified, '/ensemble/Softmax_output_0' is not.\n",
      "\n",
      "Próba kwantyzacji statycznej (2/3): Statyczna z wykluczeniem operacji pomocniczych\n",
      "  Wykluczanie węzłów (wg nazw): ['/ensemble/Softmax', '/ensemble/Softmax_1', '/ensemble/Softmax_2'] dla typów ['Softmax', 'ReduceMean', 'Tanh', 'Sigmoid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Próba kwantyzacji statycznej (Statyczna z wykluczeniem operacji pomocniczych) nie powiodła się: No data is collected.\n",
      "\n",
      "Próba kwantyzacji statycznej (3/3): Statyczna uproszczona\n",
      "  Wykluczanie węzłów (wg nazw): ['/ensemble/Softmax', '/ensemble/Softmax_1', '/ensemble/Softmax_2'] dla typów ['Softmax', 'ReduceMean', 'Tanh', 'Sigmoid']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Próba kwantyzacji statycznej (Statyczna uproszczona) nie powiodła się: No data is collected.\n",
      "Wszystkie próby kwantyzacji statycznej nieudane.\n",
      "Ostrzeżenie: Kwantyzacja static nie powiodła się: Wszystkie próby kwantyzacji statycznej nieudane. Ostatni błąd: No data is collected.\n",
      "\n",
      "Model wybrany do weryfikacji: NIESKWANTYZOWANY (exported_models/onnx_20250520_174735\\ensemble_model.onnx)\n",
      "\n",
      "--- Krok 4: Weryfikacja finalnego modelu ONNX ---\n",
      "Dostępni dostawcy ONNX Runtime: ['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Używam CPU do inferencji ONNX (wersja ONNX Runtime: 1.19.2)\n",
      "Używam 8 wątków CPU do inferencji\n",
      "Utworzono sesję ONNX Runtime dla modelu: exported_models/onnx_20250520_174735\\ensemble_model.onnx\n",
      "Użyte dostawce (providers): ['CPUExecutionProvider']\n",
      "Oczekiwane wejścia:\n",
      "  - mel_input: ['batch_size', 1, 128, 'time_steps'] (tensor(float))\n",
      "  - mfcc_input: ['batch_size', 1, 40, 'time_steps'] (tensor(float))\n",
      "  - chroma_input: ['batch_size', 1, 12, 'time_steps'] (tensor(float))\n",
      "Wyjścia modelu:\n",
      "  - output: ['batch_size', 6] (tensor(float))\n",
      "Inicjalizacja sesji ONNX Runtime do weryfikacji...\n",
      "Sesja ONNX Runtime zainicjalizowana.\n",
      "Oczekiwane wejścia ONNX: ['mel_input', 'mfcc_input', 'chroma_input']\n",
      "Uruchamianie inferencji ONNX do weryfikacji...\n",
      "Kształt wyjścia ONNX: (1, 6)\n",
      "Porównywanie wyjść modeli PyTorch i ONNX...\n",
      "Statystyki różnic: Max=0.000000, Mean=0.000000, Median=0.000000, Std=0.000000. Tolerancja=0.007895\n",
      "Weryfikacja ONNX zakończona pomyślnie (lub tylko inferencja ONNX jeśli brak modelu PyTorch).\n",
      "Weryfikacja modelu zakończona. Max różnica vs PyTorch: 0.000000\n",
      "\n",
      "Eksport modelu się nie powiódł\n",
      "Błąd: Nieznany błąd\n"
     ]
    }
   ],
   "source": [
    "# Komórka 1: Importy i konfiguracja początkowa\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Importy z Twojego projektu\n",
    "from export_scripts.export_onnx_notebook_helper import export_model_to_onnx\n",
    "from export_scripts.callibration_utils import EnsembleCalibrationDataReader\n",
    "from config import (\n",
    "    BATCH_SIZE, DEVICE, TEST_SPLIT, CLASS_NAMES, FEATURE_RESULTS_DIR, \n",
    "    PROCESSED_FEATURES_DIR, ENSEMBLE_OUTPUT_DIR, OPTUNA_TRIALS,\n",
    "    MAX_LENGTH, N_FFT, HOP_LENGTH, N_MELS, N_MFCC, N_CHROMA\n",
    ")\n",
    "\n",
    "# Funkcja do automatycznego konfigurowania ensemble modelu\n",
    "def auto_configure_ensemble(models_base_dir=None, ensemble_save_dir=None, best_models_only=True):\n",
    "    \"\"\"\n",
    "    Automatycznie konfiguruje model ensemble na podstawie dostępnych modeli.\n",
    "    \n",
    "    Args:\n",
    "        models_base_dir: Ścieżka bazowa do katalogu z modelami\n",
    "        ensemble_save_dir: Ścieżka do zapisu modelu ensemble\n",
    "        best_models_only: Czy używać tylko najlepszych modeli\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model_ensemble, feature_types)\n",
    "    \"\"\"\n",
    "    from helpers.ensemble_model import WeightedEnsembleModel\n",
    "    from helpers.utils import load_pretrained_model\n",
    "    from helpers.resnet_model_definition import AudioResNet\n",
    "    \n",
    "    # Jeśli nie podano ścieżek, użyj domyślnych\n",
    "    if models_base_dir is None:\n",
    "        models_base_dir = FEATURE_RESULTS_DIR\n",
    "    \n",
    "    if ensemble_save_dir is None:\n",
    "        ensemble_save_dir = ENSEMBLE_OUTPUT_DIR\n",
    "    \n",
    "    # Szukaj modeli dla różnych typów cech\n",
    "    feature_types = ['melspectrogram', 'mfcc', 'chroma']\n",
    "    model_paths = {}\n",
    "    models_dict = {}\n",
    "    \n",
    "    for feature_type in feature_types:\n",
    "        # Wzorzec dla najlepszych modeli\n",
    "        if best_models_only:\n",
    "            pattern = f\"{models_base_dir}/{feature_type}/best_model_{feature_type}_*.pt\"\n",
    "        else:\n",
    "            pattern = f\"{models_base_dir}/{feature_type}/model_{feature_type}_*.pt\"\n",
    "        \n",
    "        # Szukaj modeli pasujących do wzorca\n",
    "        model_files = glob.glob(pattern)\n",
    "        \n",
    "        if model_files:\n",
    "            # Wybierz najnowszy model (zakładając, że nazwy plików zawierają timestamp)\n",
    "            newest_model = sorted(model_files)[-1]\n",
    "            model_paths[feature_type] = newest_model\n",
    "            \n",
    "            # Załaduj model\n",
    "            print(f\"Ładowanie modelu dla {feature_type}: {os.path.basename(newest_model)}\")\n",
    "            model = load_pretrained_model(\n",
    "                model_path=newest_model,\n",
    "                model_class=AudioResNet,\n",
    "                num_classes=len(CLASS_NAMES),\n",
    "                device='cpu'  # Używanie CPU do eksportu\n",
    "            )\n",
    "            \n",
    "            if model is not None:\n",
    "                models_dict[feature_type] = model\n",
    "            else:\n",
    "                print(f\"Ostrzeżenie: Nie udało się załadować modelu dla {feature_type}\")\n",
    "    \n",
    "    # Sprawdź, czy znaleziono jakiekolwiek modele\n",
    "    if not models_dict:\n",
    "        raise ValueError(\"Nie znaleziono żadnych modeli! Sprawdź ścieżki do katalogów z modelami.\")\n",
    "    \n",
    "    # Utwórz model ensemble\n",
    "    ensemble_model = WeightedEnsembleModel(models_dict, weights=None)\n",
    "    \n",
    "    return ensemble_model, list(models_dict.keys())\n",
    "\n",
    "# Komórka 2: Załadowanie lub utworzenie modelu ensemble\n",
    "# Sprawdź, czy model ensemble jest już dostępny w sesji\n",
    "ensemble_model_loaded = None\n",
    "if 'ensemble_model' in locals() or 'ensemble_model' in globals():\n",
    "    print(\"Model ensemble już załadowany w sesji.\")\n",
    "    ensemble_model_loaded = ensemble_model\n",
    "else:\n",
    "    # Spróbuj znaleźć zapisany model ensemble\n",
    "    ensemble_model_path = None\n",
    "    ensemble_patterns = [\n",
    "        \"ensemble_outputs/*/models/best_ensemble_model.pt\",  # Najlepsze modele z Optuna\n",
    "        \"ensemble_outputs/*/models/ensemble_model.pt\",       # Modele bez optymalizacji\n",
    "        \"exported_models/*/best_ensemble_model.pt\",          # Eksportowane najlepsze modele\n",
    "        \"exported_models/*/ensemble_model.pt\"                # Eksportowane modele\n",
    "    ]\n",
    "    \n",
    "    for pattern in ensemble_patterns:\n",
    "        model_files = glob.glob(pattern)\n",
    "        if model_files:\n",
    "            # Sortuj według daty modyfikacji - najnowszy na końcu\n",
    "            ensemble_model_path = sorted(model_files, key=os.path.getmtime)[-1]\n",
    "            print(f\"Znaleziono model ensemble: {ensemble_model_path}\")\n",
    "            break\n",
    "    \n",
    "    # Jeśli znaleziono model, spróbuj go załadować\n",
    "    if ensemble_model_path:\n",
    "        try:\n",
    "            from helpers.ensemble_model import WeightedEnsembleModel\n",
    "            from helpers.resnet_model_definition import AudioResNet\n",
    "            \n",
    "            # Załaduj modele bazowe (potrzebne do rekonstrukcji)\n",
    "            _, feature_types = auto_configure_ensemble()\n",
    "            \n",
    "            # Utwórz słownik modeli bazowych\n",
    "            from helpers.utils import load_pretrained_model\n",
    "            models_dict = {}\n",
    "            for ft in feature_types:\n",
    "                model = AudioResNet(num_classes=len(CLASS_NAMES))\n",
    "                models_dict[ft] = model\n",
    "            \n",
    "            # Załaduj model ensemble\n",
    "            ensemble_model_loaded, _ = WeightedEnsembleModel.load(ensemble_model_path, models_dict)\n",
    "            print(f\"Model ensemble załadowany z {ensemble_model_path}\")\n",
    "            print(f\"Typy cech: {ensemble_model_loaded.feature_types}\")\n",
    "            print(f\"Wagi: {ensemble_model_loaded.get_weights()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Błąd podczas ładowania modelu ensemble: {e}\")\n",
    "            ensemble_model_loaded = None\n",
    "    \n",
    "    # Jeśli nie udało się załadować modelu, utwórz nowy\n",
    "    if ensemble_model_loaded is None:\n",
    "        try:\n",
    "            print(\"Tworzenie nowego modelu ensemble...\")\n",
    "            ensemble_model_loaded, feature_types = auto_configure_ensemble()\n",
    "            print(f\"Utworzono model ensemble z typami cech: {feature_types}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Błąd podczas tworzenia modelu ensemble: {e}\")\n",
    "            raise\n",
    "\n",
    "# Przypisz załadowany model do zmiennej ensemble_model dla dalszego użycia\n",
    "ensemble_model = ensemble_model_loaded\n",
    "print(f\"Model ensemble gotowy do eksportu. Typy cech: {ensemble_model.feature_types}\")\n",
    "\n",
    "# Komórka 3: Eksport modelu do ONNX\n",
    "# Utwórz katalog wyjściowy z timestampem\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = f\"exported_models/onnx_{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Katalog wyjściowy: {output_dir}\")\n",
    "\n",
    "# Konfiguracja eksportu i kwantyzacji\n",
    "quantize_model = True         # Czy kwantyzować model po eksporcie\n",
    "quantization_type = \"static\"  # Typ kwantyzacji (static/dynamic) - dla CNN zalecana static\n",
    "activation_type = \"int8\"      # Typ danych aktywacji (int8 dla modeli CNN)\n",
    "weight_type = \"int8\"          # Typ danych wag (int8 dla modeli CNN)\n",
    "quant_format = \"qdq\"          # Format kwantyzacji (qdq jest zalecane dla modeli CNN)\n",
    "per_channel = True            # Kwantyzacja per-channel dla lepszej dokładności\n",
    "\n",
    "# Przygotowanie danych kalibracyjnych dla kwantyzacji statycznej\n",
    "print(\"\\nPrzygotowywanie danych kalibracyjnych dla kwantyzacji statycznej...\")\n",
    "\n",
    "# Załadowanie próbek audio do kalibracji\n",
    "audio_samples = []\n",
    "num_samples = 10  # Liczba próbek audio do kalibracji\n",
    "\n",
    "# Szukaj plików audio w różnych katalogach\n",
    "audio_paths = []\n",
    "data_dirs = ['data/test', 'data/valid', 'data_processed/test', 'data_processed/valid']\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    if os.path.exists(data_dir):\n",
    "        for ext in ['.wav', '.mp3', '.ogg']:\n",
    "            audio_files = glob.glob(f\"{data_dir}/**/*{ext}\", recursive=True)\n",
    "            if audio_files:\n",
    "                audio_paths.extend(audio_files[:num_samples])\n",
    "                if len(audio_paths) >= num_samples:\n",
    "                    break\n",
    "        if len(audio_paths) >= num_samples:\n",
    "            break\n",
    "\n",
    "# Wczytaj pliki audio lub wygeneruj sztuczne dane, jeśli nie znaleziono\n",
    "if not audio_paths:\n",
    "    print(\"Nie znaleziono plików audio. Generowanie sztucznych danych...\")\n",
    "    \n",
    "    # Generowanie sztucznych danych\n",
    "    for i in range(num_samples):\n",
    "        t = np.linspace(0, MAX_LENGTH, int(22050 * MAX_LENGTH), endpoint=False)\n",
    "        audio = np.sin(2 * np.pi * 440 * t) * 0.3  # Ton A4 (440 Hz)\n",
    "        audio += np.sin(2 * np.pi * 880 * t) * 0.2  # Harmoniczna\n",
    "        audio += np.random.normal(0, 0.1, size=len(t))  # Szum\n",
    "        audio = audio / np.max(np.abs(audio))  # Normalizacja\n",
    "        audio_samples.append(audio)\n",
    "        print(f\"Wygenerowano sztuczną próbkę audio {i+1}/{num_samples}\")\n",
    "else:\n",
    "    # Ładowanie rzeczywistych plików audio\n",
    "    for i, path in enumerate(audio_paths[:num_samples]):\n",
    "        try:\n",
    "            audio, sr = librosa.load(path, sr=22050)\n",
    "            target_length = int(MAX_LENGTH * sr)\n",
    "            if len(audio) > target_length:\n",
    "                audio = audio[:target_length]\n",
    "            else:\n",
    "                audio = np.pad(audio, (0, max(0, target_length - len(audio))), mode='constant')\n",
    "            audio_samples.append(audio)\n",
    "            print(f\"Załadowano próbkę {i+1}/{len(audio_paths[:num_samples])}: {os.path.basename(path)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Błąd ładowania {path}: {e}\")\n",
    "\n",
    "# Utwórz czytnik danych kalibracyjnych\n",
    "feature_types = ensemble_model.feature_types  # Pobierz typy cech z modelu\n",
    "print(f\"Typy cech modelu: {feature_types}\")\n",
    "\n",
    "calibration_reader = EnsembleCalibrationDataReader(\n",
    "    audio_samples=audio_samples,\n",
    "    feature_types=feature_types,\n",
    "    sample_rate=22050\n",
    ")\n",
    "\n",
    "print(f\"Utworzono czytnik danych kalibracyjnych z {len(audio_samples)} próbkami audio\")\n",
    "\n",
    "print(\"\\nRozpoczynam eksport modelu...\")\n",
    "\n",
    "# Eksport modelu do formatu ONNX\n",
    "try:\n",
    "    export_result = export_model_to_onnx(\n",
    "        ensemble_model=ensemble_model,\n",
    "        output_dir=output_dir,\n",
    "        class_names=CLASS_NAMES,\n",
    "        export_params={\n",
    "            \"quantize_model\": quantize_model,\n",
    "            \"quantization_type\": quantization_type,\n",
    "            \"activation_type\": activation_type,\n",
    "            \"weight_type\": weight_type,\n",
    "            \"quant_format\": quant_format,\n",
    "            \"per_channel\": per_channel,\n",
    "            \"reduce_range\": False,  # True tylko dla starszych CPU bez wspierania instrukcji VNNI\n",
    "            \"use_cuda\": torch.cuda.is_available(),\n",
    "            \"opset_version\": 17  # Używaj najnowszej wersji opset\n",
    "        },\n",
    "        calibration_data_reader=calibration_reader  # Przekazanie czytnika danych kalibracyjnych\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Błąd podczas eksportu modelu: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "if export_result.get(\"success\", False):\n",
    "    print(\"\\nEksport modelu ensemble do ONNX zakończony sukcesem!\")\n",
    "    print(f\"Model ONNX zapisany w: {export_result['onnx_path']}\")\n",
    "    print(f\"Zoptymalizowany model ONNX zapisany w: {export_result['optimized_onnx_path']}\")\n",
    "    \n",
    "    # Rozmiar modelu\n",
    "    original_size_mb = export_result['sizes_mb'].get('original', 0)\n",
    "    optimized_size_mb = export_result['sizes_mb'].get('optimized', 0)\n",
    "    print(f\"Rozmiar oryginalnego modelu: {original_size_mb:.2f} MB\")\n",
    "    print(f\"Rozmiar zoptymalizowanego modelu: {optimized_size_mb:.2f} MB\")\n",
    "    print(f\"Redukcja rozmiaru po optymalizacji: {export_result.get('optimization_reduction', 0):.2f}%\")\n",
    "    \n",
    "    # Sprawdź czy kwantyzacja została wykonana\n",
    "    if \"quantized_onnx_path\" in export_result and export_result['quantized_onnx_path'] and os.path.exists(export_result['quantized_onnx_path']):\n",
    "        quantized_size_mb = export_result['sizes_mb'].get('quantized', 0)\n",
    "        print(f\"Skwantyzowany model ONNX zapisany w: {export_result['quantized_onnx_path']}\")\n",
    "        print(f\"Rozmiar skwantyzowanego modelu: {quantized_size_mb:.2f} MB\")\n",
    "        print(f\"Redukcja rozmiaru po kwantyzacji: {export_result.get('quantization_reduction_vs_source', 0):.2f}%\")\n",
    "        \n",
    "        # Jeśli użyto kwantyzacji statycznej, pokaż szczegóły\n",
    "        if export_result.get(\"quantization_type_used\") == \"static\":\n",
    "            print(f\"\\nSzczegóły kwantyzacji statycznej:\")\n",
    "            if \"quantization_params_used\" in export_result:\n",
    "                params = export_result[\"quantization_params_used\"] \n",
    "                print(f\"- Używana parametryzacja: {params}\")\n",
    "                if \"op_types_to_quantize\" in params and params[\"op_types_to_quantize\"]:\n",
    "                    print(f\"- Kwantyzowane operacje: {params['op_types_to_quantize']}\")\n",
    "    else:\n",
    "        print(\"Kwantyzacja nie została przeprowadzona lub nie powiodła się.\")\n",
    "        if \"quantization_error\" in export_result:\n",
    "            print(f\"Błąd kwantyzacji: {export_result['quantization_error']}\")\n",
    "    \n",
    "    # Informacje o metadanych i weryfikacji\n",
    "    print(f\"Metadane modelu zapisane w: {export_result.get('metadata_path', 'N/A')}\")\n",
    "    \n",
    "    # Wyświetl wyniki weryfikacji\n",
    "    if \"verification_result\" in export_result:\n",
    "        result = export_result[\"verification_result\"]\n",
    "        if result.get(\"success\", False):\n",
    "            print(\"\\nWeryfikacja modelu zakończona powodzeniem\")\n",
    "            if \"max_diff\" in result:\n",
    "                print(f\"Maksymalna różnica między PyTorch i ONNX: {result['max_diff']:.6f}\")\n",
    "            if \"has_warning\" in result and result[\"has_warning\"]:\n",
    "                print(f\"Ostrzeżenie: {result.get('warning', 'Wykryto różnice, ale mieszczą się w tolerancji')}\")\n",
    "        else:\n",
    "            print(f\"\\nWeryfikacja modelu nie powiodła się: {result.get('error', 'Nieznany błąd')}\")\n",
    "else:\n",
    "    print(\"\\nEksport modelu się nie powiódł\")\n",
    "    print(f\"Błąd: {export_result.get('error', 'Nieznany błąd')}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Używane urządzenie: cpu\n",
      "Wersja PyTorch: 2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# Import funkcji eksportu\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import inspect\n",
    "from datetime import datetime\n",
    "from export_scripts.export_onnx_notebook_helper import export_model_to_onnx\n",
    "from config import BATCH_SIZE, DEVICE,TEST_SPLIT, CLASS_NAMES,FEATURE_RESULTS_DIR, PROCESSED_FEATURES_DIR, ENSEMBLE_OUTPUT_DIR, OPTUNA_TRIALS, OPTUNA_TIMEOUT, CV_FOLDS, SEED\n",
    "from helpers.resnet_model_definition import AudioResNet\n",
    "from helpers.ensemble_trainer import EnsembleModelTrainer\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Używane urządzenie: {DEVICE}\")\n",
    "print(f\"Wersja PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_model(feature_type):\n",
    "    \"\"\"Zwraca najnowszy model dla danej reprezentacji audio.\"\"\"\n",
    "    search_pattern = f\"{FEATURE_RESULTS_DIR}/{feature_type}/best_model_{feature_type}_*.pt\"\n",
    "    model_files = glob.glob(search_pattern)\n",
    "    \n",
    "    if not model_files:\n",
    "        return None\n",
    "    \n",
    "    # Sortowanie plików według daty w nazwie (najnowszy na końcu)\n",
    "    model_files.sort(key=lambda x: os.path.basename(x).split('_')[-1].split('.')[0])\n",
    "    return model_files[-1]\n",
    "\n",
    "def find_feature_file(feature_type):\n",
    "    \"\"\"Zwraca plik z cechami dla danej reprezentacji audio.\"\"\"\n",
    "    search_pattern = f\"{PROCESSED_FEATURES_DIR}/{feature_type}_*.pkl\"\n",
    "    feature_files = glob.glob(search_pattern)\n",
    "    \n",
    "    if not feature_files:\n",
    "        return None\n",
    "    \n",
    "    # W przypadku wielu plików, wybór najnowszego\n",
    "    if len(feature_files) > 1:\n",
    "        feature_files.sort(key=os.path.getmtime, reverse=True)\n",
    "    \n",
    "    return feature_files[0]\n",
    "\n",
    "def auto_configure_ensemble(feature_types=None):\n",
    "    \"\"\"\n",
    "    Konfiguracja modelu ensemble w sposób automatyczny.\n",
    "    \"\"\"\n",
    "    # Wykorzystanie zmiennych z config.py\n",
    "    if feature_types is None:\n",
    "        try:\n",
    "            feature_types = [d for d in os.listdir(FEATURE_RESULTS_DIR) \n",
    "                             if os.path.isdir(os.path.join(FEATURE_RESULTS_DIR, d))]\n",
    "        except FileNotFoundError:\n",
    "             raise FileNotFoundError(f\"Nie znaleziono katalogu '{FEATURE_RESULTS_DIR}'.\")\n",
    "    \n",
    "    print(f\"Typy cech wybrane do modelu ensemble: {', '.join(feature_types)}\")\n",
    "\n",
    "    model_paths = {}\n",
    "    feature_files = {}\n",
    "    initial_weights = {}\n",
    "    missing_feature_files = []\n",
    "    \n",
    "    for feature_type in feature_types:\n",
    "        # Weryfikacja pliku cech\n",
    "        feature_file = find_feature_file(feature_type)\n",
    "        if not feature_file:\n",
    "            missing_feature_files.append(feature_type)\n",
    "            print(f\"Ostrzeżenie: Brak pliku cech dla {feature_type}\")\n",
    "            continue # Kontynuacja do następnego typu, bez szukania modelu, gdy brak cech\n",
    "        \n",
    "        feature_files[feature_type] = feature_file\n",
    "        \n",
    "        # Wyszukiwanie modelu\n",
    "        model_path = find_latest_model(feature_type)\n",
    "        if model_path:\n",
    "            model_paths[feature_type] = model_path\n",
    "            initial_weights[feature_type] = 1.0 # Waga do normalizacji w późniejszym etapie\n",
    "        else:\n",
    "            print(f\"Ostrzeżenie: Brak modelu dla {feature_type}, mimo istnienia pliku cech.\")\n",
    "            # Decyzja o kontynuacji bez tego modelu lub zgłoszenie błędu.\n",
    "            # Na razie pominięcie tego typu cechy w modelu.\n",
    "            del feature_files[feature_type] # Usunięcie wpisu o pliku cech, gdy brak modelu\n",
    "            \n",
    "    # W przypadku brakujących plików cech, zgłoszenie błędu\n",
    "    if missing_feature_files:\n",
    "        error_message = (\n",
    "            f\"Błąd: Brak plików cech dla: {', '.join(missing_feature_files)}. \"\n",
    "            f\"Zaleca się uruchomienie funkcji `process_dataset` w pliku `ResNet_porównanie.ipynb`, \"\n",
    "            f\"aby wygenerować brakujące pliki w folderze `processed_features`.\"\n",
    "        )\n",
    "        raise FileNotFoundError(error_message)\n",
    "        \n",
    "    # Weryfikacja, czy znaleziono jakiekolwiek działające pary model-cechy\n",
    "    if not model_paths:\n",
    "        raise ValueError(\"Brak działających modeli i odpowiadających im plików cech! \"\n",
    "                         \"Zaleca się sprawdzenie folderów 'feature_comparison_results' i 'processed_features'.\")\n",
    "\n",
    "    # Normalizacja wag, aby sumowały się do 1\n",
    "    num_features = len(model_paths)\n",
    "    initial_weights = {k: 1.0 / num_features for k in model_paths.keys()}\n",
    "    \n",
    "    # Tworzenie konfiguracji\n",
    "    config = {\n",
    "        \"model_paths\": model_paths,\n",
    "        \"model_class\": AudioResNet,\n",
    "        \"feature_files\": feature_files,\n",
    "        \"initial_weights\": initial_weights,\n",
    "        \"output_dir\": ENSEMBLE_OUTPUT_DIR,\n",
    "        \"experiment_name\": f\"ensemble_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        \"test_split\": TEST_SPLIT,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"optimization\": {\n",
    "            \"n_trials\": OPTUNA_TRIALS,\n",
    "            \"timeout\": OPTUNA_TIMEOUT,\n",
    "            \"n_folds\": CV_FOLDS\n",
    "        },\n",
    "        \"class_names\": CLASS_NAMES,\n",
    "        \"random_seed\": SEED\n",
    "    }\n",
    "    \n",
    "    \n",
    "    print(f\"\\nZnaleziono {len(model_paths)} działających par model-cechy:\")\n",
    "    for feat in model_paths.keys():\n",
    "        print(f\"  - {feat}\")\n",
    "    \n",
    "    return config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typy cech wybrane do modelu ensemble: melspectrogram, mfcc, chroma\n",
      "\n",
      "Znaleziono 3 działających par model-cechy:\n",
      "  - melspectrogram\n",
      "  - mfcc\n",
      "  - chroma\n"
     ]
    }
   ],
   "source": [
    "selected_features = [\"melspectrogram\", \"mfcc\", \"chroma\"]\n",
    "\n",
    "CONFIG = auto_configure_ensemble(feature_types=selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Próba załadowania modelu dla cechy 'melspectrogram' z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/melspectrogram\\best_model_melspectrogram_20250517_001727.pt\n",
      "Model załadowany pomyślnie z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/melspectrogram\\best_model_melspectrogram_20250517_001727.pt\n",
      "Pomyślnie załadowano model dla cechy 'melspectrogram'\n",
      "Próba załadowania modelu dla cechy 'mfcc' z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/mfcc\\best_model_mfcc_20250517_004119.pt\n",
      "Model załadowany pomyślnie z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/mfcc\\best_model_mfcc_20250517_004119.pt\n",
      "Pomyślnie załadowano model dla cechy 'mfcc'\n",
      "Próba załadowania modelu dla cechy 'chroma' z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/chroma\\best_model_chroma_20250517_005304.pt\n",
      "Model załadowany pomyślnie z c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\feature_comparison_results/chroma\\best_model_chroma_20250517_005304.pt\n",
      "Pomyślnie załadowano model dla cechy 'chroma'\n",
      "\n",
      "Podsumowanie ładowania modeli:\n",
      "- Załadowane modele: 3/3\n",
      "- Niepowodzenia: 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/19 09:20:51 INFO mlflow.tracking.fluent: Experiment with name 'ensemble_optimization_20250519_092050' does not exist. Creating a new experiment.\n",
      "[I 2025-05-19 09:20:51,076] A new study created in memory with name: ensemble_optimization_20250519_092050\n",
      "[I 2025-05-19 09:21:16,387] Trial 0 finished with value: 0.98837749883775 and parameters: {'melspectrogram': 0.3745401188473625, 'mfcc': 0.9507143064099162, 'chroma': 0.7319939418114051}. Best is trial 0 with value: 0.98837749883775.\n",
      "[I 2025-05-19 09:21:40,063] Trial 1 finished with value: 0.9869827986982799 and parameters: {'melspectrogram': 0.5986584841970366, 'mfcc': 0.15601864044243652, 'chroma': 0.15599452033620265}. Best is trial 0 with value: 0.98837749883775.\n",
      "[I 2025-05-19 09:22:02,756] Trial 2 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.05808361216819946, 'mfcc': 0.8661761457749352, 'chroma': 0.6011150117432088}. Best is trial 2 with value: 0.9888423988842399.\n",
      "[I 2025-05-19 09:22:25,031] Trial 3 finished with value: 0.9860529986052998 and parameters: {'melspectrogram': 0.7080725777960455, 'mfcc': 0.020584494295802447, 'chroma': 0.9699098521619943}. Best is trial 2 with value: 0.9888423988842399.\n",
      "[I 2025-05-19 09:22:46,974] Trial 4 finished with value: 0.9869827986982799 and parameters: {'melspectrogram': 0.8324426408004217, 'mfcc': 0.21233911067827616, 'chroma': 0.18182496720710062}. Best is trial 2 with value: 0.9888423988842399.\n",
      "[I 2025-05-19 09:23:08,765] Trial 5 finished with value: 0.9865178986517898 and parameters: {'melspectrogram': 0.18340450985343382, 'mfcc': 0.3042422429595377, 'chroma': 0.5247564316322378}. Best is trial 2 with value: 0.9888423988842399.\n",
      "[I 2025-05-19 09:23:30,822] Trial 6 finished with value: 0.9865178986517898 and parameters: {'melspectrogram': 0.43194501864211576, 'mfcc': 0.2912291401980419, 'chroma': 0.6118528947223795}. Best is trial 2 with value: 0.9888423988842399.\n",
      "[I 2025-05-19 09:23:52,232] Trial 7 finished with value: 0.9869827986982799 and parameters: {'melspectrogram': 0.13949386065204183, 'mfcc': 0.29214464853521815, 'chroma': 0.3663618432936917}. Best is trial 2 with value: 0.9888423988842399.\n",
      "[I 2025-05-19 09:24:13,722] Trial 8 finished with value: 0.9893072989307298 and parameters: {'melspectrogram': 0.45606998421703593, 'mfcc': 0.7851759613930136, 'chroma': 0.19967378215835974}. Best is trial 8 with value: 0.9893072989307298.\n",
      "[I 2025-05-19 09:24:35,474] Trial 9 finished with value: 0.98930729893073 and parameters: {'melspectrogram': 0.5142344384136116, 'mfcc': 0.5924145688620425, 'chroma': 0.046450412719997725}. Best is trial 9 with value: 0.98930729893073.\n",
      "[I 2025-05-19 09:24:57,391] Trial 10 finished with value: 0.9869827986982798 and parameters: {'melspectrogram': 0.9451365442303868, 'mfcc': 0.6499487225623292, 'chroma': 0.007547319316293266}. Best is trial 9 with value: 0.98930729893073.\n",
      "[I 2025-05-19 09:25:19,325] Trial 11 finished with value: 0.98837749883775 and parameters: {'melspectrogram': 0.3468508914066219, 'mfcc': 0.6400885024675564, 'chroma': 0.2842687426077016}. Best is trial 9 with value: 0.98930729893073.\n",
      "[I 2025-05-19 09:25:43,126] Trial 12 finished with value: 0.9897721989772199 and parameters: {'melspectrogram': 0.5798511290004849, 'mfcc': 0.7471522741026703, 'chroma': 0.038579466065655904}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:26:07,281] Trial 13 finished with value: 0.9874476987447699 and parameters: {'melspectrogram': 0.6437127366238236, 'mfcc': 0.5045490626558228, 'chroma': 0.008686035996669186}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:26:30,909] Trial 14 finished with value: 0.9874476987447699 and parameters: {'melspectrogram': 0.5705159860373523, 'mfcc': 0.5356242640354899, 'chroma': 0.3826742869365834}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:26:52,480] Trial 15 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.7546466097724528, 'mfcc': 0.7331265877657684, 'chroma': 0.10611651328702915}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:27:13,976] Trial 16 finished with value: 0.9879125987912598 and parameters: {'melspectrogram': 0.26699107294536967, 'mfcc': 0.4276572368943822, 'chroma': 0.3419447044306994}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:27:35,203] Trial 17 finished with value: 0.9879125987912598 and parameters: {'melspectrogram': 0.5277788930300321, 'mfcc': 0.99234106721687, 'chroma': 0.8718207898963263}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:27:56,614] Trial 18 finished with value: 0.9869827986982798 and parameters: {'melspectrogram': 0.8558487731576202, 'mfcc': 0.6245714188726499, 'chroma': 0.05232843533684121}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:28:17,700] Trial 19 finished with value: 0.9879125987912598 and parameters: {'melspectrogram': 0.9960165317474355, 'mfcc': 0.8456422826767468, 'chroma': 0.24666513588105113}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:28:39,153] Trial 20 finished with value: 0.9865178986517898 and parameters: {'melspectrogram': 0.6872707510679238, 'mfcc': 0.41692460832750067, 'chroma': 0.09734341337512589}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:29:00,828] Trial 21 finished with value: 0.9897721989772199 and parameters: {'melspectrogram': 0.4778919246561321, 'mfcc': 0.756396774553744, 'chroma': 0.20191449549975196}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:29:22,444] Trial 22 finished with value: 0.9897721989772199 and parameters: {'melspectrogram': 0.5017273607245788, 'mfcc': 0.7329569200275828, 'chroma': 0.10475712837705971}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:29:43,815] Trial 23 finished with value: 0.98837749883775 and parameters: {'melspectrogram': 0.3729015078077597, 'mfcc': 0.7290790657918866, 'chroma': 0.2660155847271595}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:30:06,279] Trial 24 finished with value: 0.9893072989307298 and parameters: {'melspectrogram': 0.2737025157820342, 'mfcc': 0.8742124071096369, 'chroma': 0.46089071411007027}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:30:27,557] Trial 25 finished with value: 0.9893072989307298 and parameters: {'melspectrogram': 0.4630659134973512, 'mfcc': 0.7536097643611694, 'chroma': 0.13621237457538082}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:30:49,032] Trial 26 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.5932022372608293, 'mfcc': 0.6895222284882294, 'chroma': 0.22970648316374825}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:31:11,149] Trial 27 finished with value: 0.98930729893073 and parameters: {'melspectrogram': 0.7892622647663816, 'mfcc': 0.8264347993597301, 'chroma': 0.10484006689960411}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:31:32,636] Trial 28 finished with value: 0.98930729893073 and parameters: {'melspectrogram': 0.6596908511602413, 'mfcc': 0.9143852059760643, 'chroma': 0.31176576548124896}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:31:54,677] Trial 29 finished with value: 0.98837749883775 and parameters: {'melspectrogram': 0.31999316860971216, 'mfcc': 0.9385679238579697, 'chroma': 0.7509972360125259}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:32:17,801] Trial 30 finished with value: 0.9874476987447699 and parameters: {'melspectrogram': 0.4122589336189653, 'mfcc': 0.5685573638058781, 'chroma': 0.413655361820489}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:32:41,312] Trial 31 finished with value: 0.98930729893073 and parameters: {'melspectrogram': 0.5305097326278353, 'mfcc': 0.5942965120944094, 'chroma': 0.06389988727608029}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:33:03,730] Trial 32 finished with value: 0.9897721989772199 and parameters: {'melspectrogram': 0.5030067096806224, 'mfcc': 0.68917920056482, 'chroma': 0.17491981391369707}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:33:25,752] Trial 33 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.6115941925609283, 'mfcc': 0.7119071040856556, 'chroma': 0.17508504057930907}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:33:47,835] Trial 34 finished with value: 0.9893072989307298 and parameters: {'melspectrogram': 0.4708328009670898, 'mfcc': 0.7958677618935732, 'chroma': 0.1576939450229779}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:34:09,995] Trial 35 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.5646669175404117, 'mfcc': 0.6785710272178809, 'chroma': 0.1953214984501455}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:34:32,042] Trial 36 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.40360763844928266, 'mfcc': 0.4599447911421645, 'chroma': 0.1335498979442138}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:34:53,477] Trial 37 finished with value: 0.9897721989772199 and parameters: {'melspectrogram': 0.49754079334710943, 'mfcc': 0.8982982668189167, 'chroma': 0.00875174577268236}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:35:15,895] Trial 38 finished with value: 0.9879125987912598 and parameters: {'melspectrogram': 0.7284137842452189, 'mfcc': 0.7924630487580581, 'chroma': 0.5038725655174225}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:35:37,857] Trial 39 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.20418298636039556, 'mfcc': 0.9974884302911802, 'chroma': 0.21504625160546406}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:35:59,633] Trial 40 finished with value: 0.9855880985588099 and parameters: {'melspectrogram': 0.0853853689703643, 'mfcc': 0.08188115492492543, 'chroma': 0.3240450676768639}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:36:21,421] Trial 41 finished with value: 0.9897721989772199 and parameters: {'melspectrogram': 0.49156772452172864, 'mfcc': 0.9017041232840802, 'chroma': 0.06272718335414926}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:36:43,390] Trial 42 finished with value: 0.9893072989307298 and parameters: {'melspectrogram': 0.6312347543994457, 'mfcc': 0.8565586822871205, 'chroma': 0.012001176733061364}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:37:05,255] Trial 43 finished with value: 0.9897721989772199 and parameters: {'melspectrogram': 0.5467907797928104, 'mfcc': 0.7646135721885478, 'chroma': 0.09563836937174355}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:37:26,580] Trial 44 finished with value: 0.98837749883775 and parameters: {'melspectrogram': 0.4267653053415788, 'mfcc': 0.8114845261832093, 'chroma': 0.6615521562826412}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:37:47,813] Trial 45 finished with value: 0.9893072989307298 and parameters: {'melspectrogram': 0.4958154794110836, 'mfcc': 0.6614421325221612, 'chroma': 0.0077022313190932695}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:38:08,977] Trial 46 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.33241656289980087, 'mfcc': 0.8916281380239106, 'chroma': 0.15717080380913523}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:38:30,608] Trial 47 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.4438539110833267, 'mfcc': 0.9515011447558825, 'chroma': 0.06564892429148818}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:38:51,744] Trial 48 finished with value: 0.98930729893073 and parameters: {'melspectrogram': 0.5823910633819078, 'mfcc': 0.7029081550434996, 'chroma': 0.11946319671139426}. Best is trial 12 with value: 0.9897721989772199.\n",
      "[I 2025-05-19 09:39:13,131] Trial 49 finished with value: 0.9888423988842399 and parameters: {'melspectrogram': 0.374526641979816, 'mfcc': 0.7661062427862346, 'chroma': 0.27357227878644863}. Best is trial 12 with value: 0.9897721989772199.\n",
      "c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\helpers\\ensemble_trainer.py:304: ExperimentalWarning: plot_optimization_history is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna.visualization.matplotlib.plot_optimization_history(study)\n",
      "c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\helpers\\ensemble_trainer.py:314: ExperimentalWarning: plot_param_importances is experimental (supported from v2.2.0). The interface can change in the future.\n",
      "  optuna.visualization.matplotlib.plot_param_importances(study)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model ensemble i konfiguracja znalezione!\n",
      "Katalog wyjściowy: exported_models/onnx_20250519_093924\n",
      "\n",
      "Rozpoczynam eksport modelu...\n",
      "Katalog wyjściowy: exported_models/onnx_20250519_093924\n",
      "\n",
      "Generowanie przykładowych danych wejściowych...\n",
      "Wymiary wejść:\n",
      " - mel_input:   torch.Size([1, 1, 128, 130])\n",
      " - mfcc_input:  torch.Size([1, 1, 40, 130])\n",
      " - chroma_input: torch.Size([1, 1, 12, 130])\n",
      "\n",
      "--- Eksport modelu do formatu ONNX ---\n",
      "Rozpoczynam eksport modelu ensemble do formatu ONNX: exported_models/onnx_20250519_093924\\ensemble_model.onnx\n",
      "Test forward pass zakończony sukcesem. Kształt wyjścia: torch.Size([1, 6])\n",
      "Wymiary wejść:\n",
      " - mel_input:   torch.Size([1, 1, 128, 130])\n",
      " - mfcc_input:  torch.Size([1, 1, 40, 130])\n",
      " - chroma_input: torch.Size([1, 1, 12, 130])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\.venv\\lib\\site-packages\\onnxscript\\converter.py:816: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\.venv\\lib\\site-packages\\onnxscript\\converter.py:816: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\export_scripts\\ensemble_onnx_wrapper.py:97: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `EnsembleONNXWrapper([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `EnsembleONNXWrapper([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\.venv\\lib\\site-packages\\torch\\export\\_unlift.py:81: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer\n",
      "  getattr_node = gm.graph.get_attr(lifted_node)\n",
      "c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\.venv\\lib\\site-packages\\torch\\fx\\graph.py:1772: UserWarning: Node ensemble_lifted_tensor_0 target ensemble.lifted_tensor_0 lifted_tensor_0 of ensemble does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conversion to opset < 18 is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 3 of general pattern rewrite rules.\n",
      "Eksport do ONNX zakończony pomyślnie!\n",
      "Weryfikacja modelu ONNX...\n",
      "✓ Model ONNX poprawny!\n",
      "Rozmiar modelu: 0.49 MB\n",
      "Liczba węzłów grafu: 226\n",
      "Liczba inicjalizatorów: 366\n",
      "Model wyeksportowany do: exported_models/onnx_20250519_093924\\ensemble_model.onnx\n",
      "\n",
      "--- Optymalizacja modelu ONNX ---\n",
      "Rozpoczęcie optymalizacji modelu ONNX: exported_models/onnx_20250519_093924\\ensemble_model.onnx\n",
      "Utworzono kopię zapasową oryginalnego modelu: exported_models/onnx_20250519_093924\\ensemble_model.onnx.backup\n",
      "Rozmiar oryginalnego modelu: 0.49 MB\n",
      "Używanie bezpiecznych optymalizacji...\n",
      "Model po optymalizacji poprawny.\n",
      "Ostrzeżenie: Zoptymalizowany model jest znacznie większy (128.08 MB) niż oryginalny (0.49 MB). Używanie oryginału.\n",
      "\n",
      "--- Kwantyzacja modelu ONNX ---\n",
      "Rozpoczęcie kwantyzacji modelu ONNX: exported_models/onnx_20250519_093924\\ensemble_model_optimized.onnx\n",
      "Model ONNX poprawny przed kwantyzacją.\n",
      "Shape inference przeprowadzone pomyślnie.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Użycie kwantyzacji do unsigned int8\n",
      "Próba kwantyzacji podstawowej...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podstawowa kwantyzacja nie powiodła się: [ShapeInferenceError] Inferred shape and existing shape differ in dimension 0: (512) vs (6)\n",
      "Próba kwantyzacji z pominięciem problematycznych operatorów...\n",
      "Pominięcie operatorów: ['Reshape', 'Transpose', 'Concat', 'Slice', 'Squeeze', 'Unsqueeze']\n",
      "Również nie powiodła się kwantyzacja z pominięciem operatorów: quantize_dynamic() got an unexpected keyword argument 'op_types_to_exclude'\n",
      "Próba ostatecznej, minimalnej kwantyzacji...\n",
      "Kwantyzacja tylko operatorów: ['Conv', 'MatMul']\n",
      "Wszystkie próby kwantyzacji nieudane: [ShapeInferenceError] Inferred shape and existing shape differ in dimension 0: (512) vs (6)\n",
      "Ostrzeżenie: Kwantyzacja modelu nie powiodła się: Wszystkie próby kwantyzacji nieudane: [ShapeInferenceError] Inferred shape and existing shape differ in dimension 0: (512) vs (6)\n",
      "Kontynuowanie bez kwantyzacji.\n",
      "\n",
      "--- Weryfikacja modelu ONNX ---\n",
      "Dostępni dostawcy ONNX Runtime: ['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Używam CPU do inferencji ONNX (wersja ONNX Runtime: 1.19.2)\n",
      "Używam 8 wątków CPU do inferencji\n",
      "Utworzono sesję ONNX Runtime dla modelu: exported_models/onnx_20250519_093924\\ensemble_model_optimized.onnx\n",
      "Użyte dostawce (providers): ['CPUExecutionProvider']\n",
      "Oczekiwane wejścia:\n",
      "  - mel_input: [1, 1, 128, 's0'] (tensor(float))\n",
      "  - mfcc_input: [1, 1, 40, 's1'] (tensor(float))\n",
      "  - chroma_input: [1, 1, 12, 's2'] (tensor(float))\n",
      "Wyjścia modelu:\n",
      "  - output: [1, 6] (tensor(float))\n",
      "Inicjalizacja sesji ONNX Runtime...\n",
      "Sesja ONNX Runtime zainicjalizowana pomyślnie.\n",
      "Oczekiwane wejścia ONNX: ['mel_input', 'mfcc_input', 'chroma_input']\n",
      "Uruchamianie inferencji ONNX...\n",
      "Kształt wyjścia ONNX: (1, 6)\n",
      "Porównywanie wyjść modeli PyTorch i ONNX...\n",
      "Statystyki różnic między wyjściami PyTorch i ONNX:\n",
      "  - Maksymalna różnica: 0.00000001\n",
      "  - Średnia różnica: 0.00000000\n",
      "  - Mediana różnic: 0.00000000\n",
      "  - Odchylenie std. różnic: 0.00000001\n",
      "  - Użyta wartość progowa: 0.00078944\n",
      "Weryfikacja ONNX zakończona pomyślnie.\n",
      "Weryfikacja modelu zakończona powodzeniem\n",
      "Maksymalna różnica między PyTorch i ONNX: 0.000000\n",
      "\n",
      "Metadane modelu zapisane w: exported_models/onnx_20250519_093924\\ensemble_model_metadata.json\n",
      "\n",
      "Eksport modelu ensemble do ONNX zakończony sukcesem!\n",
      "Model ONNX zapisany w: exported_models/onnx_20250519_093924\\ensemble_model.onnx\n",
      "Zoptymalizowany model ONNX zapisany w: exported_models/onnx_20250519_093924\\ensemble_model_optimized.onnx\n",
      "Rozmiar oryginalnego modelu: 0.49 MB\n",
      "Rozmiar zoptymalizowanego modelu: 0.49 MB\n",
      "Redukcja rozmiaru po optymalizacji: 0.00%\n",
      "Kwantyzacja nie została przeprowadzona w ramach procesu eksportu.\n",
      "Metadane modelu zapisane w: exported_models/onnx_20250519_093924\\ensemble_model_metadata.json\n",
      "\n",
      "Weryfikacja modelu zakończona powodzeniem\n",
      "Maksymalna różnica między PyTorch i ONNX: 0.000000\n",
      "\n",
      "Aby użyć wyeksportowanego modelu ONNX:\n",
      "1. Zainstaluj onnxruntime: pip install onnxruntime\n",
      "2. Do inferencji na GPU zainstaluj: pip install onnxruntime-gpu\n",
      "3. Skorzystaj z przykładowego kodu w pliku example_usage.py\n",
      "\n",
      "Eksport modelu ensemble do ONNX zakończony sukcesem!\n",
      "Model ONNX zapisany w: exported_models/onnx_20250519_093924\\ensemble_model.onnx\n",
      "Zoptymalizowany model ONNX zapisany w: exported_models/onnx_20250519_093924\\ensemble_model_optimized.onnx\n",
      "Rozmiar oryginalnego modelu: 0.49 MB\n",
      "Rozmiar zoptymalizowanego modelu: 0.49 MB\n",
      "Redukcja rozmiaru po optymalizacji: 0.00%\n",
      "Kwantyzacja nie została przeprowadzona w ramach procesu eksportu.\n",
      "Metadane modelu zapisane w: exported_models/onnx_20250519_093924\\ensemble_model_metadata.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Komórka odpowiedzialna za optymalizację wag modelu, w której definiowane są parametry optymalizacji\n",
    "n_trials = CONFIG[\"optimization\"][\"n_trials\"]\n",
    "timeout = CONFIG[\"optimization\"][\"timeout\"]\n",
    "n_folds = CONFIG[\"optimization\"][\"n_folds\"]\n",
    "test_size = CONFIG[\"test_split\"]\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# Wykorzystanie podstawowego output_dir z konfiguracji\n",
    "base_output_dir = CONFIG.get(\"output_dir\", \"ensemble_outputs\") \n",
    "output_dir = os.path.join(base_output_dir, f\"ensemble_run_{timestamp}\")\n",
    "\n",
    "trainer = EnsembleModelTrainer(\n",
    "    model_paths=CONFIG[\"model_paths\"],\n",
    "    feature_files=CONFIG[\"feature_files\"],\n",
    "    model_class=CONFIG[\"model_class\"],\n",
    "    output_dir=output_dir\n",
    ")\n",
    "# custom_weights = {\n",
    "# \"chroma\": 0.13857384163072003,\n",
    "# \"melspectrogram\": 0.31651310994496806,\n",
    "# \"mfcc\": 0.544913048424312\n",
    "# }\n",
    "best_score, best_weights = trainer.optimize_weights(\n",
    "    n_trials=n_trials,\n",
    "    timeout=timeout,\n",
    "    n_folds=n_folds,\n",
    "    test_size=test_size\n",
    ")\n",
    "\n",
    "weights_to_use = best_weights\n",
    "ensemble_model, test_results = trainer.train_and_evaluate(\n",
    "    weights=weights_to_use,\n",
    "    test_size=CONFIG[\"test_split\"],\n",
    "    batch_size=CONFIG[\"batch_size\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sprawdź czy model i konfiguracja są dostępne\n",
    "if 'ensemble_model' in locals() and 'CONFIG' in locals():\n",
    "    print(\"\\nModel ensemble i konfiguracja znalezione!\")\n",
    "    \n",
    "    # Utwórz katalog wyjściowy z timestampem\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = f\"exported_models/onnx_{timestamp}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Katalog wyjściowy: {output_dir}\")\n",
    "    \n",
    "    # Konfiguracja kwantyzacji\n",
    "    quantize_model = True         # Czy kwantyzować model po eksporcie\n",
    "    quantization_type = \"dynamic\" # Typ kwantyzacji \n",
    "    quantization_dtype = \"uint8\"  # Typ danych kwantyzacji\n",
    "    \n",
    "    print(\"\\nRozpoczynam eksport modelu...\")\n",
    "    \n",
    "    # Eksport modelu do formatu ONNX\n",
    "    try:\n",
    "        export_result = export_model_to_onnx(\n",
    "            ensemble_model=ensemble_model,\n",
    "            output_dir=output_dir,\n",
    "            class_names=CONFIG.get(\"class_names\"),\n",
    "            export_params={\n",
    "                \"quantize_model\": quantize_model,\n",
    "                \"quantization_type\": quantization_type,\n",
    "                \"quantization_dtype\": quantization_dtype,\n",
    "                \"use_cuda\": device.type == 'cuda'\n",
    "            }\n",
    "        )\n",
    "    except TypeError as e:\n",
    "        print(f\"Błąd: {e}\")\n",
    "        try:\n",
    "            sig = inspect.signature(export_model_to_onnx)\n",
    "            print(f\"Oczekiwane parametry: {list(sig.parameters.keys())}\")\n",
    "        except:\n",
    "            print(\"Nie udało się uzyskać parametrów funkcji.\")\n",
    "        raise\n",
    "    \n",
    "    if export_result.get(\"success\", False):\n",
    "        print(\"\\nEksport modelu ensemble do ONNX zakończony sukcesem!\")\n",
    "        print(f\"Model ONNX zapisany w: {export_result['onnx_path']}\")\n",
    "        print(f\"Zoptymalizowany model ONNX zapisany w: {export_result['optimized_onnx_path']}\")\n",
    "        \n",
    "        # Rozmiar modelu\n",
    "        original_size_mb = os.path.getsize(export_result['onnx_path']) / (1024*1024)\n",
    "        optimized_size_mb = os.path.getsize(export_result['optimized_onnx_path']) / (1024*1024)\n",
    "        print(f\"Rozmiar oryginalnego modelu: {original_size_mb:.2f} MB\")\n",
    "        print(f\"Rozmiar zoptymalizowanego modelu: {optimized_size_mb:.2f} MB\")\n",
    "        print(f\"Redukcja rozmiaru po optymalizacji: {export_result.get('optimization_reduction', 0):.2f}%\")\n",
    "        \n",
    "        # Sprawdź czy kwantyzacja została wykonana w funkcji eksportu\n",
    "        if \"quantized_onnx_path\" in export_result and os.path.exists(export_result['quantized_onnx_path']):\n",
    "            quantized_size_mb = os.path.getsize(export_result['quantized_onnx_path']) / (1024*1024)\n",
    "            print(f\"Skwantyzowany model ONNX zapisany w: {export_result['quantized_onnx_path']}\")\n",
    "            print(f\"Rozmiar skwantyzowanego modelu: {quantized_size_mb:.2f} MB\")\n",
    "            print(f\"Redukcja rozmiaru po kwantyzacji: {export_result.get('quantization_reduction', 0):.2f}%\")\n",
    "        else:\n",
    "            print(\"Kwantyzacja nie została przeprowadzona w ramach procesu eksportu.\")\n",
    "        \n",
    "        # Informacje o metadanych i weryfikacji\n",
    "        print(f\"Metadane modelu zapisane w: {export_result.get('metadata_path', 'N/A')}\")\n",
    "        \n",
    "        # Wyświetl wyniki weryfikacji\n",
    "        if \"verification_result\" in export_result:\n",
    "            result = export_result[\"verification_result\"]\n",
    "            if result.get(\"success\", False):\n",
    "                print(\"\\nWeryfikacja modelu zakończona powodzeniem\")\n",
    "                if \"max_diff\" in result:\n",
    "                    print(f\"Maksymalna różnica między PyTorch i ONNX: {result['max_diff']:.6f}\")\n",
    "                if \"has_warning\" in result and result[\"has_warning\"]:\n",
    "                    print(f\"Ostrzeżenie: {result.get('warning', 'Wykryto różnice, ale mieszczą się w tolerancji')}\")\n",
    "            else:\n",
    "                print(f\"\\nWeryfikacja modelu nie powiodła się: {result.get('error', 'Nieznany błąd')}\")\n",
    "    else:\n",
    "        print(\"\\nEksport modelu się nie powiódł\")\n",
    "        print(f\"Błąd: {export_result.get('error', 'Nieznany błąd')}\")\n",
    "else:\n",
    "    print(\"Błąd: Nie znaleziono zmiennych ensemble_model i/lub CONFIG.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Audio Emotion Recognition)",
   "language": "python",
   "name": "audio-emotion-recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
