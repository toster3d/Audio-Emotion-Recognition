{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model równoległy Conv 1D / RNN dla Zero Crossing Rate + RMS Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architektura zawartego w tym notebooku modelu łączy dwie popularne ścieżki przetwarzania sygnałów audio: jednowymiarowe warstwy konwolucyjne (Conv 1D) i sieci rekurencyjne (RNN). Obie sieci działają równolegle. Wejściem dla modelu są cechy ZCR i RMS Energy obliczone dla każdej ramki czasowej sygnału audio. \n",
    "\n",
    "Etapy tworzenia modelu:\n",
    "\n",
    "1. Przetworzenie danych wejściowych:\n",
    "\n",
    "- Ekstrakcja cech Zero Crossing Rate (ZCR) i RMS Energy z plików audio:  \n",
    "<ins> Zero Crossing Rate (ZCR) </ins> mierzy, jak często amplituda sygnału przechodzi z wartości dodatniej na ujemną lub z ujemnej na dodatnią. Określa liczbę przejść przez zero na jednostkę czasu, zazwyczaj na ramkę analizy. Na przykład wyższa wartość ZCR wskazuje na dźwięki wysokoczęstotliwościowe (np. syczące spółgłoski 's', 'f'), natomiast niższa wartość jest charakterystyczna dla dźwięków niskoczęstotliwościowych (np. samogłoski).  \n",
    "<ins> RMS Energy </ins> to miara mocy lub głośności sygnału audio. Oblicza się ją jako pierwiastek kwadratowy ze średniej kwadratów amplitud sygnału w analizowanej ramce czasowej. Wyższa wartość wskazuje na głośniejszy dźwięk, niższa wartość na cichszy dźwięk lub ciszę.  \n",
    "- Normalizacja i ujednolicenie długości sekwencji cech.\n",
    "\n",
    "\n",
    "2. Budowa równoległej architektury:\n",
    "- Ścieżka Conv1D wyodrębnia lokalne wzorce i cechy z sekwencji ZCR i RMS Energy, wykrywając krótkoterminowe zależności i charakterystyczne wzorce energii/przejść przez zero.\n",
    "- Ścieżka RNN dobrze radzi sobie z modelowaniem długoterminowych zależności czasowych i kontekstu w sekwencjach ZCR i RMS Energy, co jest kluczowe dla rozpoznawania wzorców akustycznych.\n",
    "- Obie ścieżki łączą się po ekstrakcji cech.\n",
    "\n",
    "3. Implementacja funkcji pomocniczych:\n",
    "\n",
    "- Wizualizacja wyników treningu, macierzy pomyłek.\n",
    "- Obliczenie metryk klasyfikacji.\n",
    "- Zapisanie modelu i mapowanie etykiet.\n",
    "\n",
    "\n",
    "Przedstawiona architektura pozwala na równoległe przetwarzanie różnych rodzajów cech akustycznych, co może wpłynąć na otrzymanie lepszych wyników dla rozpoznawania emocji, ponieważ:\n",
    "\n",
    "- Conv1D dobrze radzi sobie z lokalnymi wzorcami w sygnale (np. charakterystyczne częstotliwości przejść dla różnych emocji).\n",
    "- BiLSTM potrafi uchwycić długoterminowe zależności i dynamiczne zmiany w energii sygnału."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katalog główny projektu: c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\n",
      "Czy katalog src istnieje: True\n",
      "Wyniki modelu będą zapisywane w: c:\\Users\\kubas\\Desktop\\Projekt dyplomowy\\Audio-Emotion-Recognition\\src\\Conv1D_RNN\\model_outputs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Dodaj katalog główny projektu do sys.path\n",
    "current_dir = (\n",
    "    os.path.dirname(os.path.abspath(__file__))\n",
    "    if \"__file__\" in globals()\n",
    "    else os.getcwd()\n",
    ")\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\", \"..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Katalog główny projektu: {project_root}\")\n",
    "print(f\"Czy katalog src istnieje: {os.path.exists(os.path.join(project_root, 'src'))}\")\n",
    "\n",
    "model_output_dir = os.path.join(project_root, \"src\", \"CONV1D_RNN\", \"model_outputs\")\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "print(f\"Wyniki modelu będą zapisywane w: {model_output_dir}\")\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Klasyfikacja ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Deep Learning (PyTorch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Wizualizacja\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie zbioru nEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja istnienia folderu z danymi oraz załadowanie zbioru danych\n",
    "try:\n",
    "    from src.config import DATASET_PATH\n",
    "    from src.create_data import download_and_save_dataset\n",
    "\n",
    "    dataset_path = DATASET_PATH\n",
    "    if os.path.exists(dataset_path):\n",
    "        try:\n",
    "            print(\"Rozpoczęcie ładowania datasetu z dysku...\")\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Wystąpił błąd podczas ładowania datasetu: {e}\")\n",
    "            print(\"Inicjowanie ponownego pobierania datasetu...\")\n",
    "            dataset = download_and_save_dataset()\n",
    "    else:\n",
    "        print(\"Folder 'data' nie został znaleziony. Inicjowanie pobierania datasetu...\")\n",
    "        dataset = download_and_save_dataset()\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"Moduły src.config i src.create_data nie są dostępne. Ładowanie przykładowego datasetu z HuggingFace.\"\n",
    "    )\n",
    "    dataset = load_dataset(\"amu-cai/nEMO\", split=\"train\")\n",
    "\n",
    "# Sprawdź strukturę datasetu i wybierz właściwy split\n",
    "if hasattr(dataset, \"keys\"):\n",
    "    print(f\"Dostępne splits: {list(dataset.keys())}\")\n",
    "    # Użyj split 'train' jeśli dostępny\n",
    "    if \"train\" in dataset:\n",
    "        dataset = dataset[\"train\"]\n",
    "        print(\"Używam split 'train'\")\n",
    "    else:\n",
    "        # Jeśli nie ma 'train', użyj pierwszego dostępnego\n",
    "        first_key = list(dataset.keys())[0]\n",
    "        dataset = dataset[first_key]\n",
    "        print(f\"Używam split '{first_key}'\")\n",
    "\n",
    "print(f\"Rozmiar datasetu: {len(dataset)}\")\n",
    "print(f\"Przykładowy sample: {dataset[0] if len(dataset) > 0 else 'Brak danych'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zastosowanie augmentacji plików audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dodaje biały szum do audio.\"\"\"\n",
    "\n",
    "\n",
    "def add_noise(audio, noise_factor=0.005):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    augmented_audio = audio + noise_factor * noise\n",
    "    return augmented_audio\n",
    "\n",
    "\n",
    "\"\"\"Rozciąga czas (przyspiesza lub spowalnia) audio.\"\"\"\n",
    "\n",
    "\n",
    "def time_stretch(audio, rate=None):\n",
    "    if rate is None:\n",
    "        # Losowy współczynnik między 0.85 a 1.15\n",
    "        rate = np.random.uniform(0.85, 1.15)\n",
    "    return librosa.effects.time_stretch(y=audio, rate=rate)\n",
    "\n",
    "\n",
    "\"\"\"Zmienia wysokość tonu audio.\"\"\"\n",
    "\n",
    "\n",
    "def pitch_shift(audio, sr, n_steps=None):\n",
    "    if n_steps is None:\n",
    "        # Losowa zmiana tonu w zakresie od -2 do 2 półtonów\n",
    "        n_steps = np.random.uniform(-2, 2)\n",
    "    return librosa.effects.pitch_shift(y=audio, sr=sr, n_steps=n_steps)\n",
    "\n",
    "\n",
    "\"\"\"Zmienia głośność audio.\"\"\"\n",
    "\n",
    "\n",
    "def change_volume(audio, volume_factor=None):\n",
    "    if volume_factor is None:\n",
    "        # Losowy współczynnik głośności między 0.8 a 1.2\n",
    "        volume_factor = np.random.uniform(0.8, 1.2)\n",
    "    return audio * volume_factor\n",
    "\n",
    "\n",
    "\"\"\"Aplikuje losowe augmentacje z określonym prawdopodobieństwem.\"\"\"\n",
    "\n",
    "\n",
    "def apply_augmentation(audio, sr, augment_prob=0.5):\n",
    "    augmented = np.copy(audio)\n",
    "    # Dodaj szum\n",
    "    if random.random() < augment_prob:\n",
    "        augmented = add_noise(augmented, noise_factor=random.uniform(0.001, 0.01))\n",
    "    # Rozciągnij czas\n",
    "    if random.random() < augment_prob:\n",
    "        augmented = time_stretch(augmented)\n",
    "    # Zmień wysokość tonu\n",
    "    if random.random() < augment_prob:\n",
    "        augmented = pitch_shift(augmented, sr)\n",
    "    # Zmień głośność\n",
    "    if random.random() < augment_prob:\n",
    "        augmented = change_volume(augmented)\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodane techniki augmentacji:\n",
    "\n",
    "- Dodawanie szumu - Symuluje różne warunki nagrywania, zwiększając odporność modelu na zakłócenia.\n",
    "- Zmiana tempa (time stretching) - Zmienia tempo audio bez zmiany wysokości tonu.\n",
    "- Zmiana wysokości tonu (pitch shifting) - Zmienia ton audio bez zmiany tempa.\n",
    "- Zmiana głośności - Symuluje różne odległości od mikrofonu.\n",
    "\n",
    "Korzyści z augmentacji danych:\n",
    "\n",
    "- Zwiększona ilość danych treningowych - Każda próbka audio jest augmentowana 2-krotnie, co trzykrotnie zwiększa rozmiar zbioru treningowego.\n",
    "- Lepsza generalizacja - Model będzie odporniejszy na różne warunki nagrywania i różnice w mówieniu.\n",
    "- Redukcja przeuczenia - Większa różnorodność danych pomaga modelowi skupić się na istotnych cechach, a nie na szumach specyficznych dla danego nagrania.\n",
    "- Zrównoważenie klas - Augmentacja może częściowo pomóc w problemie niezrównoważenia klas.\n",
    "\n",
    "Dodatkowo, kod zawiera funkcję wizualizacji efektów augmentacji, która pomaga zobaczyć, jak różne techniki wpływają na sygnał audio i ekstrahowane cechy (ZCR i RMS Energy).\n",
    "Augmentacja danych jest szczególnie ważna w przypadku rozpoznawania emocji z głosu, ponieważ ten sam tekst wypowiedziany z tą samą emocją może brzmieć zupełnie inaczej u różnych osób lub w różnych warunkach nagrywania. Dzięki augmentacji uczymy model rozpoznawać wzorce emocji niezależnie od tych zmiennych czynników."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja efektów augmentacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_array, sr=24000, frame_length=1024, hop_length=256):\n",
    "    # Normalizacja amplitudy\n",
    "    audio_array = audio_array / (np.max(np.abs(audio_array)) + 1e-10)\n",
    "\n",
    "    # Ekstrakcja Zero Crossing Rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(\n",
    "        y=audio_array, frame_length=frame_length, hop_length=hop_length\n",
    "    )\n",
    "\n",
    "    # Ekstrakcja RMS Energy\n",
    "    rms = librosa.feature.rms(\n",
    "        y=audio_array, frame_length=frame_length, hop_length=hop_length\n",
    "    )\n",
    "\n",
    "    return zcr[0], rms[0]  # Zwracamy jako 1D array\n",
    "\n",
    "\n",
    "# Przygotowanie danych\n",
    "audio_files = [sample[\"audio\"][\"array\"] for sample in dataset]\n",
    "sample_rates = [sample[\"audio\"][\"sampling_rate\"] for sample in dataset]\n",
    "labels = [sample[\"emotion\"] for sample in dataset]\n",
    "\n",
    "# Konwertowanie etykiet emocji na liczby\n",
    "emotion_labels = {emotion: idx for idx, emotion in enumerate(sorted(set(labels)))}\n",
    "numeric_labels = [emotion_labels[label] for label in labels]\n",
    "\n",
    "print(\"Mapowanie etykiet emocji:\")\n",
    "for emotion, idx in emotion_labels.items():\n",
    "    print(f\" {emotion}: {idx}\")\n",
    "\n",
    "# Ekstrakcja cech audio i przygotowanie danych\n",
    "max_sequence_length = 200  # Maksymalna długość sekwencji cech\n",
    "zcr_sequences = []\n",
    "rms_sequences = []\n",
    "aug_labels = []  # Etykiety z uwzględnieniem augmentowanych próbek\n",
    "\n",
    "# Dla każdej próbki audio\n",
    "for audio, sr, label in zip(audio_files, sample_rates, numeric_labels):\n",
    "    # Przycinanie lub paddowanie audio do stałej długości (5 sekund)\n",
    "    max_len = 5 * sr\n",
    "    if len(audio) > max_len:\n",
    "        start = np.random.randint(0, len(audio) - max_len)\n",
    "        audio = audio[start : start + max_len]\n",
    "    else:\n",
    "        padding = np.zeros(max_len - len(audio))\n",
    "        audio = np.concatenate([audio, padding])\n",
    "\n",
    "    # Dodanie oryginalnej próbki\n",
    "    zcr, rms = extract_audio_features(audio, sr=sr)\n",
    "\n",
    "    # Dostosowanie długości sekwencji\n",
    "    if len(zcr) > max_sequence_length:\n",
    "        zcr = zcr[:max_sequence_length]\n",
    "        rms = rms[:max_sequence_length]\n",
    "    else:\n",
    "        pad_len = max_sequence_length - len(zcr)\n",
    "        zcr = np.pad(zcr, (0, pad_len), \"constant\")\n",
    "        rms = np.pad(rms, (0, pad_len), \"constant\")\n",
    "\n",
    "    zcr_sequences.append(zcr)\n",
    "    rms_sequences.append(rms)\n",
    "    aug_labels.append(label)\n",
    "\n",
    "    # Dodanie augmentowanych próbek dla każdej emocji\n",
    "    # Augmentujemy każdą próbkę 2 razy\n",
    "    for _ in range(2):\n",
    "        augmented_audio = apply_augmentation(audio, sr)\n",
    "        aug_zcr, aug_rms = extract_audio_features(augmented_audio, sr=sr)\n",
    "\n",
    "        if len(aug_zcr) > max_sequence_length:\n",
    "            aug_zcr = aug_zcr[:max_sequence_length]\n",
    "            aug_rms = aug_rms[:max_sequence_length]\n",
    "        else:\n",
    "            pad_len = max_sequence_length - len(aug_zcr)\n",
    "            aug_zcr = np.pad(aug_zcr, (0, pad_len), \"constant\")\n",
    "            aug_rms = np.pad(aug_rms, (0, pad_len), \"constant\")\n",
    "\n",
    "        zcr_sequences.append(aug_zcr)\n",
    "        rms_sequences.append(aug_rms)\n",
    "        aug_labels.append(label)\n",
    "\n",
    "# Konwersja na numpy arrays\n",
    "X_zcr = np.array(zcr_sequences).reshape(-1, max_sequence_length, 1)\n",
    "X_rms = np.array(rms_sequences).reshape(-1, max_sequence_length, 1)\n",
    "y = np.array(aug_labels).astype(\"int64\")\n",
    "\n",
    "# Wyświetlanie informacji o kształcie danych\n",
    "print(f\"Kształt danych X_zcr: {X_zcr.shape}\")\n",
    "print(f\"Kształt danych X_rms: {X_rms.shape}\")\n",
    "print(f\"Kształt danych y: {y.shape}\")\n",
    "print(f\"Liczba oryginalnych próbek: {len(audio_files)}\")\n",
    "print(f\"Liczba próbek po augmentacji: {len(zcr_sequences)}\")\n",
    "\n",
    "# Podział na zbiory treningowy, walidacyjny i testowy\n",
    "indices = np.arange(len(y))\n",
    "indices_train, indices_temp, y_train, y_temp = train_test_split(\n",
    "    indices, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "indices_val, indices_test, y_val, y_test = train_test_split(\n",
    "    indices_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "X_zcr_train, X_zcr_val, X_zcr_test = (\n",
    "    X_zcr[indices_train],\n",
    "    X_zcr[indices_val],\n",
    "    X_zcr[indices_test],\n",
    ")\n",
    "X_rms_train, X_rms_val, X_rms_test = (\n",
    "    X_rms[indices_train],\n",
    "    X_rms[indices_val],\n",
    "    X_rms[indices_test],\n",
    ")\n",
    "\n",
    "print(f\"Rozmiar zbioru treningowego: {len(y_train)} próbek\")\n",
    "print(f\"Rozmiar zbioru walidacyjnego: {len(y_val)} próbek\")\n",
    "print(f\"Rozmiar zbioru testowego: {len(y_test)} próbek\")\n",
    "\n",
    "# Wyświetl rozkład klas w zbiorze treningowym\n",
    "class_counts = np.bincount(y_train)\n",
    "print(\"\\nRozkład klas w zbiorze treningowym:\")\n",
    "for label, count in enumerate(class_counts):\n",
    "    emotion = [k for k, v in emotion_labels.items() if v == label][0]\n",
    "    print(f\" {emotion}: {count} próbek\")\n",
    "\n",
    "# Obliczenie wag klas dla zrównoważenia\n",
    "class_weights = {}\n",
    "total_samples = len(y_train)\n",
    "n_classes = len(emotion_labels)\n",
    "for label, count in enumerate(class_counts):\n",
    "    class_weights[label] = total_samples / (n_classes * count)\n",
    "\n",
    "print(\"\\nWagi klas:\")\n",
    "for label, weight in class_weights.items():\n",
    "    emotion = [k for k, v in emotion_labels.items() if v == label][0]\n",
    "    print(f\" {emotion}: {weight:.4f}\")\n",
    "\n",
    "# Przygotowanie danych w formacie PyTorch\n",
    "X_zcr_train_tensor = torch.tensor(X_zcr_train, dtype=torch.float32)\n",
    "X_rms_train_tensor = torch.tensor(X_rms_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_zcr_val_tensor = torch.tensor(X_zcr_val, dtype=torch.float32)\n",
    "X_rms_val_tensor = torch.tensor(X_rms_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_zcr_test_tensor = torch.tensor(X_zcr_test, dtype=torch.float32)\n",
    "X_rms_test_tensor = torch.tensor(X_rms_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Tworzenie TensorDatasets\n",
    "train_dataset = TensorDataset(X_zcr_train_tensor, X_rms_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_zcr_val_tensor, X_rms_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_zcr_test_tensor, X_rms_test_tensor, y_test_tensor)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Definiowanie modelu w PyTorch\n",
    "class ParallelConv1DRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ParallelConv1DRNN, self).__init__()\n",
    "\n",
    "        # Ścieżka Conv1D dla ZCR\n",
    "        self.conv1_zcr = nn.Conv1d(1, 64, kernel_size=5, padding=2)\n",
    "        self.bn1_zcr = nn.BatchNorm1d(64)\n",
    "        self.pool1_zcr = nn.MaxPool1d(2)\n",
    "        self.dropout1_zcr = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv2_zcr = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2_zcr = nn.BatchNorm1d(128)\n",
    "        self.pool2_zcr = nn.MaxPool1d(2)\n",
    "        self.dropout2_zcr = nn.Dropout(0.4)\n",
    "\n",
    "        self.global_avg_pool_zcr = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Ścieżka RNN dla RMS\n",
    "        self.lstm1_rms = nn.LSTM(\n",
    "            input_size=1, hidden_size=hidden_size, batch_first=True, bidirectional=True\n",
    "        )\n",
    "\n",
    "        # POPRAWKA: Używamy hidden_size*2 zamiast max_sequence_length\n",
    "        # Było: self.bn1_rms = nn.BatchNorm1d(max_sequence_length)\n",
    "        self.bn1_rms = nn.BatchNorm1d(hidden_size * 2)  # POPRAWIONE\n",
    "\n",
    "        self.dropout1_rms = nn.Dropout(0.3)\n",
    "\n",
    "        self.lstm2_rms = nn.LSTM(\n",
    "            input_size=hidden_size * 2,  # *2 for bidirectional\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.bn2_rms = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout2_rms = nn.Dropout(0.4)\n",
    "\n",
    "        # Warstwy połączeniowe\n",
    "        # 128 from Conv1D + 128 from Bidirectional LSTM (64*2)\n",
    "        merged_size = 128 + hidden_size * 2\n",
    "        self.fc1 = nn.Linear(merged_size, 128)\n",
    "        self.bn_fc = nn.BatchNorm1d(128)\n",
    "        self.dropout_fc = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x_zcr, x_rms):\n",
    "        # Ścieżka Conv1D dla ZCR\n",
    "        # Transpozycja dla Conv1d (batch, channels, seq_len)\n",
    "        x_zcr = x_zcr.permute(0, 2, 1)\n",
    "\n",
    "        x_zcr = self.conv1_zcr(x_zcr)\n",
    "        x_zcr = self.bn1_zcr(x_zcr)\n",
    "        x_zcr = F.relu(x_zcr)\n",
    "        x_zcr = self.pool1_zcr(x_zcr)\n",
    "        x_zcr = self.dropout1_zcr(x_zcr)\n",
    "\n",
    "        x_zcr = self.conv2_zcr(x_zcr)\n",
    "        x_zcr = self.bn2_zcr(x_zcr)\n",
    "        x_zcr = F.relu(x_zcr)\n",
    "        x_zcr = self.pool2_zcr(x_zcr)\n",
    "        x_zcr = self.dropout2_zcr(x_zcr)\n",
    "\n",
    "        x_zcr = self.global_avg_pool_zcr(x_zcr)\n",
    "        x_zcr = x_zcr.view(x_zcr.size(0), -1)  # Flatten\n",
    "\n",
    "        # Ścieżka RNN dla RMS\n",
    "        x_rms_out, _ = self.lstm1_rms(x_rms)\n",
    "\n",
    "        # Permutacja dla BatchNorm1d (batch, features, seq_len)\n",
    "        x_rms_bn = x_rms_out.permute(0, 2, 1)\n",
    "        x_rms_bn = self.bn1_rms(x_rms_bn)\n",
    "        x_rms_bn = x_rms_bn.permute(0, 2, 1)  # Back to (batch, seq_len, features)\n",
    "\n",
    "        x_rms_bn = self.dropout1_rms(x_rms_bn)\n",
    "\n",
    "        _, (h_n, _) = self.lstm2_rms(x_rms_bn)\n",
    "        # h_n shape: (2, batch, hidden_size) - 2 for bidirectional\n",
    "\n",
    "        # Concatenate the final hidden states from both directions\n",
    "        x_rms = torch.cat((h_n[0], h_n[1]), dim=1)\n",
    "\n",
    "        x_rms = self.bn2_rms(x_rms)\n",
    "        x_rms = self.dropout2_rms(x_rms)\n",
    "\n",
    "        # Połączenie cech\n",
    "        x_combined = torch.cat((x_zcr, x_rms), dim=1)\n",
    "\n",
    "        # Warstwy klasyfikacyjne\n",
    "        x = self.fc1(x_combined)\n",
    "        x = self.bn_fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Inicjalizacja modelu\n",
    "input_size = 1  # Pojedyncza cecha (ZCR lub RMS) w każdym kanale\n",
    "hidden_size = 64  # Rozmiar warstw ukrytych\n",
    "num_classes = len(emotion_labels)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Używane urządzenie: {device}\")\n",
    "\n",
    "# Tworzenie modelu\n",
    "model = ParallelConv1DRNN(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Wyświetlenie liczby parametrów modelu\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Liczba parametrów modelu: {total_params}\")\n",
    "\n",
    "# Kompilacja modelu\n",
    "# Konwersja wag klas na tensor PyTorch\n",
    "class_weights_tensor = torch.tensor(\n",
    "    [class_weights[i] for i in range(len(class_weights))], dtype=torch.float32\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Funkcja zmieniająca współczynnik uczenia w zależności od epoki\n",
    "def lr_scheduler_func(epoch, initial_lr):\n",
    "    if epoch < 5:\n",
    "        return initial_lr\n",
    "    elif epoch < 10:\n",
    "        return initial_lr * 0.5\n",
    "    elif epoch < 15:\n",
    "        return initial_lr * 0.25\n",
    "    else:\n",
    "        return initial_lr * 0.1\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_weights)\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "\n",
    "# Historia treningu\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Trening\n",
    "epochs = 25\n",
    "initial_lr = 0.001\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "print(\"\\n===== TRENING MODELU RÓWNOLEGŁEGO CONV1D I RNN =====\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Aktualizacja learning rate\n",
    "    new_lr = lr_scheduler_func(epoch, initial_lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = new_lr\n",
    "\n",
    "    # Trening\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch_zcr, batch_rms, batch_labels in train_loader:\n",
    "        batch_zcr, batch_rms, batch_labels = (\n",
    "            batch_zcr.to(device),\n",
    "            batch_rms.to(device),\n",
    "            batch_labels.to(device),\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_zcr, batch_rms)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += batch_labels.size(0)\n",
    "        train_correct += predicted.eq(batch_labels).sum().item()\n",
    "\n",
    "    # Walidacja\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_zcr, batch_rms, batch_labels in val_loader:\n",
    "            batch_zcr, batch_rms, batch_labels = (\n",
    "                batch_zcr.to(device),\n",
    "                batch_rms.to(device),\n",
    "                batch_labels.to(device),\n",
    "            )\n",
    "\n",
    "            outputs = model(batch_zcr, batch_rms)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += batch_labels.size(0)\n",
    "            val_correct += predicted.eq(batch_labels).sum().item()\n",
    "\n",
    "    # Obliczenie średnich\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    train_acc = train_correct / train_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    # Zapisanie historii\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoka {epoch + 1}/{epochs}: \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {new_lr:.6f}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stopping(val_loss, model):\n",
    "        print(f\"Early stopping w epoce {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "# Zapisanie najlepszego modelu\n",
    "torch.save(\n",
    "    model.state_dict(), os.path.join(model_output_dir, \"best_conv1d_rnn_model.pt\")\n",
    ")\n",
    "print(\n",
    "    f\"Zapisano najlepszy model do pliku {os.path.join(model_output_dir, 'best_conv1d_rnn_model.pt')}\"\n",
    ")\n",
    "\n",
    "# Ocena modelu na zbiorze testowym\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_zcr, batch_rms, batch_labels in test_loader:\n",
    "        batch_zcr, batch_rms, batch_labels = (\n",
    "            batch_zcr.to(device),\n",
    "            batch_rms.to(device),\n",
    "            batch_labels.to(device),\n",
    "        )\n",
    "\n",
    "        outputs = model(batch_zcr, batch_rms)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += batch_labels.size(0)\n",
    "        test_correct += predicted.eq(batch_labels).sum().item()\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "test_acc = test_correct / test_total\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"\\nDokładność na zbiorze testowym: {test_acc:.4f}\")\n",
    "print(f\"Strata na zbiorze testowym: {test_loss:.4f}\")\n",
    "\n",
    "y_pred_classes = np.array(all_predictions)\n",
    "y_test_final = np.array(all_labels)\n",
    "\n",
    "# Raport klasyfikacji\n",
    "print(\"\\nRaport klasyfikacji:\")\n",
    "print(classification_report(y_test_final, y_pred_classes))\n",
    "\n",
    "# Macierz pomyłek\n",
    "cm = confusion_matrix(y_test_final, y_pred_classes)\n",
    "print(\"\\nMacierz pomyłek:\")\n",
    "print(cm)\n",
    "\n",
    "# Obliczanie F1-score\n",
    "f1_macro = f1_score(y_test_final, y_pred_classes, average=\"macro\")\n",
    "print(f\"\\nMacro F1-score: {f1_macro:.4f}\")\n",
    "\n",
    "# Wykresy\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[k for k in emotion_labels.keys()],\n",
    "    yticklabels=[k for k in emotion_labels.keys()],\n",
    ")\n",
    "plt.xlabel(\"Przewidziana emocja\")\n",
    "plt.ylabel(\"Prawdziwa emocja\")\n",
    "plt.title(\"Macierz pomyłek\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(model_output_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Historia treningu\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_accuracies, label=\"Dokładność treningu\")\n",
    "plt.plot(val_accuracies, label=\"Dokładność walidacji\")\n",
    "plt.title(\"Dokładność modelu\")\n",
    "plt.xlabel(\"Epoka\")\n",
    "plt.ylabel(\"Dokładność\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses, label=\"Strata treningu\")\n",
    "plt.plot(val_losses, label=\"Strata walidacji\")\n",
    "plt.title(\"Strata modelu\")\n",
    "plt.xlabel(\"Epoka\")\n",
    "plt.ylabel(\"Strata\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(model_output_dir, \"training_history.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Zapisanie mapowania etykiet\n",
    "with open(os.path.join(model_output_dir, \"emotion_labels.json\"), \"w\") as f:\n",
    "    json.dump(emotion_labels, f)\n",
    "print(\n",
    "    f\"Zapisano mapowanie etykiet do pliku {os.path.join(model_output_dir, 'emotion_labels.json')}\"\n",
    ")\n",
    "\n",
    "# Znormalizowana macierz pomyłek\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=list(emotion_labels.keys()),\n",
    "    yticklabels=list(emotion_labels.keys()),\n",
    ")\n",
    "plt.xlabel(\"Etykiety przewidywane\")\n",
    "plt.ylabel(\"Etykiety rzeczywiste\")\n",
    "plt.title(\"Znormalizowana Macierz Pomyłek\")\n",
    "plt.savefig(\n",
    "    os.path.join(model_output_dir, \"normalized_confusion_matrix.png\"),\n",
    "    format=\"png\",\n",
    "    dpi=300,\n",
    ")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Audio Emotion Recognition)",
   "language": "python",
   "name": "audio-emotion-recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
