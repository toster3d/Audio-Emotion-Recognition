{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacja CNN VGG16 dla klasyfikacji emocji z danych audio\n",
    "\n",
    "## Implementacja i analiza \n",
    "\n",
    "Ten notebook przedstawia implementacjƒô g≈Çƒôbokiej sieci neuronowej bazujƒÖcej na architekturze VGG16 do rozpoznawania emocji z nagra≈Ñ g≈Çosowych. W projekcie wykorzystujemy reprezentacjƒô nagra≈Ñ audio w formie mel-spektrogram√≥w, kt√≥re sƒÖ nastƒôpnie analizowane przez sieƒá konwolucyjnƒÖ.\n",
    "\n",
    "### Architektura VGG16\n",
    "\n",
    "VGG16 (Visual Geometry Group 16) to g≈Çƒôboka sieƒá konwolucyjna sk≈ÇadajƒÖca siƒô z 16 warstw z wagami, opracowana przez Karen Simonyan i Andrew Zissermana z Uniwersytetu Oksfordzkiego. Wyr√≥≈ºnia siƒô:\n",
    "\n",
    "- **StrukturƒÖ sekwencyjnƒÖ**: 13 warstw konwolucyjnych i 3 warstwy w pe≈Çni po≈ÇƒÖczone\n",
    "- **Prostym i jednorodnym designem**: Wykorzystanie ma≈Çych filtr√≥w konwolucyjnych (3x3) przez ca≈ÇƒÖ sieƒá\n",
    "- **G≈Çƒôboko≈õciƒÖ**: Sekwencja warstw z rosnƒÖcƒÖ liczbƒÖ filtr√≥w (64-512)\n",
    "- **Zastosowaniem max-poolingu**: Redukcja wymiarowo≈õci przy zachowaniu istotnych cech\n",
    "\n",
    "W tym projekcie adaptujemy architekturƒô VGG16 do rozpoznawania wzorc√≥w w spektrogramach audio reprezentujƒÖcych r√≥≈ºne emocje w g≈Çosie. Wykorzystujemy istniejƒÖcƒÖ implementacjƒô architektury VGG16 z biblioteki torchvision, z odpowiednimi modyfikacjami warstw, kt√≥re dostosowujƒÖ model do naszego zadania klasyfikacji emocji.\n",
    "\n",
    "### Mel-spektrogramy jako reprezentacja danych audio\n",
    "\n",
    "Zamiast bezpo≈õredniej analizy surowego sygna≈Çu audio, przekszta≈Çcamy nagrania do mel-spektrogram√≥w, co pozwala na:\n",
    "\n",
    "- **Wizualizacjƒô d≈∫wiƒôku**: Dwuwymiarowa reprezentacja, gdzie:\n",
    "  - O≈õ pozioma reprezentuje czas\n",
    "  - O≈õ pionowa reprezentuje czƒôstotliwo≈õci w skali mel (lepiej odpowiadajƒÖcej ludzkiemu s≈Çyszeniu)\n",
    "  - Intensywno≈õƒá kolor√≥w odzwierciedla energiƒô w danym pa≈õmie czƒôstotliwo≈õci\n",
    "\n",
    "- **Wydobycie cech charakterystycznych dla emocji**:\n",
    "  - Wzorce intonacji i modulacji g≈Çosu\n",
    "  - Charakterystyczne sygnatury czƒôstotliwo≈õciowe dla r√≥≈ºnych stan√≥w emocjonalnych\n",
    "  - Zmiany w dynamice i barwie g≈Çosu\n",
    "\n",
    "Ta reprezentacja umo≈ºliwia wykorzystanie architektury CNN (standardowo u≈ºywanej do analizy obraz√≥w) do efektywnej analizy wzorc√≥w d≈∫wiƒôkowych odpowiadajƒÖcych r√≥≈ºnym emocjom.\n",
    "\n",
    "### Modu≈Çy pomocnicze\n",
    "\n",
    "W implementacji wykorzystujemy nastƒôpujƒÖce modu≈Çy pomocnicze:\n",
    "\n",
    "##### `helpers/data.py`\n",
    "- `load_nemo_dataset()`: Funkcja ≈ÇadujƒÖca zbi√≥r danych NeMo Emotion Dataset\n",
    "- `prepare_data()`: Przetwarza dane audio do mel-spektrogram√≥w i dzieli je na zbiory treningowy, walidacyjny i testowy\n",
    "- `predict_emotion()`: Umo≈ºliwia klasyfikacjƒô emocji dla nowych nagra≈Ñ audio\n",
    "\n",
    "##### `helpers/VGG16_definition.py`\n",
    "- `build_vgg16_model()`: Tworzy model VGG16 dostosowany do klasyfikacji audio\n",
    "- `train_model()`: Implementuje proces trenowania modelu z zapisem historii\n",
    "- `save_model()`: Zapisuje wytrenowany model do p√≥≈∫niejszego wykorzystania\n",
    "\n",
    "##### `helpers/utils.py`\n",
    "- `evaluate_model()`: Generuje metryki oceny modelu oraz macierz pomy≈Çek\n",
    "- `plot_training_history()`: Wizualizuje przebieg treningu (dok≈Çadno≈õƒá i stratƒô)\n",
    "\n",
    "##### `config.py`\n",
    "Zawiera globalne parametry konfiguracyjne projektu:\n",
    "- Parametry uczenia (LEARNING_RATE, BATCH_SIZE, EPOCHS)\n",
    "- Liczba klas emocji (NUM_CLASSES)\n",
    "- Parametry ekstrakcji cech audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ustawienia ≈õrodowiska i importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importowanie bibliotek\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm  \n",
    "\n",
    "# Importowanie PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "# Importowanie konfiguracji\n",
    "from config import *\n",
    "\n",
    "# Podmiana tqdm w module data\n",
    "import helpers.data\n",
    "helpers.data.tqdm = tqdm\n",
    "\n",
    "# Importowanie stworzonych modu≈Ç√≥w\n",
    "from helpers.data import load_nemo_dataset, prepare_data, predict_emotion\n",
    "from helpers.VGG16_definition import build_vgg16_model, train_model\n",
    "from helpers.utils import evaluate_model, plot_training_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzenie i konfiguracja dostƒôpno≈õci GPU\n",
    "print(\"Sprawdzanie dostƒôpno≈õci GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"‚úÖ GPU zosta≈Ço poprawnie skonfigurowane: {torch.cuda.get_device_name(current_device)}\")\n",
    "    \n",
    "    # Wy≈õwietlenie informacji o pamiƒôci GPU\n",
    "    print(f\"   U≈ºywana karta: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Ca≈Çkowita pamiƒôƒá: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   Architektura: {torch.cuda.get_device_capability(0)}\")\n",
    "    \n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ùå Brak dostƒôpnego GPU z obs≈ÇugƒÖ CUDA. U≈ºywanie CPU...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ≈Åadowanie i przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Rozpoczynam ≈Çadowanie i przygotowanie danych...\")\n",
    "try:\n",
    "    dataset = load_nemo_dataset()\n",
    "    \n",
    "    # Przygotowanie danych - usuniƒôto for_torch=True, je≈õli wywo≈Çuje b≈ÇƒÖd\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(dataset)\n",
    "    \n",
    "    # Konwersja danych do format√≥w PyTorch\n",
    "    print(\"üîÑ Konwersja danych do formatu PyTorch...\")\n",
    "    X_train_tensor = torch.FloatTensor(X_train).permute(0, 3, 1, 2)  # [batch, channels, height, width]\n",
    "    X_val_tensor = torch.FloatTensor(X_val).permute(0, 3, 1, 2)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).permute(0, 3, 1, 2)\n",
    "    \n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "    \n",
    "    # Tworzenie DataLoader√≥w\n",
    "    print(\"üîÑ Tworzenie DataLoader√≥w...\")\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=2)\n",
    "    \n",
    "    # Podsumowanie danych\n",
    "    print(\"‚úÖ Dane zosta≈Çy pomy≈õlnie przygotowane:\")\n",
    "    print(f\"   ‚ñ™ Zbi√≥r treningowy: {X_train_tensor.shape[0]} pr√≥bek, kszta≈Çt: {X_train_tensor.shape}\")\n",
    "    print(f\"   ‚ñ™ Zbi√≥r walidacyjny: {X_val_tensor.shape[0]} pr√≥bek, kszta≈Çt: {X_val_tensor.shape}\")\n",
    "    print(f\"   ‚ñ™ Zbi√≥r testowy: {X_test_tensor.shape[0]} pr√≥bek, kszta≈Çt: {X_test_tensor.shape}\")\n",
    "    print(f\"   ‚ñ™ Batch size: {BATCH_SIZE}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd podczas przygotowywania danych: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Budowa i trenowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"üîÑ Budowanie modelu VGG16...\")\n",
    "    model = build_vgg16_model(num_classes=NUM_CLASSES)\n",
    "    model = model.to(device)\n",
    "    print(model)\n",
    "    \n",
    "    # Trenowanie modelu\n",
    "    print(\"üöÄ Rozpoczynam trenowanie modelu...\")\n",
    "    model, history = train_model(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        device,\n",
    "        epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE\n",
    "    )\n",
    "    print(\"‚úÖ Trenowanie zako≈Ñczone!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd podczas trenowania modelu: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ewaluacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"üîç Ewaluacja modelu na zbiorze testowym...\")\n",
    "    \n",
    "    # Definiujemy folder zapisu\n",
    "    save_dir = 'model_outputs'\n",
    "    \n",
    "    # Ewaluacja modelu z zapisem znormalizowanej macierzy pomy≈Çek\n",
    "    test_acc, test_loss, confusion_matrix = evaluate_model(\n",
    "        model, test_loader, device, save_dir=save_dir, normalize=True\n",
    "    )\n",
    "    \n",
    "    # Wizualizacja wynik√≥w historii treningu\n",
    "    print(\"üìä Wizualizacja wynik√≥w trenowania...\")\n",
    "    plot_training_history(history, save_dir=save_dir)\n",
    "    \n",
    "    print(f\"‚úÖ Wykresy i macierz pomy≈Çek zapisane w katalogu: {save_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd podczas ewaluacji modelu: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Zapisanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Zapisywanie modelu...\n",
      "‚úÖ Model zosta≈Ç pomy≈õlnie zapisany!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"üíæ Zapisywanie modelu...\")\n",
    "    \n",
    "    # Sprawd≈∫ czy katalog istnieje\n",
    "    if not os.path.exists('model_outputs'):\n",
    "        os.makedirs('model_outputs')\n",
    "    \n",
    "    # Zapisz model bezpo≈õrednio u≈ºywajƒÖc Torch\n",
    "    torch.save(model.state_dict(), 'model_outputs/best_model_vgg16.pt')\n",
    "    \n",
    "    # Zapisz historiƒô treningu\n",
    "    import pickle\n",
    "    with open('model_outputs/history.pkl', 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    \n",
    "    print(\"‚úÖ Model zosta≈Ç pomy≈õlnie zapisany!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå B≈ÇƒÖd podczas zapisywania: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
