{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importowanie niezbędnych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardowe biblioteki Pythona\n",
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Biblioteki do pracy z dźwiękiem i sygnałami\n",
    "import librosa\n",
    "\n",
    "# Biblioteki naukowe i manipulacja danymi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Biblioteki do wizualizacji\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Biblioteki ML i uczenia głębokiego\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Biblioteki do przygotowania danych i oceny modelu\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Tworzenie połączonego wykresu przy użyciu subplots\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Biblioteki specyficzne dla projektu\n",
    "from create_data import download_and_save_dataset\n",
    "from datasets import load_from_disk\n",
    "from config import (\n",
    "    BATCH_SIZE, DATASET_PATH, DROPOUT_RATE, EARLY_STOPPING_PATIENCE, \n",
    "    LEARNING_RATE, MAX_LENGTH, NUM_EPOCHS, SEED, WEIGHT_DECAY\n",
    ")\n",
    "from helpers.augment_for_all_types import AugmentedAudioDataset\n",
    "from helpers.early_stopping import EarlyStopping\n",
    "from helpers.resnet_model_definition import AudioResNet\n",
    "from helpers.utils import find_results_directory, read_results_from_files\n",
    "from helpers.data_proccesing import read_emotion_results\n",
    "from helpers.vizualization import generate_accuracy_comparison_plot, generate_emotion_visualizations\n",
    "\n",
    "# Ustawienie seed dla powtarzalności wyników\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Utworzenie katalogu dla wyników\n",
    "results_dir = 'feature_comparison_results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Sprawdź dostępność CUDA\n",
    "print(f\"CUDA dostępna: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Liczba urządzeń CUDA: {torch.cuda.device_count()}\")\n",
    "    print(f\"Nazwa urządzenia: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź strukturę katalogu\n",
    "print(f\"Struktura katalogu {DATASET_PATH}:\")\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    for root, dirs, files in os.walk(DATASET_PATH):\n",
    "        level = root.replace(DATASET_PATH, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f\"{subindent}{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wczytanie datasetu\n",
    "print(\"Wczytywanie datasetu...\")\n",
    "dataset = load_from_disk('data/nemo_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_array, sr, feature_type, max_length=3.0, \n",
    "                     n_mels=128, n_mfcc=40, n_chroma=12, \n",
    "                     n_fft=2048, hop_length=512, normalize=True):\n",
    "    \"\"\"Ekstrakcja różnych cech z sygnału audio.\"\"\"\n",
    "    # Ustalenie docelowej długości sygnału\n",
    "    target_length = int(max_length * sr)\n",
    "    if len(audio_array) > target_length:\n",
    "        audio_array = audio_array[:target_length]\n",
    "    else:\n",
    "        padding = np.zeros(target_length - len(audio_array))\n",
    "        audio_array = np.concatenate([audio_array, padding])\n",
    "    \n",
    "    feature = None\n",
    "    \n",
    "    if feature_type == \"melspectrogram\":\n",
    "        # Ekstrakcja melspektrogramu\n",
    "        S = librosa.feature.melspectrogram(\n",
    "            y=audio_array, sr=sr, n_mels=n_mels,\n",
    "            n_fft=n_fft, hop_length=hop_length\n",
    "        )\n",
    "        feature = librosa.power_to_db(S, ref=np.max)\n",
    "    \n",
    "    elif feature_type == \"spectrogram\":\n",
    "        # Obliczanie standardowego spektrogramu\n",
    "        D = np.abs(librosa.stft(audio_array, n_fft=n_fft, hop_length=hop_length))\n",
    "        feature = librosa.amplitude_to_db(D, ref=np.max)\n",
    "    \n",
    "    elif feature_type == \"mfcc\":\n",
    "        # Obliczanie MFCC (Mel-frequency cepstral coefficients)\n",
    "        feature = librosa.feature.mfcc(\n",
    "            y=audio_array, sr=sr, n_mfcc=n_mfcc,\n",
    "            n_fft=n_fft, hop_length=hop_length\n",
    "        )\n",
    "    \n",
    "    elif feature_type == \"chroma\":\n",
    "        # Obliczanie chromagramu\n",
    "        feature = librosa.feature.chroma_stft(\n",
    "            y=audio_array, sr=sr, n_chroma=n_chroma,\n",
    "            n_fft=n_fft, hop_length=hop_length\n",
    "        )\n",
    "    \n",
    "    elif feature_type == \"spectral_contrast\":\n",
    "        # Obliczanie spektralnego kontrastu\n",
    "        feature = librosa.feature.spectral_contrast(\n",
    "            y=audio_array, sr=sr, n_fft=n_fft, hop_length=hop_length\n",
    "        )\n",
    "    \n",
    "    elif feature_type == \"zcr\":\n",
    "        # Obliczanie Zero Crossing Rate\n",
    "        feature = librosa.feature.zero_crossing_rate(\n",
    "            audio_array, hop_length=hop_length\n",
    "        )\n",
    "        # Rozszerzanie wymiaru dla ZCR\n",
    "        expanded = np.zeros((n_mels, feature.shape[1]))\n",
    "        normalized_feature = (feature - np.min(feature)) / (np.max(feature) - np.min(feature) + 1e-8)\n",
    "        for i in range(n_mels):\n",
    "            scale_factor = 1.0 - (i / float(n_mels))\n",
    "            expanded[i, :] = normalized_feature * scale_factor\n",
    "        feature = expanded\n",
    "\n",
    "    elif feature_type == \"rms\":\n",
    "        # Obliczanie RMS Energy\n",
    "        feature = librosa.feature.rms(\n",
    "            y=audio_array, hop_length=hop_length\n",
    "        )\n",
    "        # Rozszerzanie wymiaru dla RMS\n",
    "        expanded = np.zeros((n_mels, feature.shape[1]))\n",
    "        normalized_feature = (feature - np.min(feature)) / (np.max(feature) - np.min(feature) + 1e-8)\n",
    "        for i in range(n_mels):\n",
    "            scale_factor = np.exp(-3.0 * (i / float(n_mels)))\n",
    "            expanded[i, :] = normalized_feature * scale_factor\n",
    "        feature = expanded\n",
    "\n",
    "    elif feature_type == \"tempogram\":\n",
    "        # Obliczanie tempogramu\n",
    "        feature = librosa.feature.tempogram(\n",
    "            y=audio_array, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "    \n",
    "    elif feature_type == \"tonnetz\":\n",
    "        # Obliczanie Tonnetz - harmonicznych relacji\n",
    "        y_harm = librosa.effects.harmonic(audio_array, margin=4.0)\n",
    "        chroma = librosa.feature.chroma_cqt(\n",
    "            y=y_harm, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "        feature = librosa.feature.tonnetz(chroma=chroma, sr=sr)\n",
    "    \n",
    "    elif feature_type == \"delta_mfcc\":\n",
    "        # Obliczanie Delta MFCC - zmian w MFCC\n",
    "        mfccs = librosa.feature.mfcc(\n",
    "            y=audio_array, sr=sr, n_mfcc=n_mfcc,\n",
    "            n_fft=n_fft, hop_length=hop_length\n",
    "        )\n",
    "        feature = librosa.feature.delta(mfccs)\n",
    "    \n",
    "    elif feature_type == \"delta_tempogram\":\n",
    "        # Obliczanie Delta Tempogram - zmian w tempie\n",
    "        tempogram = librosa.feature.tempogram(\n",
    "            y=audio_array, sr=sr, hop_length=hop_length\n",
    "        )\n",
    "        feature = librosa.feature.delta(tempogram)\n",
    "        \n",
    "    # NOWE TYPY CECH:\n",
    "    \n",
    "    elif feature_type == \"cqt\":\n",
    "        # Obliczanie Constant-Q Transform\n",
    "        C = librosa.cqt(y=audio_array, sr=sr, hop_length=hop_length, \n",
    "                        n_bins=84, bins_per_octave=12)\n",
    "        feature = librosa.amplitude_to_db(np.abs(C), ref=np.max)\n",
    "    \n",
    "    elif feature_type == \"harmonic_percussive\":\n",
    "        # Rozdzielenie sygnału na komponenty harmoniczne i perkusyjne\n",
    "        y_harmonic, y_percussive = librosa.effects.hpss(audio_array)\n",
    "        \n",
    "        # Generowanie spektrogramów dla obu komponentów\n",
    "        S_harmonic = librosa.feature.melspectrogram(y=y_harmonic, sr=sr, \n",
    "                                                n_mels=n_mels,\n",
    "                                                n_fft=n_fft, hop_length=hop_length)\n",
    "        S_percussive = librosa.feature.melspectrogram(y=y_percussive, sr=sr, \n",
    "                                                    n_mels=n_mels,\n",
    "                                                    n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "        # Konwersja do skali dB\n",
    "        S_harmonic_db = librosa.power_to_db(S_harmonic, ref=np.max)\n",
    "        S_percussive_db = librosa.power_to_db(S_percussive, ref=np.max)\n",
    "        \n",
    "        # Zamiast tworzyć 3-kanałową reprezentację, łączymy komponenty w jeden melspektrogram\n",
    "        # Używamy średniej ważonej - komponent harmoniczny ma większą wagę\n",
    "        feature = 0.7 * S_harmonic_db + 0.3 * S_percussive_db\n",
    "    \n",
    "    elif feature_type == \"mfcc_delta_visual\":\n",
    "        # MFCC z Delta i Delta-Delta \n",
    "        mfccs = librosa.feature.mfcc(y=audio_array, sr=sr, n_mfcc=n_mfcc,\n",
    "                                    n_fft=n_fft, hop_length=hop_length)\n",
    "        delta_mfcc = librosa.feature.delta(mfccs)\n",
    "        delta2_mfcc = librosa.feature.delta(mfccs, order=2)\n",
    "        \n",
    "        # Normalizacja każdego kanału\n",
    "        mfcc_norm = (mfccs - np.min(mfccs)) / (np.max(mfccs) - np.min(mfccs) + 1e-8)\n",
    "        delta_norm = (delta_mfcc - np.min(delta_mfcc)) / (np.max(delta_mfcc) - np.min(delta_mfcc) + 1e-8)\n",
    "        delta2_norm = (delta2_mfcc - np.min(delta2_mfcc)) / (np.max(delta2_mfcc) - np.min(delta2_mfcc) + 1e-8)\n",
    "        \n",
    "        # 3-kanałowa reprezentacja\n",
    "        feature = np.stack([mfcc_norm, delta_norm, delta2_norm], axis=0)\n",
    "    \n",
    "    elif feature_type == \"tonal_contrast\":\n",
    "        # Spektrogram z uwydatnieniem tonalności\n",
    "        D = librosa.stft(audio_array, n_fft=n_fft, hop_length=hop_length)\n",
    "        S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "        \n",
    "        # Dodatkowe przetwarzanie tonalne\n",
    "        chroma = librosa.feature.chroma_stft(S=np.abs(D), sr=sr, hop_length=hop_length)\n",
    "        tonnetz_features = librosa.feature.tonnetz(chroma=chroma, sr=sr)\n",
    "        \n",
    "        # Dopasowanie wymiarów\n",
    "        if tonnetz_features.shape[1] < S_db.shape[1]:\n",
    "            ratio = S_db.shape[1] / tonnetz_features.shape[1]\n",
    "            tonnetz_features = np.repeat(tonnetz_features, int(np.ceil(ratio)), axis=1)\n",
    "            tonnetz_features = tonnetz_features[:, :S_db.shape[1]]\n",
    "        \n",
    "        # Normalizacja i wzmocnienie tonalne\n",
    "        tonnetz_norm = (tonnetz_features - np.min(tonnetz_features)) / (np.max(tonnetz_features) - np.min(tonnetz_features) + 1e-8)\n",
    "        tonal_boost = np.mean(tonnetz_norm[:3, :], axis=0)\n",
    "        tonal_boost = np.tile(tonal_boost, (S_db.shape[0], 1))\n",
    "        \n",
    "        # Wzmocnienie spektrogramu\n",
    "        tonal_weight = 0.3\n",
    "        feature = S_db + tonal_weight * tonal_boost\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Nieznany typ cechy: {feature_type}\")\n",
    "    \n",
    "    # Normalizacja cech (opcjonalna)\n",
    "    if normalize and feature is not None:\n",
    "        if feature_type in [\"mfcc\", \"delta_mfcc\"]:\n",
    "            # Normalizacja MFCC - zaimplementowana\n",
    "            feature = librosa.util.normalize(feature)\n",
    "        elif feature_type in [\"melspectrogram\", \"spectrogram\", \"cqt\"]:\n",
    "            # Spektrogramy - przekształcone do dB\n",
    "            pass\n",
    "        elif feature_type in [\"harmonic_percussive\", \"mfcc_delta_visual\"]:\n",
    "            # Już znormalizowane podczas tworzenia\n",
    "            pass\n",
    "        else:\n",
    "            # Standardowa normalizacja min-max dla pozostałych cech\n",
    "            feature_min = np.min(feature)\n",
    "            feature_max = np.max(feature)\n",
    "            if feature_max > feature_min:\n",
    "                feature = (feature - feature_min) / (feature_max - feature_min)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Równoległe Przetwarzanie Zbioru Danych Audio\n",
    "\n",
    "Funkcję `process_dataset` przetwarza zbiór danych audio na wybrany typ cechy (np. melspectrogram, mfcc, chroma) w sposób równoległy, wykorzystując wiele rdzeni procesora. Funkcja ekstraktuje cechy za pomocą `extract_features`, normalizuje dane, koduje etykiety, tworzy podziały do walidacji krzyżowej i zapisuje wyniki do pamięci podręcznej, aby uniknąć ponownego przetwarzania. Wyświetla również statystyki, takie jak liczba przetworzonych próbek i czas wykonania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, feature_type, max_length=3.0,\n",
    "                    n_mels=128, n_mfcc=40, n_chroma=12,\n",
    "                    n_fft=2048, hop_length=512, \n",
    "                    normalize_features=True, normalize_dataset=True,\n",
    "                    n_jobs=-1, cache_dir=\"processed_features\",\n",
    "                    force_recompute=False, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Równoległe przetwarzanie całego zbioru danych audio na wybrany typ cechy z obsługą cache i walidacją krzyżową.\n",
    "    \n",
    "    Argumenty:\n",
    "        dataset: Zbiór danych zawierający próbki audio.\n",
    "        feature_type: Typ cechy do ekstrakcji.\n",
    "        max_length: Maksymalna długość próbki audio w sekundach.\n",
    "        n_mels: Liczba pasm melowych dla melspektrogramu.\n",
    "        n_mfcc: Liczba współczynników MFCC.\n",
    "        n_chroma: Liczba pasm chromatycznych.\n",
    "        n_fft: Długość okna FFT.\n",
    "        hop_length: Długość przeskoku między kolejnymi ramkami.\n",
    "        normalize_features: Flaga określająca, czy normalizować pojedyncze cechy.\n",
    "        normalize_dataset: Flaga określająca, czy normalizować cały zbiór danych.\n",
    "        n_jobs: Liczba równoległych procesów (-1 oznacza wszystkie dostępne rdzenie).\n",
    "        cache_dir: Katalog do zapisywania przetworzonych cech.\n",
    "        force_recompute: Flaga wymuszająca ponowne obliczenie cech, nawet jeśli istnieją w pamięci podręcznej.\n",
    "        cv_folds: Liczba foldów do walidacji krzyżowej.\n",
    "        \n",
    "    Zwraca:\n",
    "        dict: Słownik zawierający dane treningowe, walidacyjne i testowe oraz metadane.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tworzenie katalogu pamięci podręcznej, jeśli nie istnieje\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Generowanie unikalnego identyfikatora dla zestawu parametrów\n",
    "    params_str = f\"{feature_type}_{max_length}_{n_mels}_{n_mfcc}_{n_chroma}_{n_fft}_{hop_length}_{normalize_features}_{normalize_dataset}_{cv_folds}\"\n",
    "    cache_id = hashlib.md5(params_str.encode()).hexdigest()\n",
    "    cache_file = os.path.join(cache_dir, f\"{feature_type}_{cache_id}.pkl\")\n",
    "    \n",
    "    # Sprawdzanie istnienia pliku pamięci podręcznej\n",
    "    if os.path.exists(cache_file) and not force_recompute:\n",
    "        print(f\"Wczytywanie przetworzonych cech z pliku pamięci podręcznej: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    print(f\"Przetwarzanie próbek audio dla cechy: {feature_type}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Przygotowanie danych do przetwarzania\n",
    "    audio_samples = []\n",
    "    all_labels = []\n",
    "    sample_ids = []\n",
    "    \n",
    "    for i, sample in enumerate(dataset['train']):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Przygotowywanie {i}/{len(dataset['train'])} próbek\")\n",
    "        sample_ids.append(i)\n",
    "        audio_samples.append((sample['audio']['array'], sample['audio']['sampling_rate']))\n",
    "        all_labels.append(sample['emotion'])\n",
    "    \n",
    "    # Funkcja do przetwarzania pojedynczej próbki audio\n",
    "    def process_single_sample(i, audio_data):\n",
    "        audio_array, sr = audio_data\n",
    "        try:\n",
    "            feature = extract_features(\n",
    "                audio_array, sr, feature_type, max_length,\n",
    "                n_mels=n_mels, n_mfcc=n_mfcc, n_chroma=n_chroma,\n",
    "                n_fft=n_fft, hop_length=hop_length,\n",
    "                normalize=normalize_features\n",
    "            )\n",
    "            \n",
    "            if feature.size == 0 or (feature.ndim > 1 and feature.shape[1] == 0):\n",
    "                return i, None, \"Pusta cecha\"\n",
    "                \n",
    "            return i, feature, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return i, None, str(e)\n",
    "    \n",
    "    # Równoległe przetwarzanie próbek audio\n",
    "    print(f\"Rozpoczęcie równoległego przetwarzania na {n_jobs if n_jobs > 0 else 'wszystkich dostępnych'} rdzeniach...\")\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_single_sample)(i, audio_data) \n",
    "        for i, audio_data in enumerate(audio_samples)\n",
    "    )\n",
    "    \n",
    "    # Zbieranie wyników przetwarzania\n",
    "    processed_features = []\n",
    "    valid_indices = []\n",
    "    error_count = 0\n",
    "    \n",
    "    for i, feature, error in results:\n",
    "        if feature is not None:\n",
    "            processed_features.append(feature)\n",
    "            valid_indices.append(i)\n",
    "        else:\n",
    "            error_count += 1\n",
    "            # Logowanie błędów dla odrzuconych próbek\n",
    "            if i % 100 == 0 or \"Pusta cecha\" not in error:  # Logowanie co 100 błędów lub niestandardowe błędy\n",
    "                print(f\"Błąd przy próbce {i}: {error}\")\n",
    "            \n",
    "    # Konwersja listy cech na tablicę numpy\n",
    "    if len(processed_features) == 0:\n",
    "        raise ValueError(f\"Nie udało się przetworzyć żadnych próbek dla cechy {feature_type}\")\n",
    "    \n",
    "    features = np.array(processed_features)\n",
    "    valid_labels = [all_labels[i] for i in valid_indices]\n",
    "    \n",
    "    # Przekształcenie do formatu 4D: [próbki, kanały, wysokość, szerokość]\n",
    "    if feature_type in [\"harmonic_percussive\", \"mfcc_delta_visual\"]:\n",
    "        # Dla wielokanałowych cech\n",
    "        print(f\"Przetwarzanie wielokanałowej cechy {feature_type}\")\n",
    "        print(f\"Oryginalny kształt: {features.shape}\")\n",
    "        \n",
    "        # Sprawdzenie kształtu tablicy\n",
    "        if features.ndim == 4 and features.shape[3] == 3:\n",
    "            # Jeśli format to [próbki, wysokość, szerokość, kanały]\n",
    "            # Zmiana kolejności wymiarów na [próbki, kanały, wysokość, szerokość]\n",
    "            features = np.transpose(features, (0, 3, 1, 2))\n",
    "            print(f\"Przekształcony kształt: {features.shape}\")\n",
    "        else:\n",
    "            # W przypadku innego formatu lub błędu, używamy jednego kanału (pierwszy)\n",
    "            print(f\"Nieoczekiwany kształt dla {feature_type}, używamy jednokanałowej reprezentacji\")\n",
    "            \n",
    "            # Zamiast próbować używać wszystkich 3 kanałów, użyjmy tylko jednego\n",
    "            # Zastosujmy prostsze podejście - weźmy średnią z kanałów (jeśli istnieją)\n",
    "            if features.ndim > 3:\n",
    "                print(\"Uśrednianie kanałów...\")\n",
    "                features = np.mean(features, axis=-1)\n",
    "            \n",
    "            features = features.reshape(features.shape[0], 1, features.shape[1], features.shape[2])\n",
    "    else:\n",
    "        # Dla standardowych jednokanałowych cech\n",
    "        features = features.reshape(features.shape[0], 1, features.shape[1], features.shape[2])\n",
    "    \n",
    "    # Kodowanie etykiet\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(valid_labels)\n",
    "    num_classes = len(np.unique(encoded_labels))\n",
    "    \n",
    "    # Normalizacja całego zbioru danych (opcjonalnie)\n",
    "    if normalize_dataset:\n",
    "        mean = np.mean(features)\n",
    "        std = np.std(features)\n",
    "        if std > 0:\n",
    "            features = (features - mean) / std\n",
    "    \n",
    "    # Tworzenie foldów dla walidacji krzyżowej\n",
    "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_splits = list(skf.split(features, encoded_labels))\n",
    "    \n",
    "    # Przygotowanie słownika wynikowego\n",
    "    result = {\n",
    "        'feature_type': feature_type,\n",
    "        'features': features,\n",
    "        'labels': encoded_labels,\n",
    "        'label_encoder': label_encoder,\n",
    "        'num_classes': num_classes,\n",
    "        'cv_splits': cv_splits,\n",
    "        'params': {\n",
    "            'max_length': max_length,\n",
    "            'n_mels': n_mels,\n",
    "            'n_mfcc': n_mfcc,\n",
    "            'n_chroma': n_chroma,\n",
    "            'n_fft': n_fft,\n",
    "            'hop_length': hop_length,\n",
    "            'normalize_features': normalize_features,\n",
    "            'normalize_dataset': normalize_dataset\n",
    "        },\n",
    "        'processing_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    # Wyświetlanie statystyk przetwarzania\n",
    "    print(f\"Całkowita liczba próbek: {len(audio_samples)}\")\n",
    "    print(f\"Liczba ważnych próbek: {len(valid_indices)}\")\n",
    "    print(f\"Liczba pustych/błędnych cech: {error_count}\")\n",
    "    print(f\"Liczba klas emocji: {num_classes}\")\n",
    "    print(f\"Mapowanie klas: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "    print(f\"Czas przetwarzania: {result['processing_time']:.2f} sekund\")\n",
    "    \n",
    "    # Zapis wyników do pamięci podręcznej\n",
    "    print(f\"Zapisywanie przetworzonych cech do pliku pamięci podręcznej: {cache_file}\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Trening Modelu z Walidacją Krzyżową\n",
    "\n",
    " Funkcja `train_with_cross_validation` realizuje trening modelu z wykorzystaniem walidacji krzyżowej. Funkcja przetwarza zbiór danych na wybrane cechy audio za pomocą process_dataset, a następnie trenuje model (domyślnie `AudioResNet` z resnet_model_definition.py) na każdym foldzie walidacji krzyżowej, monitorując stratę i dokładność. Wykorzystuje mechanizm wczesnego zatrzymania (early stopping) i harmonogram uczenia, zapisuje najlepsze modele dla każdego foldu i oblicza średnie wyniki, takie jak dokładność i czas treningu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_cross_validation(dataset, feature_type, model_class=AudioResNet, \n",
    "                               batch_size=32, learning_rate=0.001, weight_decay=1e-5,\n",
    "                               epochs=50, patience=10, n_jobs=-1, cache_dir=\"processed_features\"):\n",
    "    \"\"\"\n",
    "    Funkcja realizuje trening modelu z wykorzystaniem walidacji krzyżowej.\n",
    "    \n",
    "    Argumenty:\n",
    "        dataset: Zbiór danych, który będzie przetwarzany.\n",
    "        feature_type: Typ cechy, która ma być wyodrębniona.\n",
    "        model_class: Klasa modelu, która ma być użyta do treningu.\n",
    "        batch_size: Rozmiar partii danych do przetwarzania.\n",
    "        learning_rate: Wartość współczynnika uczenia.\n",
    "        weight_decay: Wartość współczynnika regularyzacji.\n",
    "        epochs: Maksymalna liczba epok treningowych.\n",
    "        patience: Liczba epok bez poprawy, po której następuje zatrzymanie treningu.\n",
    "        n_jobs: Liczba procesów równoległych do użycia.\n",
    "        cache_dir: Katalog, w którym będą przechowywane przetworzone cechy.\n",
    "        \n",
    "    Zwraca:\n",
    "        dict: Wyniki walidacji krzyżowej.\n",
    "    \"\"\"\n",
    "\n",
    "    # Przetwarzanie danych z walidacją krzyżową\n",
    "    data = process_dataset(dataset, feature_type, n_jobs=n_jobs, cache_dir=cache_dir, cv_folds=5)\n",
    "    \n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    cv_splits = data['cv_splits']\n",
    "    num_classes = data['num_classes']\n",
    "    \n",
    "    # Inicjalizacja listy wyników dla każdego foldu\n",
    "    cv_results = []\n",
    "    \n",
    "    # Ustalenie urządzenia do obliczeń\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Używane urządzenie: {device}\")\n",
    "    \n",
    "    # Pętla treningowa dla każdego foldu\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "        print(f\"\\n{'=' * 30} Fold {fold+1}/{len(cv_splits)} {'=' * 30}\")\n",
    "        \n",
    "        # Przygotowanie danych dla bieżącego foldu\n",
    "        X_train, X_val = features[train_idx], features[val_idx]\n",
    "        y_train, y_val = labels[train_idx], labels[val_idx]\n",
    "        \n",
    "        # Konwersja danych do tensorów PyTorch\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.LongTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.LongTensor(y_val)\n",
    "        \n",
    "        # Tworzenie zbiorów danych dla DataLoader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Inicjalizacja modelu\n",
    "        input_shape = features[0].shape\n",
    "        model = model_class(input_shape, num_classes).to(device)\n",
    "        \n",
    "        # Ustalenie funkcji straty oraz optymalizatora\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "        \n",
    "        # Inicjalizacja zmiennych do śledzenia najlepszego modelu\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        best_epoch = 0\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Historia statystyk treningu\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Pętla treningowa\n",
    "        for epoch in range(epochs):\n",
    "            # Ustawienie modelu w tryb treningowy\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Zerowanie gradientów\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Przechodzenie przez model\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Wsteczna propagacja i aktualizacja wag\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Obliczanie średniej straty treningowej\n",
    "            avg_train_loss = total_train_loss / len(train_dataset)\n",
    "            \n",
    "            # Ustawienie modelu w tryb ewaluacji\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, targets in val_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    total_val_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    correct += (predictions == targets).sum().item()\n",
    "                    total += targets.size(0)\n",
    "            \n",
    "            # Obliczanie średniej straty walidacyjnej oraz dokładności\n",
    "            avg_val_loss = total_val_loss / len(val_dataset)\n",
    "            val_accuracy = 100.0 * correct / total\n",
    "            \n",
    "            # Aktualizacja harmonogramu uczenia\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Zapis statystyk do historii\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            history['val_acc'].append(val_accuracy)\n",
    "            \n",
    "            print(f\"Epoka {epoch+1}/{epochs}, Strata treningu: {avg_train_loss:.4f}, \"\n",
    "                  f\"Strata walidacji: {avg_val_loss:.4f}, Dokładność walidacji: {val_accuracy:.2f}%\")\n",
    "            \n",
    "            # Sprawdzanie warunków do zatrzymania treningu\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_epoch = epoch\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Zatrzymanie treningu! Brak poprawy przez {patience} epok.\")\n",
    "                    break\n",
    "        \n",
    "        # Obliczanie czasu treningu\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Przywracanie najlepszego modelu\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Ewaluacja najlepszego modelu\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "                total += targets.size(0)\n",
    "        \n",
    "        final_accuracy = 100.0 * correct / total\n",
    "        \n",
    "        # Zapis wyników dla bieżącego foldu\n",
    "        fold_result = {\n",
    "            'fold': fold + 1,\n",
    "            'best_epoch': best_epoch + 1,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'final_accuracy': final_accuracy,\n",
    "            'training_time': training_time,\n",
    "            'history': history,\n",
    "            'model_state': best_model_state\n",
    "        }\n",
    "        \n",
    "        cv_results.append(fold_result)\n",
    "        \n",
    "        print(f\"\\nWyniki dla foldu {fold+1}:\")\n",
    "        print(f\"Najlepsza epoka: {best_epoch+1}\")\n",
    "        print(f\"Najlepsza strata walidacji: {best_val_loss:.4f}\")\n",
    "        print(f\"Końcowa dokładność: {final_accuracy:.2f}%\")\n",
    "        print(f\"Czas treningu: {training_time:.2f} sekund\")\n",
    "    \n",
    "    # Obliczanie średnich wyników\n",
    "    avg_accuracy = np.mean([res['final_accuracy'] for res in cv_results])\n",
    "    avg_val_loss = np.mean([res['best_val_loss'] for res in cv_results])\n",
    "    avg_training_time = np.mean([res['training_time'] for res in cv_results])\n",
    "    \n",
    "    print(f\"\\n{'=' * 30} Wyniki walidacji krzyżowej {'=' * 30}\")\n",
    "    print(f\"Średnia dokładność: {avg_accuracy:.2f}% ± {np.std([res['final_accuracy'] for res in cv_results]):.2f}%\")\n",
    "    print(f\"Średnia strata walidacji: {avg_val_loss:.4f}\")\n",
    "    print(f\"Średni czas treningu: {avg_training_time:.2f} sekund\")\n",
    "    \n",
    "    # Tworzenie słownika z wynikami\n",
    "    final_results = {\n",
    "        'feature_type': feature_type,\n",
    "        'cv_results': cv_results,\n",
    "        'avg_accuracy': avg_accuracy,\n",
    "        'avg_val_loss': avg_val_loss,\n",
    "        'avg_training_time': avg_training_time,\n",
    "        'params': data['params'],\n",
    "        'label_encoder': data['label_encoder'],\n",
    "        'num_classes': num_classes\n",
    "    }\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja Collate dla DataLoader Audio\n",
    "\n",
    "Funkcja `audio_collate_fn` jest używana jako `collate_fn` w DataLoader do obsługi danych audio, w szczególności cech takich jak ZCR (Zero Crossing Rate) i RMS (RMS Energy). Funkcja przetwarza batch danych, pomijając puste tensory, dostosowuje wymiary tensorów (rozszerzając je do formatu 4D: [batch, kanały, wysokość, szerokość]), dopełnia je zerami do wspólnego rozmiaru i łączy w jeden batch tensorów cech oraz etykiet. W przypadku błędów zwraca dummy tensor, aby zapobiec przerwaniu procesu treningu.\n",
    "Uzasadnienie użytych featurów:\n",
    "Cechy audio, takie jak ZCR i RMS, są istotne w analizie sygnału, ponieważ dostarczają informacji o dynamice i energii dźwięku, co może być kluczowe w rozpoznawaniu emocji. Funkcja audio_collate_fn została zaprojektowana, aby obsługiwać te cechy, które często mają nietypowe wymiary (np. rozszerzone do 2D w procesie ekstrakcji), zapewniając ich poprawną integrację w batchach danych. Dopełnianie tensorów zerami pozwala na ujednolicenie rozmiarów danych wejściowych do modelu, co jest niezbędne dla architektur głębokich, takich jak ResNet, oczekujących spójnych wymiarów inputu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Funkcja collate_fn dla DataLoader, która obsługuje ZCR i RMS.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for feature, label in batch:\n",
    "        # Pomija elementy None oraz tensory bez wymiarów\n",
    "        if feature is None or feature.numel() == 0:\n",
    "            continue\n",
    "            \n",
    "        # Sprawdza, czy tensor ma prawidłowy format\n",
    "        if feature.dim() == 2:  # Jeden wymiar + kanał\n",
    "            # Rozszerza tensor do formatu 4D\n",
    "            feature = feature.unsqueeze(0).unsqueeze(0)  # [H,W] -> [1,1,H,W]\n",
    "        elif feature.dim() == 3:  # Cechy 2D bez kanału lub 1D z batch\n",
    "            if feature.shape[0] == 1:  # Format [1, H, W]\n",
    "                feature = feature.unsqueeze(0)  # [1,H,W] -> [1,1,H,W]\n",
    "            else:  # Format [B, H, W]\n",
    "                feature = feature.unsqueeze(1)  # [B,H,W] -> [B,1,H,W]\n",
    "                \n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Dopełnia tensory do wspólnego rozmiaru\n",
    "    try:\n",
    "        max_height = max([f.shape[2] for f in features])\n",
    "        max_width = max([f.shape[3] for f in features])\n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            if features[i].shape[2] < max_height or features[i].shape[3] < max_width:\n",
    "                # Dopełnia zerami do pełnego rozmiaru\n",
    "                padded = torch.zeros(features[i].shape[0], features[i].shape[1], \n",
    "                                    max_height, max_width, \n",
    "                                    device=features[i].device, dtype=features[i].dtype)\n",
    "                padded[:, :, :features[i].shape[2], :features[i].shape[3]] = features[i]\n",
    "                features[i] = padded\n",
    "        \n",
    "        features_batch = torch.cat(features, dim=0)\n",
    "        labels_batch = torch.tensor(labels)\n",
    "        \n",
    "        return features_batch, labels_batch\n",
    "    except Exception as e:\n",
    "        print(f\"Błąd podczas tworzenia batch: {e}\")\n",
    "        print(f\"Kształty tensorów: {[f.shape for f in features]}\")\n",
    "        # Zwraca dummy tensor w przypadku błędu\n",
    "        return torch.zeros((1, 1, 4, 4)), torch.zeros(1, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie Danych i Trening Modelu dla Wybranej Cechy\n",
    "\n",
    "Funkcja `prepare_dataset`s tworzy zestawy danych treningowe, walidacyjne i testowe z zastosowaniem augmentacji dla danych treningowych, korzystając z klasy     `AugmentedAudioDataset`. Funkcja `train_model_for_feature` przetwarza zbiór danych na wybraną cechę audio (np. melspectrogram), dzieli dane na podzbiory, inicjalizuje model AudioResNet, trenuje go z użyciem optymalizatora Adam, harmonogramu uczenia i mechanizmu wczesnego zatrzymania (`EarlyStopping`), a także zapisuje najlepszy model i historię treningu.\n",
    "\n",
    "W poniższym bloku wykorzystujemy klasę `AugmentedAudioDataset` zdefiniowaną w pliku `augment_for_all_types.py`. Ten plik definiuje framework do augmentacji danych audio, oferując różne strategie augmentacji (np. `SpectrogramAugmentation`, `MFCCAugmentation`) dla różnych typów cech (melspectrogram, mfcc, zcr, itp.). Wykorzystuje wzorzec projektowy strategii, umożliwiając dodawanie szumu, maskowanie częstotliwości czy przesunięcia czasowe, co zwiększa różnorodność danych treningowych i pomaga w zapobieganiu przeuczeniu. Klasa `AugmentedAudioDataset` integruje augmentację z procesem ładowania danych do modelu.\n",
    "Wykorzystana również klasa `EarlyStopping` monitoruje stratę walidacyjną podczas treningu. Jeśli strata nie poprawia się przez określoną liczbę epok (parametr patience), trening zostaje zatrzymany, a najlepszy model zapisany. Mechanizm ten zapobiega przeuczeniu i oszczędza czas obliczeń, zatrzymując trening, gdy dalsza poprawa jest mało prawdopodobna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotowanie zestawów danych z odpowiednią augmentacją\n",
    "def prepare_datasets(X_train, X_val, X_test, y_train, y_val, y_test, feature_type, batch_size):\n",
    "    \"\"\"\n",
    "    Przygotowuje zestawy danych z zastosowaniem strategii augmentacji.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tworzenie zbiorów danych z informacją o typie cechy\n",
    "    train_dataset = AugmentedAudioDataset(\n",
    "        X_train, y_train, \n",
    "        feature_type=feature_type,\n",
    "        augment=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = AugmentedAudioDataset(\n",
    "        X_val, y_val, \n",
    "        feature_type=feature_type,\n",
    "        augment=False\n",
    "    )\n",
    "    \n",
    "    test_dataset = AugmentedAudioDataset(\n",
    "        X_test, y_test, \n",
    "        feature_type=feature_type,\n",
    "        augment=False\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=audio_collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        collate_fn=audio_collate_fn\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        collate_fn=audio_collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Funkcja do trenowania modelu dla wybranej cechy\n",
    "def train_model_for_feature(dataset, feature_type, max_length=3.0):\n",
    "    \"\"\"\n",
    "    Trenuje model ResNet dla wybranej reprezentacji dźwięku.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Zbiór danych zawierający próbki audio i etykiety\n",
    "        feature_type: Typ cechy do ekstrakcji (np. 'melspectrogram', 'mfcc')\n",
    "        max_length: Maksymalna długość próbki audio w sekundach\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, test_loader, label_encoder, history, feature_dir, timestamp, feature_type, training_time)\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    feature_dir = os.path.join(results_dir, feature_type)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    # Przetwarzanie danych\n",
    "    processed_data = process_dataset(dataset, feature_type, max_length)\n",
    "    \n",
    "    # Wyodrębnienie istotnych wartości ze słownika\n",
    "    features = processed_data['features']\n",
    "    labels = processed_data['labels']\n",
    "    label_encoder = processed_data['label_encoder']\n",
    "    num_classes = processed_data['num_classes']    \n",
    "    # Podział na zbiory\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=SEED, stratify=labels)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=SEED, stratify=y_train)\n",
    "    \n",
    "    # Inicjalizacja modelu, funkcji straty i optymalizatora\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Używane urządzenie: {device}\")\n",
    "    \n",
    "    model = AudioResNet(num_classes=num_classes, dropout_rate=DROPOUT_RATE)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # Przygotowanie danych za pomocą funkcji prepare_datasets\n",
    "    train_loader, val_loader, test_loader = prepare_datasets(\n",
    "        X_train, X_val, X_test, \n",
    "        y_train, y_val, y_test, \n",
    "        feature_type=feature_type,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Ścieżka do zapisywania modelu\n",
    "    model_path = os.path.join(feature_dir, f'best_model_{feature_type}_{timestamp}.pt')\n",
    "    early_stopping = EarlyStopping(patience=EARLY_STOPPING_PATIENCE, path=model_path)\n",
    "    \n",
    "    # Historia treningu\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'val_accuracy': []\n",
    "    }\n",
    "    \n",
    "    # Proces treningu modelu\n",
    "    print(f\"Rozpoczynanie treningu dla cechy: {feature_type}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Faza treningu\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        \n",
    "        # Faza walidacji\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Obliczanie straty walidacyjnej\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Obliczanie dokładności\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        \n",
    "        print(f'Epoka {epoch+1}/{NUM_EPOCHS}, Strata treningu: {train_loss:.4f}, '\n",
    "              f'Strata walidacji: {val_loss:.4f}, Dokładność walidacji: {val_accuracy:.2f}%')\n",
    "        \n",
    "        # Aktualizacja schedulera\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Sprawdzenie warunku early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Aktywacja early stopping!\")\n",
    "            break\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Zakończenie treningu po {epoch+1} epokach. Czas: {training_time:.2f} sekund\")\n",
    "    \n",
    "    # Wczytanie najlepszego modelu\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    return model, test_loader, label_encoder, history, feature_dir, timestamp, feature_type, training_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ewaluacja Modelu i Wizualizacja Wyników\n",
    "\n",
    "Funkcja `evaluate_model` przeprowadza ewaluację wytrenowanego modelu na danych testowych. Funkcja oblicza stratę i dokładność testową, generuje macierz konfuzji, raport klasyfikacji oraz wizualizacje historii treningu (strata i dokładność w czasie). Wyniki, w tym hiperparametry i metryki wydajności, są zapisywane do plików w celu dalszej analizy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, label_encoder, history, feature_dir, timestamp, \n",
    "                  feature_type, training_time, device=None, save_results=True):\n",
    "    \"\"\"\n",
    "    Ewaluacja modelu oraz generowanie wizualizacji wyników.\n",
    "    \n",
    "    Args:\n",
    "        model: Wytrenowany model do ewaluacji.\n",
    "        test_loader: DataLoader zawierający dane testowe.\n",
    "        label_encoder: Enkoder etykiet do konwersji etykiet.\n",
    "        history: Historia treningu modelu.\n",
    "        feature_dir: Katalog przeznaczony do zapisywania wyników.\n",
    "        timestamp: Znacznik czasowy dla unikalności plików.\n",
    "        feature_type: Typ cechy, która jest analizowana.\n",
    "        training_time: Czas trwania treningu modelu.\n",
    "        device: Urządzenie, na którym przeprowadzana jest ewaluacja (CPU/GPU).\n",
    "        save_results: Flaga określająca, czy wyniki mają być zapisywane do plików.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Zawiera dokładność testu oraz historię treningu.\n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                # Sprawdzenie, czy batch zawiera dane\n",
    "                if inputs.numel() == 0 or labels.numel() == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Weryfikacja wymiarów danych wejściowych\n",
    "                if inputs.dim() != 4:\n",
    "                    continue\n",
    "                \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Obliczanie straty testowej\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                # Obliczanie dokładności\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Zbieranie predykcji oraz rzeczywistych etykiet\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        if len(test_loader) > 0:\n",
    "            test_loss = test_loss / len(test_loader)\n",
    "        else:\n",
    "            return 0.0, history\n",
    "            \n",
    "        test_accuracy = 100 * test_correct / test_total if test_total > 0 else 0.0\n",
    "        \n",
    "        if save_results and all_preds and all_labels:\n",
    "            # Obliczanie macierzy konfuzji\n",
    "            cm = confusion_matrix(all_labels, all_preds)\n",
    "            class_names = label_encoder.classes_\n",
    "            \n",
    "            # Wizualizacja macierzy konfuzji\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', \n",
    "                       xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.title(f'Znormalizowana macierz konfuzji - {feature_type}')\n",
    "            plt.ylabel('Rzeczywista etykieta')\n",
    "            plt.xlabel('Przewidziana etykieta')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(feature_dir, f'confusion_matrix_{feature_type}_{timestamp}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Generowanie raportu klasyfikacji\n",
    "            report = classification_report(all_labels, all_preds, \n",
    "                                          target_names=class_names, output_dict=True)\n",
    "            report_df = pd.DataFrame(report).transpose()\n",
    "            report_df.to_csv(os.path.join(feature_dir, f'classification_report_{feature_type}_{timestamp}.csv'))\n",
    "            \n",
    "            # Wizualizacja historii treningu\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history['train_loss'], label='Trening')\n",
    "            plt.plot(history['val_loss'], label='Walidacja')\n",
    "            plt.title(f'Strata podczas treningu - {feature_type}')\n",
    "            plt.xlabel('Epoka')\n",
    "            plt.ylabel('Strata')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history['val_accuracy'], label='Walidacja')\n",
    "            plt.title(f'Dokładność podczas treningu - {feature_type}')\n",
    "            plt.xlabel('Epoka')\n",
    "            plt.ylabel('Dokładność (%)')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(feature_dir, f'training_history_{feature_type}_{timestamp}.png'))\n",
    "            plt.close()\n",
    "            \n",
    "            # Zapisanie hiperparametrów oraz wyników\n",
    "            results = {\n",
    "                'feature_type': feature_type,\n",
    "                'hyperparameters': {\n",
    "                    'batch_size': BATCH_SIZE,\n",
    "                    'initial_lr': LEARNING_RATE,\n",
    "                    'weight_decay': WEIGHT_DECAY,   \n",
    "                    'dropout_rate': DROPOUT_RATE,\n",
    "                    'early_stopping_patience': EARLY_STOPPING_PATIENCE,\n",
    "                    'max_epochs': NUM_EPOCHS,\n",
    "                    'actual_epochs': len(history['train_loss'])\n",
    "                },\n",
    "                'performance': {\n",
    "                    'test_accuracy': test_accuracy,\n",
    "                    'test_loss': test_loss,\n",
    "                    'val_accuracy': history['val_accuracy'][-1] if history['val_accuracy'] else None,\n",
    "                    'val_loss': history['val_loss'][-1] if history['val_loss'] else None,\n",
    "                    'training_time': training_time\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Zapisanie wyników do pliku\n",
    "            with open(os.path.join(feature_dir, f'results_{feature_type}_{timestamp}.txt'), 'w') as f:\n",
    "                for section, values in results.items():\n",
    "                    if isinstance(values, dict):\n",
    "                        f.write(f\"{section.upper()}:\\n\")\n",
    "                        for key, value in values.items():\n",
    "                            f.write(f\"  {key}: {value}\\n\")\n",
    "                        f.write(\"\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"{section}: {values}\\n\\n\")\n",
    "                        \n",
    "        return test_accuracy, history\n",
    "        \n",
    "    except Exception as e:\n",
    "        return 0.0, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eksperyment Porównawczy Różnych Reprezentacji Audio\n",
    "\n",
    "Funkcja `run_feature_comparison_experiment` przeprowadza eksperymenty porównawcze dla różnych reprezentacji dźwięku. Funkcja trenuje modele dla każdego typu cechy (np. melspectrogram, mfcc), ewaluuje ich dokładność na danych testowych, zapisuje wyniki częściowe i końcowe do plików CSV oraz generuje wizualizacje porównujące dokładność i czas treningu dla różnych cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozszerzenie listy typów cech o nowe reprezentacje\n",
    "feature_types = [\n",
    "    \"spectrogram\",\n",
    "    \"melspectrogram\",\n",
    "    \"mfcc\",\n",
    "    \"chroma\",\n",
    "    \"spectral_contrast\",\n",
    "    \"zcr\",\n",
    "    \"rms\",\n",
    "    \"tempogram\",\n",
    "    \"tonnetz\", \n",
    "    \"delta_mfcc\", \n",
    "    \"delta_tempogram\",\n",
    "    # Dodanie nowych typów cech\n",
    "    \"cqt\",\n",
    "    \"harmonic_percussive\",\n",
    "    \"mfcc_delta_visual\",\n",
    "    \"tonal_contrast\",\n",
    "]\n",
    "\n",
    "def run_feature_comparison_experiment(dataset, feature_types_to_run=None, \n",
    "                                     skip_trained=True, save_interim=True,\n",
    "                                     n_mels=128, n_mfcc=40, n_chroma=12,\n",
    "                                     n_fft=2048, hop_length=512, \n",
    "                                     normalize_features=True, normalize_dataset=True):\n",
    "    \"\"\"\n",
    "    Uruchamia eksperymenty dla różnych reprezentacji dźwięku oraz porównuje wyniki.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Zbiór danych do przetwarzania.\n",
    "        feature_types_to_run: Lista typów cech do uruchomienia (domyślnie wszystkie).\n",
    "        skip_trained: Flaga określająca, czy pomijać cechy, dla których istnieją już wyniki.\n",
    "        save_interim: Flaga określająca, czy zapisywać wyniki częściowe po każdym typie cechy.\n",
    "        n_mels: Liczba pasm melowych dla melspektrogramu.\n",
    "        n_mfcc: Liczba współczynników MFCC.\n",
    "        n_chroma: Liczba pasm chromatycznych.\n",
    "        n_fft: Długość okna FFT.\n",
    "        hop_length: Długość przeskoku między kolejnymi ramkami.\n",
    "        normalize_features: Flaga określająca, czy normalizować pojedyncze cechy.\n",
    "        normalize_dataset: Flaga określająca, czy normalizować cały zbiór danych.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame z podsumowaniem wyników.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Katalog do przechowywania wyników\n",
    "    results_dir = 'feature_comparison_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Inicjalizacja słownika do przechowywania wyników\n",
    "    results = {}\n",
    "    \n",
    "    # Użycie przekazanej listy cech lub domyślnie wszystkich\n",
    "    if feature_types_to_run is None:\n",
    "        feature_types_to_run = feature_types\n",
    "    \n",
    "    # Sprawdzenie istnienia wcześniejszych wyników\n",
    "    summary_path = os.path.join(results_dir, 'feature_comparison_summary.csv')\n",
    "    if os.path.exists(summary_path) and skip_trained:\n",
    "        try:\n",
    "            existing_results = pd.read_csv(summary_path)\n",
    "            trained_features = existing_results['Feature Type'].tolist()\n",
    "            \n",
    "            # Wczytanie istniejących wyników\n",
    "            for ft in trained_features:\n",
    "                if ft in feature_types_to_run:\n",
    "                    accuracy = existing_results[existing_results['Feature Type'] == ft]['Test Accuracy (%)'].values[0]\n",
    "                    results[ft] = {\n",
    "                        'accuracy': accuracy,\n",
    "                        'history': None\n",
    "                    }\n",
    "            \n",
    "            # Usunięcie przetrenowanych cech z listy do uruchomienia\n",
    "            feature_types_to_run = [ft for ft in feature_types_to_run if ft not in trained_features]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Nie udało się wczytać istniejących wyników: {e}\")\n",
    "    \n",
    "    # Uruchamianie eksperymentów dla każdego typu cechy\n",
    "    start_time_all = time.time()\n",
    "    \n",
    "    for i, feature_type in enumerate(feature_types_to_run):\n",
    "        try:\n",
    "            # Trenowanie modelu z przekazaniem wszystkich parametrów\n",
    "            model, test_loader, label_encoder, history, feature_dir, timestamp, feature_type, training_time = train_model_for_feature(\n",
    "                dataset, feature_type, max_length=MAX_LENGTH,\n",
    "                n_mels=n_mels, n_mfcc=n_mfcc, n_chroma=n_chroma,\n",
    "                n_fft=n_fft, hop_length=hop_length,\n",
    "                normalize_features=normalize_features, \n",
    "                normalize_dataset=normalize_dataset,\n",
    "            )\n",
    "            \n",
    "            # Ewaluacja modelu\n",
    "            device = next(model.parameters()).device\n",
    "            accuracy, history = evaluate_model(\n",
    "                model, test_loader, label_encoder, history, \n",
    "                feature_dir, timestamp, feature_type, \n",
    "                training_time, device\n",
    "            )\n",
    "            \n",
    "            # Zapis wyników\n",
    "            results[feature_type] = {\n",
    "                'accuracy': accuracy,\n",
    "                'history': history\n",
    "            }\n",
    "            \n",
    "            # Zapis częściowych wyników, jeśli włączono tę opcję\n",
    "            if save_interim:\n",
    "                interim_results = {k: results[k]['accuracy'] for k in results}\n",
    "                interim_df = pd.DataFrame({\n",
    "                    'Feature Type': list(interim_results.keys()),\n",
    "                    'Test Accuracy (%)': list(interim_results.values())\n",
    "                }).sort_values('Test Accuracy (%)', ascending=False)\n",
    "                \n",
    "                interim_df.to_csv(os.path.join(results_dir, 'interim_results.csv'), index=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Zapis informacji o błędzie\n",
    "            results[feature_type] = {\n",
    "                'accuracy': 0.0,\n",
    "                'history': None,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    total_time = time.time() - start_time_all\n",
    "    \n",
    "    # Dodanie wcześniej przetrenowanych cech\n",
    "    all_results = {}\n",
    "    all_results.update(results)\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'Feature Type': list(all_results.keys()),\n",
    "        'Test Accuracy (%)': [all_results[ft]['accuracy'] for ft in all_results.keys()]\n",
    "    })\n",
    "    \n",
    "    results_df = results_df.sort_values('Test Accuracy (%)', ascending=False)\n",
    "    \n",
    "    # Zapis podsumowania do pliku CSV\n",
    "    results_df.to_csv(os.path.join(results_dir, 'feature_comparison_summary.csv'), index=False)\n",
    "    \n",
    "    # Wizualizacja porównania dokładności\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(results_df['Feature Type'], results_df['Test Accuracy (%)'])\n",
    "    plt.title('Porównanie dokładności dla różnych reprezentacji audio')\n",
    "    plt.xlabel('Typ cechy')\n",
    "    plt.ylabel('Dokładność testu (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(results_dir, 'accuracy_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Dodanie wizualizacji czasu treningu, jeśli dostępne\n",
    "    if any('training_time' in all_results.get(ft, {}) for ft in all_results):\n",
    "        times_df = pd.DataFrame({\n",
    "            'Feature Type': [ft for ft in all_results if 'training_time' in all_results[ft]],\n",
    "            'Training Time (s)': [all_results[ft].get('training_time', 0) for ft in all_results \n",
    "                                 if 'training_time' in all_results[ft]]\n",
    "        })\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(times_df['Feature Type'], times_df['Training Time (s)'])\n",
    "        plt.title('Porównanie czasu treningu dla różnych reprezentacji audio')\n",
    "        plt.xlabel('Typ cechy')\n",
    "        plt.ylabel('Czas treningu (s)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(results_dir, 'training_time_comparison.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odczyt Dokładności z Zapisanych Wyników\n",
    "\n",
    " Funkcja `read_accuracy_from_results` odczytuje dokładność testową dla określonego typu cechy audio z zapisanych wyników w katalogu **feature_comparison_results**. Funkcja przeszukuje podfoldery w poszukiwaniu pliku **results.json** i próbuje wyciągnąć wartość dokładności z różnych możliwych kluczy (accuracy, test_accuracy, val_accuracy), zwracając ją jako liczbę zmiennoprzecinkową lub None, jeśli wynik nie zostanie znaleziony.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_accuracy_from_results(feature_type, results_base_dir='feature_comparison_results'):\n",
    "    \"\"\"\n",
    "    Odczytuje dokładność z zapisanych wyników dla określonego typu cechy.\n",
    "    \n",
    "    Args:\n",
    "        feature_type: Typ cechy (np. 'mfcc', 'spectrogram').\n",
    "        results_base_dir: Katalog bazowy zawierający wyniki.\n",
    "        \n",
    "    Returns:\n",
    "        Dokładność jako liczba zmiennoprzecinkowa lub None, jeśli nie znaleziono.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    feature_dir = os.path.join(results_base_dir, feature_type)\n",
    "    \n",
    "    if not os.path.exists(feature_dir):\n",
    "        return None\n",
    "    \n",
    "    # Wyszukiwanie pliku results.json w podfolderach\n",
    "    for root, dirs, files in os.walk(feature_dir):\n",
    "        for file in files:\n",
    "            if file == 'results.json':\n",
    "                try:\n",
    "                    with open(os.path.join(root, file), 'r') as f:\n",
    "                        results = json.load(f)\n",
    "                        # Sprawdzanie różnych możliwych kluczy dla dokładności\n",
    "                        for key in ['accuracy', 'test_accuracy', 'val_accuracy']:\n",
    "                            if key in results:\n",
    "                                return float(results[key])\n",
    "                except Exception as e:\n",
    "                    print(f\"Wystąpił błąd podczas odczytu pliku {os.path.join(root, file)}: {str(e)}\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening i Porównanie Modeli dla Różnych Cech Audio\n",
    "\n",
    "Funkcja `run_training_experiment` przeprowadza trening modeli dla wybranej cechy audio lub wszystkich dostępnych reprezentacji (np. melspectrogram, mfcc). Funkcja pomija cechy z istniejącymi wynikami (jeśli ustawiono skip_trained), trenuje modele, ewaluuje ich dokładność, mierzy czas treningu i generuje szczegółowe raporty oraz interaktywne wizualizacje (wykresy dokładności i czasu treningu) przy użyciu bibliotek Plotly i Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_experiment(dataset, feature_type=None, skip_trained=False, results_base_dir='feature_comparison_results'):\n",
    "    \"\"\"\n",
    "    Uruchamia trening dla wybranej cechy lub wszystkich cech.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Zbiór danych do treningu.\n",
    "        feature_type: Konkretna cecha do treningu (None oznacza wszystkie cechy).\n",
    "        skip_trained: Flaga wskazująca, czy pomijać cechy, dla których istnieją już wyniki.\n",
    "        results_base_dir: Katalog bazowy do zapisywania wyników.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame z podsumowaniem wyników.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista wszystkich dostępnych typów cech\n",
    "    all_feature_types = [\n",
    "        \"spectrogram\", \"melspectrogram\", \"mfcc\", \"chroma\", \n",
    "        \"spectral_contrast\", \"zcr\", \"rms\", \"tempogram\",\n",
    "        \"tonnetz\", \"delta_mfcc\", \"delta_tempogram\", \"cqt\",\n",
    "    \"harmonic_percussive\",\"mfcc_delta_visual\",\"tonal_contrast\"\n",
    "    ]\n",
    "\n",
    "    # Ustalenie, które cechy będą trenowane\n",
    "    feature_types_to_train = [feature_type] if feature_type else all_feature_types\n",
    "    \n",
    "    # Tworzenie katalogów dla wyników\n",
    "    os.makedirs(results_base_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Słowniki do przechowywania wyników i czasów treningu\n",
    "    results = {}\n",
    "    training_times = {}\n",
    "    \n",
    "    # Trening dla każdej cechy\n",
    "    for feat_type in feature_types_to_train:\n",
    "        # Sprawdzenie, czy istnieją wyniki dla danej cechy\n",
    "        feature_dir = os.path.join(results_base_dir, feat_type)\n",
    "        \n",
    "        if skip_trained and os.path.exists(feature_dir) and len(os.listdir(feature_dir)) > 0:\n",
    "            print(f\"\\nPomijanie cechy {feat_type.upper()} - znaleziono istniejące wyniki.\")\n",
    "            \n",
    "            # Odczyt dokładności z istniejących wyników\n",
    "            accuracy = read_accuracy_from_results(feat_type, results_base_dir)\n",
    "            if accuracy:\n",
    "                results[feat_type] = accuracy\n",
    "                print(f\"Odczytana dokładność: {accuracy:.2f}%\")\n",
    "            continue\n",
    "        \n",
    "        # Rozpoczęcie treningu dla danej cechy\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"Trening modelu na reprezentacji: {feat_type.upper()}\")\n",
    "        print(f\"{'=' * 50}\\n\")\n",
    "        \n",
    "        # Pomiar czasu treningu\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Trening modelu\n",
    "        try:\n",
    "            model, test_loader, label_encoder, history, feature_dir, feat_timestamp, _, training_time = train_model_for_feature(dataset, feat_type)\n",
    "            \n",
    "            # Ewaluacja modelu\n",
    "            device = next(model.parameters()).device\n",
    "            accuracy, _ = evaluate_model(model, test_loader, label_encoder, history, \n",
    "                                        feature_dir, feat_timestamp, feat_type, \n",
    "                                        time.time() - start_time, device)\n",
    "            \n",
    "            results[feat_type] = accuracy\n",
    "            training_times[feat_type] = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\nTrening dla {feat_type} zakończony sukcesem. Dokładność: {accuracy:.2f}%\")\n",
    "            print(f\"Czas treningu: {training_times[feat_type]:.2f} sekund\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nBłąd podczas treningu dla cechy {feat_type}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Generowanie raportu podsumowującego (tylko jeśli trenowano więcej niż jedną cechę)\n",
    "    if len(results) > 1:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Wszystkie treningi zakończone. Generowanie raportu zbiorczego...\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        # Tworzenie DataFrame z wynikami\n",
    "        df = pd.DataFrame({\n",
    "            'Feature Type': list(results.keys()),\n",
    "            'Test Accuracy (%)': [results[ft] for ft in results.keys()],\n",
    "            'Training Time (s)': [training_times.get(ft, 0) for ft in results.keys()]\n",
    "        })\n",
    "        \n",
    "        # Sortowanie według dokładności\n",
    "        df = df.sort_values('Test Accuracy (%)', ascending=False)\n",
    "        \n",
    "        # Zapis do CSV\n",
    "        csv_path = os.path.join(results_base_dir, f'accuracy_summary_{timestamp}.csv')\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Zapisano podsumowanie do: {csv_path}\")\n",
    "        \n",
    "        # Tworzenie wykresu dokładności przy użyciu Plotly\n",
    "        fig_accuracy = px.bar(\n",
    "            df, \n",
    "            x='Feature Type', \n",
    "            y='Test Accuracy (%)', \n",
    "            title='Porównanie dokładności dla różnych reprezentacji audio',\n",
    "            color_discrete_sequence=['purple']\n",
    "        )\n",
    "        \n",
    "        fig_accuracy.update_layout(\n",
    "            xaxis_title='Typ cechy',\n",
    "            yaxis_title='Dokładność testu (%)',\n",
    "            xaxis_tickangle=-45,\n",
    "            yaxis_range=[0, max(df['Test Accuracy (%)']) * 1.1],\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Dodanie wartości nad słupkami\n",
    "        fig_accuracy.update_traces(\n",
    "            texttemplate='%{y:.1f}%', \n",
    "            textposition='outside'\n",
    "        )\n",
    "        \n",
    "        # Zapisanie wykresu dokładności\n",
    "        accuracy_plot_path = os.path.join(results_base_dir, f'accuracy_comparison_{timestamp}.html')\n",
    "        fig_accuracy.write_html(accuracy_plot_path)\n",
    "        \n",
    "        # Tworzenie wykresu czasu treningu przy użyciu Plotly\n",
    "        fig_time = px.bar(\n",
    "            df, \n",
    "            x='Feature Type', \n",
    "            y='Training Time (s)', \n",
    "            title='Porównanie czasu treningu dla różnych reprezentacji audio',\n",
    "            color_discrete_sequence=['purple']  # Kolor fioletowy\n",
    "        )\n",
    "        \n",
    "        fig_time.update_layout(\n",
    "            xaxis_title='Typ cechy',\n",
    "            yaxis_title='Czas treningu (s)',\n",
    "            xaxis_tickangle=-45,\n",
    "            yaxis_range=[0, max(df['Training Time (s)']) * 1.1],\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Dodanie wartości nad słupkami\n",
    "        fig_time.update_traces(\n",
    "            texttemplate='%{y:.0f}s', \n",
    "            textposition='outside'\n",
    "        )\n",
    "        \n",
    "        # Zapisanie wykresu czasu treningu\n",
    "        time_plot_path = os.path.join(results_base_dir, f'training_time_comparison_{timestamp}.html')\n",
    "        fig_time.write_html(time_plot_path)\n",
    "    \n",
    "        \n",
    "        fig_combined = make_subplots(\n",
    "            rows=1, cols=2,\n",
    "            subplot_titles=('Porównanie dokładności dla różnych reprezentacji audio', \n",
    "                           'Porównanie czasu treningu dla różnych reprezentacji audio')\n",
    "        )\n",
    "        \n",
    "        # Dodanie słupków dokładności\n",
    "        fig_combined.add_trace(\n",
    "            go.Bar(\n",
    "                x=df['Feature Type'], \n",
    "                y=df['Test Accuracy (%)'],\n",
    "                text=df['Test Accuracy (%)'].apply(lambda x: f'{x:.1f}%'),\n",
    "                textposition='outside',\n",
    "                marker_color='purple',\n",
    "                name='Dokładność'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Dodanie słupków czasu treningu\n",
    "        fig_combined.add_trace(\n",
    "            go.Bar(\n",
    "                x=df['Feature Type'], \n",
    "                y=df['Training Time (s)'],\n",
    "                text=df['Training Time (s)'].apply(lambda x: f'{int(x)}s'),\n",
    "                textposition='outside',\n",
    "                marker_color='purple',\n",
    "                name='Czas treningu'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Aktualizacja układu\n",
    "        fig_combined.update_layout(\n",
    "            height=600,\n",
    "            width=1200,\n",
    "            showlegend=False,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        \n",
    "        # Aktualizacja osi X i Y dla obu wykresów\n",
    "        fig_combined.update_xaxes(title_text='Typ cechy', tickangle=-45, row=1, col=1)\n",
    "        fig_combined.update_xaxes(title_text='Typ cechy', tickangle=-45, row=1, col=2)\n",
    "        fig_combined.update_yaxes(title_text='Dokładność testu (%)', range=[0, max(df['Test Accuracy (%)']) * 1.1], row=1, col=1)\n",
    "        fig_combined.update_yaxes(title_text='Czas treningu (s)', range=[0, max(df['Training Time (s)']) * 1.1], row=1, col=2)\n",
    "        \n",
    "        # Zapisanie połączonego wykresu\n",
    "        combined_plot_path = os.path.join(results_base_dir, f'feature_comparison_{timestamp}.html')\n",
    "        fig_combined.write_html(combined_plot_path)\n",
    "        \n",
    "        print(f\"Zapisano interaktywne wizualizacje do: {accuracy_plot_path}, {time_plot_path}, {combined_plot_path}\")\n",
    "        print(\"\\nPodsumowanie wyników:\")\n",
    "        print(df)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Zwrócenie wyniku dla pojedynczej cechy\n",
    "    elif len(results) == 1:\n",
    "        feature = list(results.keys())[0]\n",
    "        print(f\"\\nWynik dla cechy {feature}: {results[feature]:.2f}%\")\n",
    "        return results[feature]\n",
    "    \n",
    "    # Informacja o braku przeprowadzonego treningu\n",
    "    else:\n",
    "        print(\"\\nNie przeprowadzono żadnego treningu.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uruchom trening dla wszystkich typów cech\n",
    "#results_df = run_training_experiment(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening modelu dla CQT\n",
    "#accuracy_cqt = run_training_experiment(dataset, feature_type=\"cqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening modelu dla Harmonic-Percussive Source Separation\n",
    "accuracy_hpss = run_training_experiment(dataset, feature_type=\"harmonic_percussive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening modelu dla MFCC Delta Visual\n",
    "#accuracy_mfcc_delta = run_training_experiment(dataset, feature_type=\"mfcc_delta_visual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening modelu dla Tonal Contrast\n",
    "#accuracy_tonal = run_training_experiment(dataset, feature_type=\"tonal_contrast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening przeprowadzany wyłącznie dla melspektrogramu\n",
    "#accuracy = run_training_experiment(dataset, feature_type=\"chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = run_training_experiment(dataset, feature_type=\"melspectrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = run_training_experiment(dataset, feature_type=\"mfcc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rozpoczyna trening, pomijając cechy, dla których wyniki są już dostępne\n",
    "# results_df = run_training_experiment(dataset, skip_trained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie Wizualizacji Wyników Analizy\n",
    "\n",
    " Funkcja `generate_all_visualizations` automatycznie generuje i zapisuje wizualizacje wyników analizy różnych cech audio. Funkcja wyszukuje katalog z wynikami, odczytuje dane dotyczące dokładności i emocji, sortuje wyniki według dokładności, zapisuje je do plików CSV oraz tworzy wykresy porównawcze i wizualizacje emocji, zapisując je w odpowiednich lokalizacjach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_visualizations():\n",
    "    \"\"\"Generuje i zapisuje wszystkie wizualizacje wyników analizy.\"\"\"\n",
    "    # Wyszukiwanie katalogu z wynikami\n",
    "    base_dir = find_results_directory()\n",
    "    if base_dir is None:\n",
    "        return\n",
    "    \n",
    "    # Odczyt wyników z plików\n",
    "    results_df = read_results_from_files(base_dir)\n",
    "    emotions_df = read_emotion_results(base_dir)\n",
    "    \n",
    "    # Ustalenie katalogu do zapisu\n",
    "    save_dir = base_dir\n",
    "    \n",
    "    # Przetwarzanie wyników dokładności\n",
    "    if results_df is not None and not results_df.empty:\n",
    "        # Sortowanie wyników według dokładności w porządku malejącym\n",
    "        results_df = results_df.sort_values('Test Accuracy (%)', ascending=False)\n",
    "        \n",
    "        # Wyświetlenie DataFrame dla przeglądu wyników\n",
    "        print(f\"\\nZnalezione wyniki dokładności:\\n{results_df}\")\n",
    "        \n",
    "        # Zapis wyników do pliku CSV\n",
    "        csv_path = os.path.join(save_dir, 'feature_comparison_summary_auto.csv')\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Zapisano wyniki dokładności do: {csv_path}\")\n",
    "        \n",
    "        # Generowanie wykresu porównania dokładności\n",
    "        combined_path = generate_accuracy_comparison_plot(results_df, save_dir)\n",
    "        print(f\"Zapisano wykres porównania dokładności do: {combined_path}\")\n",
    "    else:\n",
    "        print(\"Brak danych dokładności do wygenerowania wykresów.\")\n",
    "    \n",
    "    # Przetwarzanie wyników emocji\n",
    "    if emotions_df is not None and not emotions_df.empty:\n",
    "        # Zapis wyników emocji do pliku CSV\n",
    "        emotions_csv_path = os.path.join(save_dir, 'emotions_comparison_auto.csv')\n",
    "        emotions_df.to_csv(emotions_csv_path, index=False)\n",
    "        print(f\"\\nZapisano wyniki emocji do: {emotions_csv_path}\")\n",
    "        \n",
    "        # Generowanie wizualizacji emocji\n",
    "        emotion_paths = generate_emotion_visualizations(emotions_df, results_df, save_dir)\n",
    "        print(\"\\nWygenerowane wizualizacje emocji:\")\n",
    "        for name, path in emotion_paths.items():\n",
    "            if path:\n",
    "                print(f\"- {name}: {path}\")\n",
    "        \n",
    "        print(\"\\nGenerowanie wszystkich wizualizacji zakończone pomyślnie.\")\n",
    "    else:\n",
    "        print(\"Brak danych emocji do wygenerowania wykresów.\")\n",
    "\n",
    "# Uruchomienie generowania wszystkich wizualizacji\n",
    "generate_all_visualizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_feature_statistics(cache_dir=\"processed_features\", stats_dir=\"normalization_stats\"):\n",
    "    \"\"\"\n",
    "    Analizuje statystyki cech z zapisanych plików cache bez modyfikacji oryginalnego kodu.\n",
    "    \n",
    "    Args:\n",
    "        cache_dir: Katalog z zapisanymi cechami\n",
    "        stats_dir: Katalog do zapisania statystyk\n",
    "        \n",
    "    Returns:\n",
    "        dict: Słownik ze statystykami dla każdego typu cechy\n",
    "    \"\"\"\n",
    "    os.makedirs(stats_dir, exist_ok=True)\n",
    "    all_stats = {}\n",
    "    \n",
    "    # Wyszukiwanie plików cache\n",
    "    cache_files = glob.glob(os.path.join(cache_dir, \"*.pkl\"))\n",
    "    \n",
    "    for cache_file in cache_files:\n",
    "        try:\n",
    "            # Odczyt danych z cache\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            feature_type = data['feature_type']\n",
    "            features = data['features']\n",
    "            \n",
    "            # Wyliczanie statystyk\n",
    "            stats = {\n",
    "                'feature_type': feature_type,\n",
    "                'mean': float(np.mean(features)),\n",
    "                'std': float(np.std(features)),\n",
    "                'min': float(np.min(features)),\n",
    "                'max': float(np.max(features)),\n",
    "                'shape': features.shape,\n",
    "                'n_samples': features.shape[0],\n",
    "                'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            }\n",
    "            \n",
    "            # Dodatkowe statystyki per kanał/wymiar\n",
    "            if features.ndim > 2:\n",
    "                stats['channel_means'] = [float(np.mean(features[:, i, :, :])) \n",
    "                                        for i in range(features.shape[1])]\n",
    "                stats['channel_stds'] = [float(np.std(features[:, i, :, :])) \n",
    "                                       for i in range(features.shape[1])]\n",
    "            \n",
    "            # Zapisywanie do pliku JSON\n",
    "            stats_file = os.path.join(stats_dir, f\"{feature_type}_stats.json\")\n",
    "            with open(stats_file, 'w') as f:\n",
    "                json.dump(stats, f, indent=4)\n",
    "            \n",
    "            all_stats[feature_type] = stats\n",
    "            \n",
    "            # Wyświetlanie podsumowania\n",
    "            print(f\"\\nStatystyki dla {feature_type}:\")\n",
    "            print(f\"Liczba próbek: {stats['n_samples']}\")\n",
    "            print(f\"Kształt danych: {stats['shape']}\")\n",
    "            print(f\"Średnia globalna: {stats['mean']:.4f}\")\n",
    "            print(f\"Odchylenie standardowe: {stats['std']:.4f}\")\n",
    "            print(f\"Min: {stats['min']:.4f}\")\n",
    "            print(f\"Max: {stats['max']:.4f}\")\n",
    "            if 'channel_means' in stats:\n",
    "                print(\"Średnie per kanał:\", \n",
    "                      [f\"{x:.4f}\" for x in stats['channel_means']])\n",
    "            print(f\"Zapisano do: {stats_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Błąd podczas przetwarzania {cache_file}: {str(e)}\")\n",
    "    \n",
    "    # Zapisywanie zbiorczego podsumowania\n",
    "    summary_file = os.path.join(stats_dir, \"all_features_summary.json\")\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(all_stats, f, indent=4)\n",
    "    \n",
    "    # Generowanie wykresu porównawczego\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    feature_types = list(all_stats.keys())\n",
    "    means = [all_stats[ft]['mean'] for ft in feature_types]\n",
    "    stds = [all_stats[ft]['std'] for ft in feature_types]\n",
    "    \n",
    "    x = np.arange(len(feature_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, means, width, label='Średnia')\n",
    "    plt.bar(x + width/2, stds, width, label='Odchylenie std.')\n",
    "    \n",
    "    plt.xlabel('Typ cechy')\n",
    "    plt.ylabel('Wartość')\n",
    "    plt.title('Porównanie statystyk dla różnych typów cech')\n",
    "    plt.xticks(x, feature_types, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Zapisywanie wykresu\n",
    "    plt.savefig(os.path.join(stats_dir, 'features_statistics_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return all_stats\n",
    "\n",
    "# Po zakończeniu treningu wszystkich modeli\n",
    "stats = analyze_feature_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
