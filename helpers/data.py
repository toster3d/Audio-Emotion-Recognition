"""
Modu≈Ç zawierajƒÖcy funkcje przetwarzania danych dla modelu VGG16
"""

import numpy as np
import librosa
from sklearn.model_selection import train_test_split
from datasets import load_dataset
import time
from tqdm import tqdm
import os

# Importowanie konfiguracji
import sys
#sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config import *


def extract_melspectrogram(audio, sr=SAMPLE_RATE):
    """
    Ekstrakcja spektrogramu Mela z sygna≈Çu audio
    
    Args:
        audio: Sygna≈Ç audio
        sr: Czƒôstotliwo≈õƒá pr√≥bkowania (domy≈õlnie: SAMPLE_RATE)
        
    Returns:
        mel_spectrogram: Znormalizowany spektrogram Mela
    """
    # Przycinanie lub paddowanie do sta≈Çej d≈Çugo≈õci
    if len(audio) > sr * DURATION:
        audio = audio[:sr * DURATION]
    else:
        audio = np.pad(audio, (0, max(0, sr * DURATION - len(audio))), 'constant')

    # Generowanie spektrogramu Mela
    mel_spectrogram = librosa.feature.melspectrogram(
        y=audio,
        sr=sr,
        n_mels=N_MELS,
        n_fft=N_FFT,
        hop_length=HOP_LENGTH
    )

    # Konwersja do decybeli i normalizacja 
    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)
    mel_spectrogram = (mel_spectrogram - np.min(mel_spectrogram)) / (np.max(mel_spectrogram) - np.min(mel_spectrogram))
    return mel_spectrogram


def load_nemo_dataset():
    """
    ≈Åadowanie datasetu nEMO z Hugging Face z paskiem postƒôpu
    
    Returns:
        dataset: Za≈Çadowany dataset nEMO
    """
    print("üîÑ Rozpoczynam ≈Çadowanie datasetu nEMO...")
    try:
        start_time = time.time()
        dataset = load_dataset("amu-cai/nEMO")
        elapsed_time = time.time() - start_time
        print(f"‚úÖ Dataset nEMO zosta≈Ç pomy≈õlnie za≈Çadowany ({elapsed_time:.2f}s)")
        print(f"Przyk≈Çady treningowe: {len(dataset['train'])}")
        print(f"Przyk≈Çady testowe: {len(dataset.get('test', dataset.get('validation', [])))} (test/validation)")
        
        # Sprawdzenie dostƒôpnych emocji
        emotions = {}
        for sample in dataset['train']:
            emotion = sample['emotion']
            emotions[emotion] = emotions.get(emotion, 0) + 1
        
        print("Rozk≈Çad emocji w zbiorze treningowym:")
        for emotion, count in emotions.items():
            print(f"   - {emotion}: {count} pr√≥bek ({count/len(dataset['train'])*100:.1f}%)")
            
        return dataset
    except Exception as e:
        print(f"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd podczas ≈Çadowania datasetu: {str(e)}")
        raise e


def prepare_data(dataset, for_torch=True):
    """
    Przygotowanie danych: ekstrakcja spektrogram√≥w Mela i etykiet z paskiem postƒôpu
    
    Args:
        dataset: Dataset do przetworzenia
        for_torch: Czy dane majƒÖ byƒá przygotowane dla PyTorch (domy≈õlnie: True)
        
    Returns:
        X_train, X_val, X_test: Zbiory danych (treningowe, walidacyjne, testowe)
        y_train, y_val, y_test: Etykiety dla zbior√≥w
    """
    print("üîÑ Rozpoczynam przygotowanie danych...")

    # Listy na dane
    mel_spectrograms = []
    labels = []
    count_by_emotion = {emotion: 0 for emotion in EMOTION_MAPPING.keys()}
    skipped_samples = 0
    
    # Przetwarzanie zestawu treningowego z paskiem postƒôpu
    print("Ekstrahowanie spektrogram√≥w Mela...")
    for sample in tqdm(dataset['train'], desc="Przetwarzanie pr√≥bek"):
        try:
            audio = np.array(sample['audio']['array'])
            emotion = sample['emotion']

            if emotion in EMOTION_MAPPING:
                mel_spec = extract_melspectrogram(audio)
                mel_spectrograms.append(mel_spec)
                labels.append(EMOTION_MAPPING[emotion])
                count_by_emotion[emotion] += 1
            else:
                skipped_samples += 1
        except Exception as e:
            skipped_samples += 1
            print(f"‚ùå Pominiƒôto pr√≥bkƒô z powodu b≈Çƒôdu: {str(e)}")

    # Konwersja do numpy arrays
    X = np.array(mel_spectrograms)
    y = np.array(labels)

    print(f"Przygotowano {len(X)} pr√≥bek z {len(dataset['train'])} dostƒôpnych")
    print(f"Pominiƒôto {skipped_samples} pr√≥bek")
    
    print("Liczba pr√≥bek dla ka≈ºdej emocji:")
    for emotion, count in count_by_emotion.items():
        if count > 0:
            print(f"   - {emotion}: {count} pr√≥bek")

    # Dostosowanie wymiar√≥w dla VGG16 (wymaga 3 kana≈Ç√≥w kolor√≥w)
    print("Dostosowywanie danych do formatu VGG16...")
    X = np.repeat(X[:, :, :, np.newaxis], 3, axis=3)
    
    if for_torch:
        print("Dane bƒôdƒÖ u≈ºywane w formacie PyTorch")
    else:
        print("Dane przygotowane w formacie TensorFlow")

    # Podzia≈Ç na zbiory treningowe i testowe
    print("Dzielenie danych na zbiory treningowy, walidacyjny i testowy...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)

    # Wy≈õwietlenie informacji o kszta≈Çcie danych
    print(f"‚úÖ Dane zosta≈Çy pomy≈õlnie przygotowane:")
    print(f"Zbi√≥r treningowy: {X_train.shape[0]} pr√≥bek, kszta≈Çt: {X_train.shape}")
    print(f"Zbi√≥r walidacyjny: {X_val.shape[0]} pr√≥bek, kszta≈Çt: {X_val.shape}")
    print(f"Zbi√≥r testowy: {X_test.shape[0]} pr√≥bek, kszta≈Çt: {X_test.shape}")

    return X_train, X_val, X_test, y_train, y_val, y_test


def predict_emotion(model, audio_path, device=None):
    """
    Predykcja emocji dla nowego pliku audio
    
    Args:
        model: Wytrenowany model
        audio_path: ≈öcie≈ºka do pliku audio
        device: UrzƒÖdzenie (cuda/cpu) - dla PyTorch
        
    Returns:
        predicted_class: Indeks przewidzianej klasy
        prediction: Wektor prawdopodobie≈Ñstw dla wszystkich klas
    """
    print(f"üéµ Analizowanie pliku audio: {os.path.basename(audio_path)}")
    try:
        # Wczytanie audio
        audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE)
        print(f"Plik wczytany: d≈Çugo≈õƒá {len(audio)/sr:.2f}s, czƒôstotliwo≈õƒá {sr}Hz")

        # Ekstrakcja spektrogramu Mela
        mel_spec = extract_melspectrogram(audio)
        
        # Przygotowanie do predykcji - replikacja na 3 kana≈Çy
        mel_spec = np.repeat(mel_spec[:, :, np.newaxis], 3, axis=2)
        
        import torch
        if isinstance(model, torch.nn.Module):
            # Obs≈Çuga modeli PyTorch
            print(" Wykryto model PyTorch")
            if device is None:
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                
            # Przygotowanie tensora wej≈õciowego
            mel_spec = torch.FloatTensor(mel_spec).unsqueeze(0).permute(0, 3, 1, 2)
            mel_spec = mel_spec.to(device)
            
            # Predykcja z modelem PyTorch
            model.eval()
            with torch.no_grad():
                outputs = model(mel_spec)
                probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]
                predicted_class = torch.argmax(probabilities).item()
                probabilities = probabilities.cpu().numpy()
        else:
            # Obs≈Çuga modeli TensorFlow/Keras
            print("   Wykryto model TensorFlow/Keras")
            mel_spec = mel_spec.reshape(1, mel_spec.shape[0], mel_spec.shape[1], 3)
            
            # Predykcja
            probabilities = model.predict(mel_spec)[0]
            predicted_class = np.argmax(probabilities)
            
        # Wyniki
        print(f"‚úÖ Wyniki analizy emocji:")
        print(f"   Przewidziana emocja: {EMOTION_NAMES[predicted_class]} ({probabilities[predicted_class]:.2%})")
        print("   Wszystkie prawdopodobie≈Ñstwa:")
        
        # Sortowanie emocji wed≈Çug prawdopodobie≈Ñstwa
        emotions_probs = [(EMOTION_NAMES[i], probabilities[i]) for i in range(len(EMOTION_NAMES))]
        emotions_probs.sort(key=lambda x: x[1], reverse=True)
        
        for emotion, prob in emotions_probs:
            print(f"   - {emotion}: {prob:.2%}")
            
        return predicted_class, probabilities
        
    except Exception as e:
        print(f"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd podczas analizy pliku audio: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None



